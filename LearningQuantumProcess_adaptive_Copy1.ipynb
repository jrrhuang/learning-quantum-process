{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "24434da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import ast\n",
    "import math\n",
    "import scipy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sigI = np.array([[1.0, 0.0j], [0.0j, 1.0]])\n",
    "sigX = np.array([[0.0j, 1.0], [1.0, 0.0j]])\n",
    "sigY = np.array([[0.0j, -1.0j], [1.0j, 0.0j]])\n",
    "sigZ = np.array([[1.0, 0.0j], [0.0j, -1.0]])\n",
    "\n",
    "N = 50\n",
    "\n",
    "def kron(ls):\n",
    "    A = ls[0]\n",
    "    for X in ls[1:]:\n",
    "        A = np.kron(A, X)\n",
    "    return A\n",
    "\n",
    "def generate_all_zero_state():\n",
    "    return [np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_one_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) for i in range(N)]\n",
    "\n",
    "def generate_half_half_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i < N/2 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_neel_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i % 2 == 0 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_plus_state():\n",
    "    return [np.array([[0.5, 0.5], [0.5, 0.5+0.0j]]) for i in range(N)]\n",
    "\n",
    "def generate_random_product_state():\n",
    "    list_rhoi = []\n",
    "    for i in range(N):\n",
    "        v = np.random.normal(size=3)\n",
    "        v /= np.linalg.norm(v)\n",
    "        rhoi = sigI / 2.0 + (v[0] * sigX / 2.0) + (v[1] * sigY / 2.0) + (v[2] * sigZ / 2.0)\n",
    "        list_rhoi.append(rhoi)\n",
    "    return list_rhoi\n",
    "\n",
    "def twobytwo_to_Pauli(list_rhoi):\n",
    "    list_rhoi_new = []\n",
    "    for rhoi in list_rhoi:\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "    return list_rhoi_new\n",
    "\n",
    "def get_RDM_in_Pauli(list_rhoi, k):\n",
    "    feat_vec = []\n",
    "    for i in range(N-k+1):\n",
    "        for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "            val = 1.0\n",
    "            for c, P in enumerate(list_P):\n",
    "                if P == -1: continue\n",
    "                val *= list_rhoi[(3*(i+c))+P]\n",
    "            assert(np.abs(val.imag) < 1e-7)\n",
    "            feat_vec.append(val.real)\n",
    "    return feat_vec\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML(all_states, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "        print(\"Pos:\", pos)\n",
    "\n",
    "        def twobytwo_to_Pauli(list_rhoi):\n",
    "            list_rhoi_new = []\n",
    "            for rhoi in list_rhoi:\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "            return list_rhoi_new\n",
    "\n",
    "        def get_RDM_in_Pauli(list_rhoi, k):\n",
    "            feat_vec = []\n",
    "            for i in range(N-k+1):\n",
    "                for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "                    val = 1.0\n",
    "                    for c, P in enumerate(list_P):\n",
    "                        if P == -1: continue\n",
    "                        val *= list_rhoi[(3*(i+c))+P]\n",
    "                    assert(np.abs(val.imag) < 1e-7)\n",
    "                    feat_vec.append(val.real)\n",
    "            return feat_vec\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_states)), range(len(all_states)), test_size=test_size, random_state=random_seed)\n",
    "\n",
    "        for k in [1, 2, 3, 4]:\n",
    "            print(\"Validate k =\", k)\n",
    "            X, y_true, y_noisy = [], [], []\n",
    "\n",
    "            for data in zip(all_states, all_values):\n",
    "                X.append(get_RDM_in_Pauli(data[0], k))\n",
    "                y_true.append(data[1][pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[1][pos]+1)/2, 1)[0] / 500) - 1)\n",
    "\n",
    "            X = np.array(X)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "                print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML_transformed(all_X_list, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "#         print(\"Pos:\", pos)\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_values)), range(len(all_values)), test_size=test_size, random_state=random_seed)\n",
    "        \n",
    "        for k in [1, 2]:\n",
    "#             print(\"Validate k =\", k)\n",
    "\n",
    "            X = all_X_list[k-1]\n",
    "            \n",
    "            y_true, y_noisy = [], []\n",
    "            for data in all_values:\n",
    "                y_true.append(data[pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[pos]+1)/2, 1)[0] / 500) - 1)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "#                 print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "def transform_states(all_states):\n",
    "    all_X_list = []\n",
    "    \n",
    "    for k in [1, 2]:\n",
    "        X = []\n",
    "        for data in all_states:\n",
    "            X.append(get_RDM_in_Pauli(data, k))\n",
    "        all_X_list.append(np.array(X))\n",
    "    return all_X_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de994661",
   "metadata": {},
   "source": [
    "# XY model with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "8f7e29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "sample_size = 100\n",
    "all_data_training_set_scaling = []\n",
    "seed = 0\n",
    "test_size = 0.5\n",
    "num_holdout = 1\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "\n",
    "# Train / holdout split\n",
    "np.random.seed(seed)\n",
    "sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "train_states, train_values = np.array(all_states)[\n",
    "    sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "\n",
    "# I setup the framework for multiple holdout but right now still should be the same as using \n",
    "# only one holdout in my current code below\n",
    "holdout_states_ens, holdout_values_ens = [],[]\n",
    "for i in range(1, num_holdout+1):\n",
    "    holdout_states, holdout_values = np.array(all_states)[\n",
    "        sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "    holdout_states_ens.append(holdout_states)\n",
    "    holdout_values_ens.append(holdout_values)\n",
    "h = holdout_states_ens[0]\n",
    "\n",
    "train_X_list = transform_states(train_states)\n",
    "holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "51d54a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.1809684234136905 0.18331344719082018\n",
      "Training clf_holdout...\n",
      "Scores: 0.1370288928713825 0.08149093179813323\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Obtain classifiers\n",
    "print(\"Training clf_train...\")\n",
    "list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "    train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "clf_train = list_of_clf_train[0]\n",
    "k_train = list_of_bestk_train[0]\n",
    "\n",
    "print(\"Training clf_holdout...\")\n",
    "clf_holdout_ens = []\n",
    "score_holdout_ens = []\n",
    "k_holdout_ens = []\n",
    "for i in range(num_holdout):\n",
    "    list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "        holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "    clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "    score_holdout_ens.append(list_of_score_holdout[0])\n",
    "    k_holdout_ens.append(list_of_bestk_holdout[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "3d419fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# Adaptive optimization\n",
    "\n",
    "def predict_holdout(x):\n",
    "    # Get holdout prediction\n",
    "    predictions = []\n",
    "    for i in range(num_holdout):\n",
    "        # Get best classifier with best k\n",
    "        clf_holdout = clf_holdout_ens[i]\n",
    "        k_holdout = k_holdout_ens[i]\n",
    "        X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "        predictions.append(clf_holdout.predict(X_holdout))\n",
    "    return predictions[0][0]\n",
    "\n",
    "def predict_train(x):\n",
    "    # Get train prediction\n",
    "    X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "    return clf_train.predict(X_train)[0]\n",
    "\n",
    "def get_product_state(y):\n",
    "    # Compute product state from input spherical coordinates\n",
    "    x = []\n",
    "    for i in range(N):\n",
    "        phi = y[i]\n",
    "        theta = y[N+i]\n",
    "        x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "    return x\n",
    "\n",
    "def objective(y):\n",
    "    # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -abs(y_train - y_holdout)\n",
    "\n",
    "def objective_train(y):\n",
    "    # Maximizes train predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    return -y_train\n",
    "\n",
    "def objective_holdout(y):\n",
    "    x = get_product_state(y)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -y_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "789d2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "def objective_state(x):\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return abs(y_train - y_holdout)\n",
    "\n",
    "# max_err = 0\n",
    "# max_idx = 0\n",
    "# avg_err = 0\n",
    "# print(\"Print: max error, index of maximum error, average error\")\n",
    "# np.random.seed(seed)\n",
    "# for i in range(100):\n",
    "#     y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "#     if i % 500 == 0:\n",
    "#         print(f\"step {i}: {max_err}, {max_idx}, {avg_err}\")\n",
    "#     err = -objective(y0)\n",
    "#     if err > max_err:\n",
    "#         max_err = err\n",
    "#         max_idx = i\n",
    "#     avg_err += err / 10000\n",
    "# print(max_err)\n",
    "\n",
    "# Sometimes I get something like this, i.e. classifier always outputs the same number\n",
    "# for i in range(10000):\n",
    "#     if i % 500 == 0:\n",
    "#         print(predict_train(all_states[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "b1a0fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          100     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.04176D-02    |proj g|=  8.14497D-02\n",
      "\n",
      "At iterate    1    f=  4.49316D-02    |proj g|=  7.17918D-02\n",
      "\n",
      "At iterate    2    f= -1.66070D-01    |proj g|=  4.45008D-02\n",
      "\n",
      "At iterate    3    f= -1.87221D-01    |proj g|=  3.03842D-02\n",
      "\n",
      "At iterate    4    f= -1.94897D-01    |proj g|=  6.67487D-03\n",
      "\n",
      "At iterate    5    f= -1.98043D-01    |proj g|=  8.17441D-03\n",
      "\n",
      "At iterate    6    f= -2.14766D-01    |proj g|=  1.19964D-02\n",
      "\n",
      "At iterate    7    f= -2.21893D-01    |proj g|=  1.08330D-02\n",
      "\n",
      "At iterate    8    f= -2.24519D-01    |proj g|=  3.40317D-03\n",
      "\n",
      "At iterate    9    f= -2.25831D-01    |proj g|=  5.57586D-03\n",
      "\n",
      "At iterate   10    f= -2.27506D-01    |proj g|=  6.91223D-03\n",
      "\n",
      "At iterate   11    f= -2.29020D-01    |proj g|=  4.39133D-03\n",
      "\n",
      "At iterate   12    f= -2.29883D-01    |proj g|=  1.36219D-03\n",
      "\n",
      "At iterate   13    f= -2.30070D-01    |proj g|=  6.54837D-04\n",
      "\n",
      "At iterate   14    f= -2.30124D-01    |proj g|=  6.88771D-04\n",
      "\n",
      "At iterate   15    f= -2.30136D-01    |proj g|=  1.22871D-03\n",
      "\n",
      "At iterate   16    f= -2.30150D-01    |proj g|=  3.07848D-04\n",
      "\n",
      "At iterate   17    f= -2.30155D-01    |proj g|=  3.72247D-04\n",
      "\n",
      "At iterate   18    f= -2.30174D-01    |proj g|=  9.16101D-04\n",
      "\n",
      "At iterate   19    f= -2.30225D-01    |proj g|=  1.61875D-03\n",
      "\n",
      "At iterate   20    f= -2.31399D-01    |proj g|=  6.38500D-03\n",
      "  ys=-8.750E-04  -gs= 4.670E-04 BFGS update SKIPPED\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   21    f= -2.31782D-01    |proj g|=  5.08168D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   22    f= -2.32876D-01    |proj g|=  5.68266D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   23    f= -2.33293D-01    |proj g|=  4.74369D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   24    f= -2.34484D-01    |proj g|=  6.25546D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   25    f= -2.34896D-01    |proj g|=  4.83271D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   26    f= -2.35812D-01    |proj g|=  8.02018D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   27    f= -2.36512D-01    |proj g|=  4.58321D-03\n",
      "\n",
      "At iterate   28    f= -2.39490D-01    |proj g|=  2.24085D-03\n",
      "\n",
      "At iterate   29    f= -2.39744D-01    |proj g|=  2.14533D-03\n",
      "\n",
      "At iterate   30    f= -2.40062D-01    |proj g|=  8.35387D-04\n",
      "\n",
      "At iterate   31    f= -2.40191D-01    |proj g|=  7.37066D-04\n",
      "\n",
      "At iterate   32    f= -2.40197D-01    |proj g|=  1.89215D-04\n",
      "\n",
      "At iterate   33    f= -2.40198D-01    |proj g|=  1.89615D-04\n",
      "\n",
      "At iterate   34    f= -2.40206D-01    |proj g|=  1.90514D-04\n",
      "\n",
      "At iterate   35    f= -2.40216D-01    |proj g|=  1.91769D-04\n",
      "\n",
      "At iterate   36    f= -2.40234D-01    |proj g|=  2.71649D-04\n",
      "\n",
      "At iterate   37    f= -2.40273D-01    |proj g|=  5.93647D-04\n",
      "\n",
      "At iterate   38    f= -2.40323D-01    |proj g|=  5.46396D-04\n",
      "\n",
      "At iterate   39    f= -2.40410D-01    |proj g|=  1.18487D-03\n",
      "\n",
      "At iterate   40    f= -2.40432D-01    |proj g|=  1.10832D-03\n",
      "\n",
      "At iterate   41    f= -2.40474D-01    |proj g|=  4.00302D-04\n",
      "\n",
      "At iterate   42    f= -2.40491D-01    |proj g|=  2.29827D-04\n",
      "\n",
      "At iterate   43    f= -2.40501D-01    |proj g|=  2.14095D-04\n",
      "\n",
      "At iterate   44    f= -2.40503D-01    |proj g|=  8.46734D-04\n",
      "\n",
      "At iterate   45    f= -2.40512D-01    |proj g|=  1.89038D-04\n",
      "\n",
      "At iterate   46    f= -2.40515D-01    |proj g|=  1.87350D-04\n",
      "\n",
      "At iterate   47    f= -2.40520D-01    |proj g|=  1.79684D-04\n",
      "\n",
      "At iterate   48    f= -2.40532D-01    |proj g|=  2.99227D-04\n",
      "\n",
      "At iterate   49    f= -2.40550D-01    |proj g|=  6.55154D-04\n",
      "\n",
      "At iterate   50    f= -2.40559D-01    |proj g|=  3.85159D-04\n",
      "\n",
      "At iterate   51    f= -2.40571D-01    |proj g|=  1.67394D-04\n",
      "\n",
      "At iterate   52    f= -2.40575D-01    |proj g|=  1.33321D-04\n",
      "\n",
      "At iterate   53    f= -2.40576D-01    |proj g|=  2.23532D-04\n",
      "\n",
      "At iterate   54    f= -2.40576D-01    |proj g|=  1.29330D-04\n",
      "\n",
      "At iterate   55    f= -2.40577D-01    |proj g|=  1.91402D-05\n",
      "\n",
      "At iterate   56    f= -2.40577D-01    |proj g|=  3.22131D-05\n",
      "\n",
      "At iterate   57    f= -2.40577D-01    |proj g|=  2.83329D-05\n",
      "\n",
      "At iterate   58    f= -2.40577D-01    |proj g|=  1.45051D-05\n",
      "\n",
      "At iterate   59    f= -2.40577D-01    |proj g|=  1.44162D-05\n",
      "\n",
      "At iterate   60    f= -2.40577D-01    |proj g|=  1.64313D-05\n",
      "\n",
      "At iterate   61    f= -2.40578D-01    |proj g|=  2.75502D-05\n",
      "\n",
      "At iterate   62    f= -2.40578D-01    |proj g|=  3.77476D-05\n",
      "\n",
      "At iterate   63    f= -2.40578D-01    |proj g|=  9.30978D-05\n",
      "\n",
      "At iterate   64    f= -2.40578D-01    |proj g|=  4.55747D-05\n",
      "\n",
      "At iterate   65    f= -2.40579D-01    |proj g|=  3.84026D-05\n",
      "\n",
      "At iterate   66    f= -2.40579D-01    |proj g|=  1.07603D-04\n",
      "\n",
      "At iterate   67    f= -2.40579D-01    |proj g|=  3.10640D-05\n",
      "\n",
      "At iterate   68    f= -2.40579D-01    |proj g|=  6.45040D-05\n",
      "\n",
      "At iterate   69    f= -2.40580D-01    |proj g|=  4.52027D-05\n",
      "\n",
      "At iterate   70    f= -2.40580D-01    |proj g|=  7.42684D-05\n",
      "\n",
      "At iterate   71    f= -2.40580D-01    |proj g|=  1.92180D-05\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -0.24057966646879983\n",
      "        x: [ 0.000e+00  0.000e+00 ...  3.142e+00  5.540e+00]\n",
      "      nit: 72\n",
      "      jac: [ 0.000e+00  4.083e-02 ...  4.974e-06  0.000e+00]\n",
      "     nfev: 8282\n",
      "     njev: 82\n",
      " hess_inv: <100x100 LbfgsInvHessProduct with dtype=float64>\n",
      "\n",
      "At iterate   72    f= -2.40580D-01    |proj g|=  6.83897D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  100     72     82     75     1    14   6.839D-06  -2.406D-01\n",
      "  F = -0.24057966646879983     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Nelder-mead\n",
    "np.random.seed(seed)\n",
    "bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N)\n",
    "y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "# res = minimize(objective_train, y0, method='Powell', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective, y0, method='Nelder-Mead', bounds=bnds, options={'disp': True})\n",
    "res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective_train, y0, method='trust-constr', bounds=bnds, options={'disp': True})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "b09d5858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2741796664687998"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the error, i.e. if large, then we have overfit\n",
    "state = get_product_state(res.x)\n",
    "y_train = predict_train(state)\n",
    "y_holdout = predict_holdout(state)\n",
    "abs(y_train-y_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842de2f9",
   "metadata": {},
   "source": [
    "Repeat this for many initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "22b691f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.16462395921915518 0.12911599216449912\n",
      "Training clf_holdout...\n",
      "Scores: 0.16864085852093336 0.1638682023767973\n",
      "Scores: 0.0863293194447738 0.20104740214809017\n",
      "Scores: 0.14473888498874282 0.16870663097824282\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.0532\n",
      "Adaptive error:  0.037524520130469335\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.2072709887596873 0.21446674440225758\n",
      "Training clf_holdout...\n",
      "Scores: 0.08619344518090447 0.08878962089002698\n",
      "Scores: 0.08500319447232102 0.17236315656378495\n",
      "Scores: 0.1449320146221621 0.16573287256100375\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.17679407788259333\n",
      "Adaptive error:  0.1252\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.1809684234136905 0.18331344719082018\n",
      "Training clf_holdout...\n",
      "Scores: 0.1403567738076975 0.08566088281898569\n",
      "Scores: 0.16105392298779414 0.1831447131138543\n",
      "Scores: 0.13583516214519836 0.12564461664818954\n",
      "Number of iterations:  72\n",
      "Nonadaptive error:  0.4236019857657038\n",
      "Adaptive error:  0.2505796664687998\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.175982966402861 0.11105624734869712\n",
      "Training clf_holdout...\n",
      "Scores: 0.04013721725212728 0.19363859569769915\n",
      "Scores: 0.16019606354985289 0.14635228884159623\n",
      "Scores: 0.07063405355075479 0.13635777949062197\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.06359999999999998\n",
      "Adaptive error:  0.06359999999999998\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.14268979401810927 0.17783165819566557\n",
      "Training clf_holdout...\n",
      "Scores: 0.19296780332002683 0.12887756871647213\n",
      "Scores: 0.24853630355917877 0.12606893007071823\n",
      "Scores: 0.07351387253089108 0.16065505005169772\n",
      "Number of iterations:  13\n",
      "Nonadaptive error:  0.17495991357373739\n",
      "Adaptive error:  0.11623271070835622\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13509075655054445 0.16837202375885688\n",
      "Training clf_holdout...\n",
      "Scores: 0.1195047528157996 0.17792089551869392\n",
      "Scores: 0.10740547151156657 0.12469586596645843\n",
      "Scores: 0.14236989787817808 0.2638616277362352\n",
      "Number of iterations:  38\n",
      "Nonadaptive error:  0.347612048596656\n",
      "Adaptive error:  0.27058990466161637\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10885345207292059 0.15632694534513636\n",
      "Training clf_holdout...\n",
      "Scores: 0.12319106771247024 0.19324010704292988\n",
      "Scores: 0.20401838151845067 0.19776439520357691\n",
      "Scores: 0.13308807100012202 0.20926838530754902\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.09198017306697462\n",
      "Adaptive error:  0.09198352260684627\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.10454798493891745 0.21761204899018355\n",
      "Training clf_holdout...\n",
      "Scores: 0.14585781760366212 0.16278003691557005\n",
      "Scores: 0.16007748109630654 0.15012378471654717\n",
      "Scores: 0.12860890679730913 0.1651680878705387\n",
      "Number of iterations:  30\n",
      "Nonadaptive error:  0.318453639221777\n",
      "Adaptive error:  0.1853575831967781\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.08998591458903421 0.21443212004228746\n",
      "Training clf_holdout...\n",
      "Scores: 0.08217725243353137 0.19983742607681307\n",
      "Scores: 0.14257182176748862 0.2201229514417737\n",
      "Scores: 0.187214330325206 0.23952354897930744\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.19436321675473794\n",
      "Adaptive error:  0.06943436636755526\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.2223790010678297 0.12862899809987552\n",
      "Training clf_holdout...\n",
      "Scores: 0.1714315684282035 0.15632104376258893\n",
      "Scores: 0.120980399300163 0.14845040799232992\n",
      "Scores: 0.08498578164605877 0.15856813117118332\n",
      "Number of iterations:  37\n",
      "Nonadaptive error:  0.4343912757384337\n",
      "Adaptive error:  0.5267998434498917\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.16555487373386424 0.19823974160422003\n",
      "Training clf_holdout...\n",
      "Scores: 0.141660943390973 0.1424848792856864\n",
      "Scores: 0.1440843883641621 0.16399673358952815\n",
      "Scores: 0.1922018634882003 0.11948626370323376\n",
      "Number of iterations:  25\n",
      "Nonadaptive error:  0.35413146619979974\n",
      "Adaptive error:  0.43492512726617916\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.15888564136623867 0.16763499370116888\n",
      "Training clf_holdout...\n",
      "Scores: 0.1705993673095105 0.15079396489312846\n",
      "Scores: 0.1570716548173578 0.14379915664947904\n",
      "Scores: 0.12519898368939078 0.14546767317877948\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.012533333333333344\n",
      "Adaptive error:  0.0032000000000000067\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.16818849681507425 0.13214870555968972\n",
      "Training clf_holdout...\n",
      "Scores: 0.13761139979248033 0.13915013333977347\n",
      "Scores: 0.12014570998548377 0.15861445312600964\n",
      "Scores: 0.15593355501785194 0.15050934028276122\n",
      "Number of iterations:  12\n",
      "Nonadaptive error:  0.14120361895034797\n",
      "Adaptive error:  0.056173263953528035\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.1820802459563976 0.13775268208915606\n",
      "Training clf_holdout...\n",
      "Scores: 0.12069103145809984 0.13554635269114024\n",
      "Scores: 0.15407228265218664 0.12956972189610946\n",
      "Scores: 0.14925864700674052 0.14576529758118204\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.015866666666666668\n",
      "Adaptive error:  0.015866666666666668\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.14696839109835996 0.15162113508459343\n",
      "Training clf_holdout...\n",
      "Scores: 0.1393688097158235 0.13858201082045066\n",
      "Scores: 0.1566144799211649 0.1303837796036408\n",
      "Scores: 0.1417586735741313 0.16228733285042535\n",
      "Number of iterations:  7\n",
      "Nonadaptive error:  0.14358712842251464\n",
      "Adaptive error:  0.10358999562904242\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13119172706120452 0.16817718022893072\n",
      "Training clf_holdout...\n",
      "Scores: 0.15993806724380077 0.1794790727133574\n",
      "Scores: 0.1697509959369361 0.1608306021832971\n",
      "Scores: 0.15569988497243037 0.17205663579034156\n",
      "Number of iterations:  80\n",
      "Nonadaptive error:  0.42574279270605636\n",
      "Adaptive error:  0.529186416390411\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.17034777865109646 0.14938096391856556\n",
      "Training clf_holdout...\n",
      "Scores: 0.14377927468101465 0.14523650350992234\n",
      "Scores: 0.12289978693152562 0.14458636796148386\n",
      "Scores: 0.14629142504509568 0.16352684889904887\n",
      "Number of iterations:  23\n",
      "Nonadaptive error:  0.4318480582443247\n",
      "Adaptive error:  0.388886135459861\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.15011060667146217 0.17252408000049313\n",
      "Training clf_holdout...\n",
      "Scores: 0.13982757810413382 0.1872734984627613\n",
      "Scores: 0.1807891977501835 0.1622465558016876\n",
      "Scores: 0.16660967675533167 0.14202476756963373\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.19979037645910727\n",
      "Adaptive error:  0.005396828466509548\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.1763851908878838 0.17566595600731733\n",
      "Training clf_holdout...\n",
      "Scores: 0.1701751136835165 0.1728061907413417\n",
      "Scores: 0.19500484682849573 0.14930167210051895\n",
      "Scores: 0.1386937217165918 0.15517445095447677\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.025683676519097122\n",
      "Adaptive error:  0.025683714713862365\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.1651918622756881 0.14287307189048618\n",
      "Training clf_holdout...\n",
      "Scores: 0.14137518734864263 0.15371890550239564\n",
      "Scores: 0.14678138746387148 0.15681479716949173\n",
      "Scores: 0.13001691955689407 0.17412677105740096\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.07952396049973576\n",
      "Adaptive error:  0.04877643686561085\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.15589832750540034 0.15131445874505367\n",
      "Training clf_holdout...\n",
      "Scores: 0.14870408289542514 0.13186634459285798\n",
      "Scores: 0.13734130296602953 0.1646625965324173\n",
      "Scores: 0.14178673063317782 0.15393353032467216\n",
      "Number of iterations:  85\n",
      "Nonadaptive error:  0.4951881640908899\n",
      "Adaptive error:  0.7946857626469647\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.1375474375001225 0.13718042207371414\n",
      "Training clf_holdout...\n",
      "Scores: 0.13555574058699071 0.15245610607401194\n",
      "Scores: 0.11632517897192995 0.1364837021705868\n",
      "Scores: 0.14261656148971202 0.12830740133625182\n",
      "Number of iterations:  40\n",
      "Nonadaptive error:  0.34852333110356876\n",
      "Adaptive error:  0.531904967237371\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.1478818171210039 0.1286202090263973\n",
      "Training clf_holdout...\n",
      "Scores: 0.13537471906774196 0.1569042127146887\n",
      "Scores: 0.14380480813076657 0.15118432762963688\n",
      "Scores: 0.16254952036131576 0.1569401142872965\n",
      "Number of iterations:  9\n",
      "Nonadaptive error:  0.20235321487831048\n",
      "Adaptive error:  0.053295040762135315\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.1469757334171944 0.1318439025453525\n",
      "Training clf_holdout...\n",
      "Scores: 0.1496016555189292 0.12778865616920843\n",
      "Scores: 0.17848887261947383 0.13586761392013824\n",
      "Scores: 0.1389239499585853 0.15041384736157626\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.11091758715193178\n",
      "Adaptive error:  0.013433760831048261\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.14391429906234265 0.13438062574987794\n",
      "Training clf_holdout...\n",
      "Scores: 0.12828914662071625 0.1601820428813584\n",
      "Scores: 0.15882422443136474 0.1451121645638212\n",
      "Scores: 0.12994582658013756 0.13070737730542428\n",
      "Number of iterations:  26\n",
      "Nonadaptive error:  0.3914744831876811\n",
      "Adaptive error:  0.6334490667642209\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.15723430000564598 0.1573196732709076\n",
      "Training clf_holdout...\n",
      "Scores: 0.16201712496452997 0.15082569305894\n",
      "Scores: 0.1380222499413377 0.14064614959598498\n",
      "Scores: 0.16750559442811158 0.13506653099184937\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.17050834226257705\n",
      "Adaptive error:  0.007872417163685749\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.14501883873708188 0.13832446755646977\n",
      "Training clf_holdout...\n",
      "Scores: 0.16270213049069154 0.13338717139161504\n",
      "Scores: 0.15836346843349872 0.17397788371344086\n",
      "Scores: 0.14827766308468668 0.15909237580445423\n",
      "Number of iterations:  12\n",
      "Nonadaptive error:  0.21056463793264854\n",
      "Adaptive error:  0.23426476559437537\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.15160340838836717 0.13183852544736566\n",
      "Training clf_holdout...\n",
      "Scores: 0.17974007884401338 0.1639035353679017\n",
      "Scores: 0.13779007914406338 0.1523782226484732\n",
      "Scores: 0.11671209418415282 0.17214397672873188\n",
      "Number of iterations:  47\n",
      "Nonadaptive error:  0.37171388723174476\n",
      "Adaptive error:  0.59110396060477\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.16368886155289333 0.14689167492773003\n",
      "Training clf_holdout...\n",
      "Scores: 0.15860704709118553 0.14318461084215478\n",
      "Scores: 0.11471078483772959 0.12770047152078012\n",
      "Scores: 0.1364113588717713 0.13695293199685393\n",
      "Number of iterations:  31\n",
      "Nonadaptive error:  0.2919581526659215\n",
      "Adaptive error:  0.3529954932135292\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.15715381656013097 0.15135445472044126\n",
      "Training clf_holdout...\n",
      "Scores: 0.1362092081621273 0.152458229491861\n",
      "Scores: 0.13744539662584718 0.1393049062081293\n",
      "Scores: 0.1296175737438447 0.17282515475170604\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.07829854805941969\n",
      "Adaptive error:  0.01066933858559895\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.1479387347802868 0.16646960747008344\n",
      "Training clf_holdout...\n",
      "Scores: 0.1685471490026643 0.15084122629766777\n",
      "Scores: 0.14383329140782766 0.14429085379983567\n",
      "Scores: 0.16342211740683255 0.15248657740010596\n",
      "Number of iterations:  119\n",
      "Nonadaptive error:  0.46008641978673637\n",
      "Adaptive error:  0.780414032740742\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.14913293626441548 0.16838420691133435\n",
      "Training clf_holdout...\n",
      "Scores: 0.1393600816459571 0.12207893257563383\n",
      "Scores: 0.1387179017603445 0.13509436640876799\n",
      "Scores: 0.1391943405374363 0.16661359317140095\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.0977783413132148\n",
      "Adaptive error:  0.015055652769933663\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.14708641399973 0.14525645112994293\n",
      "Training clf_holdout...\n",
      "Scores: 0.14931961826312784 0.1400980301447811\n",
      "Scores: 0.12486677444150567 0.1215001227858097\n",
      "Scores: 0.12681446198817625 0.1394075077620706\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.27947765404352976\n",
      "Adaptive error:  0.08502307656628019\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.11241439648038853 0.1384260872995269\n",
      "Training clf_holdout...\n",
      "Scores: 0.1746166274691529 0.11090983648830133\n",
      "Scores: 0.14387864599716138 0.1280450606894335\n",
      "Scores: 0.14427036520754882 0.14255780188445832\n",
      "Number of iterations:  39\n",
      "Nonadaptive error:  0.23541987601369346\n",
      "Adaptive error:  0.12279398693917903\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.1449708692178256 0.12333904515595287\n",
      "Training clf_holdout...\n",
      "Scores: 0.13514624391539928 0.1432633743362042\n",
      "Scores: 0.13172588255596898 0.1318679917297249\n",
      "Scores: 0.15528055365736204 0.1334147147358206\n",
      "Number of iterations:  11\n",
      "Nonadaptive error:  0.19018263590761225\n",
      "Adaptive error:  0.257429946852791\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.16846948272141132 0.17257177964483242\n",
      "Training clf_holdout...\n",
      "Scores: 0.13832513795592982 0.13275433354526694\n",
      "Scores: 0.15583077135122667 0.15429630578408945\n",
      "Scores: 0.16015142499547563 0.13129802090270049\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.2632160878926862\n",
      "Adaptive error:  0.04879027996288776\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.1326032998952112 0.12671306646671732\n",
      "Training clf_holdout...\n",
      "Scores: 0.15098438446198 0.1453395642638317\n",
      "Scores: 0.1374683571624445 0.14758191634666212\n",
      "Scores: 0.14566000761021008 0.14011140966784272\n",
      "Number of iterations:  32\n",
      "Nonadaptive error:  0.3183894408430736\n",
      "Adaptive error:  0.3519898483315783\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.14867374275968975 0.14425640562480246\n",
      "Training clf_holdout...\n",
      "Scores: 0.155457965465285 0.13841211053310332\n",
      "Scores: 0.1240095480913331 0.1302479632013891\n",
      "Scores: 0.1680514312563095 0.11987145649338764\n",
      "Number of iterations:  39\n",
      "Nonadaptive error:  0.3681455680185271\n",
      "Adaptive error:  0.6186103550977582\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.15054854328102177 0.12054995778600819\n",
      "Training clf_holdout...\n",
      "Scores: 0.14128008365624967 0.14043832724232216\n",
      "Scores: 0.14461526977417177 0.14718666460979463\n",
      "Scores: 0.14941360668676515 0.14279052835550005\n",
      "Number of iterations:  23\n",
      "Nonadaptive error:  0.38537978954997526\n",
      "Adaptive error:  0.6084202488919715\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.13784210416652992 0.13348681508463764\n",
      "Training clf_holdout...\n",
      "Scores: 0.14652635671595188 0.15042273873022205\n",
      "Scores: 0.14638309609574823 0.1564690902546255\n",
      "Scores: 0.11433627746977837 0.13087239946061455\n",
      "Number of iterations:  68\n",
      "Nonadaptive error:  0.47111270249034204\n",
      "Adaptive error:  0.9623914975609799\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.14665970304927345 0.1546261114204475\n",
      "Training clf_holdout...\n",
      "Scores: 0.15323832080019648 0.09777384937748847\n",
      "Scores: 0.13279389904077338 0.12914604689016335\n",
      "Scores: 0.12834693236556408 0.13222067402501517\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0.1805814619916685\n",
      "Adaptive error:  0.015200320389869836\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.1401101410576877 0.13347575744401455\n",
      "Training clf_holdout...\n",
      "Scores: 0.1389489629413892 0.10536219673773928\n",
      "Scores: 0.1401394768764822 0.11485893801223553\n",
      "Scores: 0.15897617657141874 0.14833513874832796\n",
      "Number of iterations:  58\n",
      "Nonadaptive error:  0.4439648329103598\n",
      "Adaptive error:  0.7060774563374136\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.13230909520948106 0.13321333624582754\n",
      "Training clf_holdout...\n",
      "Scores: 0.1378529745718604 0.13011832505519666\n",
      "Scores: 0.12474773422861843 0.11249704667360469\n",
      "Scores: 0.13038636811825943 0.12807211173148114\n",
      "Number of iterations:  7\n",
      "Nonadaptive error:  0.1743655423924276\n",
      "Adaptive error:  0.03862340212735185\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.12175644823354403 0.15522384822956414\n",
      "Training clf_holdout...\n",
      "Scores: 0.1335681463598531 0.15253023108868988\n",
      "Scores: 0.13104104198230832 0.11372867742992347\n",
      "Scores: 0.13023533442039706 0.10960367834618598\n",
      "Number of iterations:  18\n",
      "Nonadaptive error:  0.2753639964367177\n",
      "Adaptive error:  0.055020852359618636\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.13772873544047007 0.1438376404894916\n",
      "Training clf_holdout...\n",
      "Scores: 0.13695073027423976 0.12326064872924032\n",
      "Scores: 0.13515482946493182 0.20626350776082408\n",
      "Scores: 0.14673614452644632 0.1418283946558288\n",
      "Number of iterations:  10\n",
      "Nonadaptive error:  0.3867552088461411\n",
      "Adaptive error:  0.02477378242667537\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.1626288730411735 0.16154153556270356\n",
      "Training clf_holdout...\n",
      "Scores: 0.13226431803836447 0.12735627335117627\n",
      "Scores: 0.14794945641397053 0.15050621378977563\n",
      "Scores: 0.11403486630184273 0.11665671844529565\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.19128198508168112\n",
      "Adaptive error:  0.04016471588565161\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.12712316323518238 0.109223075491701\n",
      "Training clf_holdout...\n",
      "Scores: 0.13796311250446275 0.14310151469507298\n",
      "Scores: 0.14115566600350793 0.11300492337674617\n",
      "Scores: 0.15052893189272984 0.12244303848761429\n",
      "Number of iterations:  37\n",
      "Nonadaptive error:  0.32182866146248057\n",
      "Adaptive error:  0.26878401731026\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.14973980022695205 0.13091995923161992\n",
      "Training clf_holdout...\n",
      "Scores: 0.14770182340627197 0.13794721668739637\n",
      "Scores: 0.1335726249631717 0.10822819344675003\n",
      "Scores: 0.14176293279875643 0.1002975897517078\n",
      "Number of iterations:  15\n",
      "Nonadaptive error:  0.17875667427854414\n",
      "Adaptive error:  0.10168425071696532\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.16437901377669756 0.15905013933507522\n",
      "Training clf_holdout...\n",
      "Scores: 0.12915636655639778 0.09931780361293749\n",
      "Scores: 0.1300088763258888 0.13277895630767278\n",
      "Scores: 0.1364255622285626 0.1215037024683007\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.28365454931731926\n",
      "Adaptive error:  0.07612882584812652\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.13690797273012742 0.14591352685451206\n",
      "Training clf_holdout...\n",
      "Scores: 0.14318883936782653 0.11780260212421116\n",
      "Scores: 0.16570253464400508 0.1294083844007487\n",
      "Scores: 0.12510706113789072 0.11941015322773078\n",
      "Number of iterations:  12\n",
      "Nonadaptive error:  0.2745613956660681\n",
      "Adaptive error:  0.03139222122518917\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.12300009059495243 0.11183348028214991\n",
      "Training clf_holdout...\n",
      "Scores: 0.13180787312318248 0.12419351615503255\n",
      "Scores: 0.13910556481114197 0.12268858265777582\n",
      "Scores: 0.11414419300132822 0.10421271739961531\n",
      "Number of iterations:  22\n",
      "Nonadaptive error:  0.1900506472656477\n",
      "Adaptive error:  0.08168337434329281\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.12704122261449793 0.11182437034145366\n",
      "Training clf_holdout...\n",
      "Scores: 0.12238104017264728 0.09654341188543401\n",
      "Scores: 0.12481609912638128 0.10541898135275442\n",
      "Scores: 0.12139395079948133 0.10216579598230602\n",
      "Number of iterations:  34\n",
      "Nonadaptive error:  0.25539975707062723\n",
      "Adaptive error:  0.07708007995249844\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.13152990827847913 0.10235560106688697\n",
      "Training clf_holdout...\n",
      "Scores: 0.13155891447581275 0.11772685143019729\n",
      "Scores: 0.13125656789504986 0.09634186729343458\n",
      "Scores: 0.1319156384300409 0.09016894638262468\n",
      "Number of iterations:  25\n",
      "Nonadaptive error:  0.19382582903739454\n",
      "Adaptive error:  0.16682260307225516\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.1267098892382392 0.10664604042948611\n",
      "Training clf_holdout...\n",
      "Scores: 0.12146445563295236 0.10669732698046959\n",
      "Scores: 0.11702778665432978 0.0972626466750422\n",
      "Scores: 0.12789662688067316 0.10316611744503831\n",
      "Number of iterations:  61\n",
      "Nonadaptive error:  0.24765159489740185\n",
      "Adaptive error:  0.15910545014648658\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.1329365527887185 0.10213681811178615\n",
      "Training clf_holdout...\n",
      "Scores: 0.1279335156863528 0.10374819576384277\n",
      "Scores: 0.13104241542284903 0.12414023165803549\n",
      "Scores: 0.14194244236272288 0.10756015741248204\n",
      "Number of iterations:  73\n",
      "Nonadaptive error:  0.26744966419080696\n",
      "Adaptive error:  0.22598566190504943\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.11718523557713631 0.10122262190743928\n",
      "Training clf_holdout...\n",
      "Scores: 0.12651728077577817 0.10639333339483864\n",
      "Scores: 0.12032889306066272 0.11662732215906624\n",
      "Scores: 0.11620293080050051 0.09981946734339144\n",
      "Number of iterations:  76\n",
      "Nonadaptive error:  0.22541834293143456\n",
      "Adaptive error:  0.21295747046893782\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.1201289913545193 0.106549406624122\n",
      "Training clf_holdout...\n",
      "Scores: 0.128945193213361 0.09643931467343148\n",
      "Scores: 0.14179457406603166 0.10622090012170088\n",
      "Scores: 0.12944367854909106 0.13150875121579259\n",
      "Number of iterations:  35\n",
      "Nonadaptive error:  0.2303612609602093\n",
      "Adaptive error:  0.1912128642900816\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.12472040498214226 0.11352037092310838\n",
      "Training clf_holdout...\n",
      "Scores: 0.13983990279266567 0.09477112510013908\n",
      "Scores: 0.1261094923407444 0.08598981000099419\n",
      "Scores: 0.1353802775610892 0.12144118271409686\n",
      "Number of iterations:  23\n",
      "Nonadaptive error:  0.2080736663330297\n",
      "Adaptive error:  0.04694171841711525\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.1244499945609808 0.09963449796154916\n",
      "Training clf_holdout...\n",
      "Scores: 0.13362573954748186 0.11726803454699203\n",
      "Scores: 0.12910393179929938 0.09553973697227391\n",
      "Scores: 0.1176210889573838 0.10152221604810802\n",
      "Number of iterations:  73\n",
      "Nonadaptive error:  0.24420624083751968\n",
      "Adaptive error:  0.32475656701464706\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.1354655091870967 0.10848101116302457\n",
      "Training clf_holdout...\n",
      "Scores: 0.12572805256268133 0.11208175753684937\n",
      "Scores: 0.12990019007678996 0.09666987534504838\n",
      "Scores: 0.1276318606964759 0.10702820941781081\n",
      "Number of iterations:  83\n",
      "Nonadaptive error:  0.2694573432345398\n",
      "Adaptive error:  0.25408411229405264\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.11236239948795357 0.08648922108937367\n",
      "Training clf_holdout...\n",
      "Scores: 0.12652912246652248 0.09225199727075499\n",
      "Scores: 0.1027126122869448 0.08243883008596879\n",
      "Scores: 0.11207402966128648 0.08525050599132933\n",
      "Number of iterations:  34\n",
      "Nonadaptive error:  0.1947105416411315\n",
      "Adaptive error:  0.00740889612786555\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.10767408707152004 0.08155067491043927\n",
      "Training clf_holdout...\n",
      "Scores: 0.12128490113247478 0.09377931211753657\n",
      "Scores: 0.11107310651392623 0.09742864601564438\n",
      "Scores: 0.11837350240558399 0.08908663613463784\n",
      "Number of iterations:  108\n",
      "Nonadaptive error:  0.2563370082699524\n",
      "Adaptive error:  0.3156201565732576\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.11878784385839575 0.08809306499705702\n",
      "Training clf_holdout...\n",
      "Scores: 0.11792707550524371 0.09357178709330609\n",
      "Scores: 0.1196443896975045 0.09807316708390365\n",
      "Scores: 0.11674655305416666 0.09230310875533725\n",
      "Number of iterations:  65\n",
      "Nonadaptive error:  0.1514010194934608\n",
      "Adaptive error:  0.08352983404784231\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.10958734885123186 0.08712895070779482\n",
      "Training clf_holdout...\n",
      "Scores: 0.10579313988142722 0.08757774815000555\n",
      "Scores: 0.11366250599754808 0.09035240226951365\n",
      "Scores: 0.10369867570016691 0.08516143020151926\n",
      "Number of iterations:  28\n",
      "Nonadaptive error:  0.13117830916888887\n",
      "Adaptive error:  0.07298897492722078\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.11445580687845831 0.08592954740389386\n",
      "Training clf_holdout...\n",
      "Scores: 0.11173543263369475 0.09198241551019117\n",
      "Scores: 0.1329357460153987 0.09201240730529518\n",
      "Scores: 0.11891000692635596 0.08093360799353559\n",
      "Number of iterations:  79\n",
      "Nonadaptive error:  0.17823866335296992\n",
      "Adaptive error:  0.10171984848224835\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.12354008961075467 0.08175843815272861\n",
      "Training clf_holdout...\n",
      "Scores: 0.10576166447441507 0.08776062887378379\n",
      "Scores: 0.10735879017480349 0.0851410421959011\n",
      "Scores: 0.10694990260864308 0.09298663409895143\n",
      "Number of iterations:  61\n",
      "Nonadaptive error:  0.26700393396252214\n",
      "Adaptive error:  0.41743680623405777\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10891570981711585 0.09339696229370084\n",
      "Training clf_holdout...\n",
      "Scores: 0.11720228409922907 0.08319692593850249\n",
      "Scores: 0.11129308965465429 0.09701536937208388\n",
      "Scores: 0.11306928814658254 0.08417776719271995\n",
      "Number of iterations:  47\n",
      "Nonadaptive error:  0.1458694709561296\n",
      "Adaptive error:  0.06426284968263563\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.1068040305186918 0.08923220127366423\n",
      "Training clf_holdout...\n",
      "Scores: 0.11148815918630808 0.08582662773676059\n",
      "Scores: 0.11183354245305589 0.08793562970759394\n",
      "Scores: 0.11464996166150251 0.07321275869413066\n",
      "Number of iterations:  80\n",
      "Nonadaptive error:  0.22948448811675323\n",
      "Adaptive error:  0.1464629565388661\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.11645628199508441 0.09456851689319677\n",
      "Training clf_holdout...\n",
      "Scores: 0.09749880247375467 0.09188662111755966\n",
      "Scores: 0.11634736449518465 0.0723484450395279\n",
      "Scores: 0.10510331853108465 0.09678213522358853\n",
      "Number of iterations:  43\n",
      "Nonadaptive error:  0.21648763603453614\n",
      "Adaptive error:  0.1467115559255051\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.11649347770978033 0.08448691600479791\n",
      "Training clf_holdout...\n",
      "Scores: 0.11969285423283936 0.10749888025119061\n",
      "Scores: 0.11326680919054823 0.0926644897367926\n",
      "Scores: 0.11406415796505175 0.09388137332350278\n",
      "Number of iterations:  51\n",
      "Nonadaptive error:  0.16333023861582202\n",
      "Adaptive error:  0.16945264840264607\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statistics\n",
    "from scipy.optimize import minimize\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "sample_size_list = [20, 60, 100, 150, 200, 300, 500]\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,len(sample_size_list)))\n",
    "\n",
    "for s, sample_size in enumerate(sample_size_list):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_eval):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "05185712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>60</th>\n",
       "      <th>100</th>\n",
       "      <th>150</th>\n",
       "      <th>200</th>\n",
       "      <th>300</th>\n",
       "      <th>500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.015675</td>\n",
       "      <td>8.079366e-02</td>\n",
       "      <td>0.299498</td>\n",
       "      <td>0.320328</td>\n",
       "      <td>-0.165381</td>\n",
       "      <td>-0.108367</td>\n",
       "      <td>-0.187302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.051594</td>\n",
       "      <td>-9.333333e-03</td>\n",
       "      <td>0.183382</td>\n",
       "      <td>-0.082723</td>\n",
       "      <td>0.262113</td>\n",
       "      <td>-0.178320</td>\n",
       "      <td>0.059283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.173022</td>\n",
       "      <td>-8.503035e-02</td>\n",
       "      <td>-0.149058</td>\n",
       "      <td>-0.194455</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>-0.027003</td>\n",
       "      <td>-0.067871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.097484</td>\n",
       "      <td>-0.112626</td>\n",
       "      <td>-0.220343</td>\n",
       "      <td>-0.088546</td>\n",
       "      <td>-0.058189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.058727</td>\n",
       "      <td>-3.999713e-02</td>\n",
       "      <td>0.241975</td>\n",
       "      <td>0.067247</td>\n",
       "      <td>-0.361981</td>\n",
       "      <td>-0.041464</td>\n",
       "      <td>-0.076519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.077022</td>\n",
       "      <td>1.034436e-01</td>\n",
       "      <td>-0.162636</td>\n",
       "      <td>-0.214426</td>\n",
       "      <td>-0.151117</td>\n",
       "      <td>-0.012461</td>\n",
       "      <td>0.150433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>-4.296192e-02</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>-0.053045</td>\n",
       "      <td>-0.039148</td>\n",
       "      <td>-0.081607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.133096</td>\n",
       "      <td>-1.943935e-01</td>\n",
       "      <td>0.219390</td>\n",
       "      <td>0.250465</td>\n",
       "      <td>-0.077072</td>\n",
       "      <td>-0.161132</td>\n",
       "      <td>-0.083022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.124929</td>\n",
       "      <td>3.819477e-08</td>\n",
       "      <td>0.061037</td>\n",
       "      <td>0.223040</td>\n",
       "      <td>-0.207526</td>\n",
       "      <td>0.080550</td>\n",
       "      <td>-0.069776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.092409</td>\n",
       "      <td>-3.074752e-02</td>\n",
       "      <td>-0.067629</td>\n",
       "      <td>0.491279</td>\n",
       "      <td>-0.243169</td>\n",
       "      <td>-0.015373</td>\n",
       "      <td>0.006122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        20            60        100       150       200       300       500\n",
       "0 -0.015675  8.079366e-02  0.299498  0.320328 -0.165381 -0.108367 -0.187302\n",
       "1 -0.051594 -9.333333e-03  0.183382 -0.082723  0.262113 -0.178320  0.059283\n",
       "2 -0.173022 -8.503035e-02 -0.149058 -0.194455 -0.135742 -0.027003 -0.067871\n",
       "3  0.000000  0.000000e+00 -0.097484 -0.112626 -0.220343 -0.088546 -0.058189\n",
       "4 -0.058727 -3.999713e-02  0.241975  0.067247 -0.361981 -0.041464 -0.076519\n",
       "5 -0.077022  1.034436e-01 -0.162636 -0.214426 -0.151117 -0.012461  0.150433\n",
       "6  0.000003 -4.296192e-02  0.023700  0.033600 -0.053045 -0.039148 -0.081607\n",
       "7 -0.133096 -1.943935e-01  0.219390  0.250465 -0.077072 -0.161132 -0.083022\n",
       "8 -0.124929  3.819477e-08  0.061037  0.223040 -0.207526  0.080550 -0.069776\n",
       "9  0.092409 -3.074752e-02 -0.067629  0.491279 -0.243169 -0.015373  0.006122"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xy = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 300, 500])\n",
    "df_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "a28c30fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    -0.054165\n",
       "60    -0.021823\n",
       "100    0.055217\n",
       "150    0.078173\n",
       "200   -0.135326\n",
       "300   -0.059126\n",
       "500   -0.040845\n",
       "dtype: float64"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xy.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff39afd",
   "metadata": {},
   "source": [
    "# Ising with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f633c9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.09148684550355596 0.10356823968380986\n",
      "Training clf_holdout...\n",
      "Scores: 0.09000554335199401 0.12394863867265815\n",
      "Scores: 0.15886182714772337 0.24736171195576845\n",
      "Scores: 0.1036874941148683 0.12195346956958858\n",
      "Number of iterations:  21\n",
      "Nonadaptive error:  0.1322223671244211\n",
      "Adaptive error:  0.27261770293731835\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.09337342602775593 0.15645991895709516\n",
      "Training clf_holdout...\n",
      "Scores: 0.13984585663163449 0.09709296278668628\n",
      "Scores: 0.11949184631430475 0.14147948750380598\n",
      "Scores: 0.0403799567443455 0.10935942101906534\n",
      "Number of iterations:  44\n",
      "Nonadaptive error:  0.25233303465688395\n",
      "Adaptive error:  0.3395264744682905\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.059231291468772915 0.10853695515858641\n",
      "Training clf_holdout...\n",
      "Scores: 0.09176398906636432 0.11235641897605583\n",
      "Scores: 0.07152507526028266 0.11090388999871681\n",
      "Scores: 0.07874880673129955 0.1146075230366167\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.012605161675882801\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.08588242843820093 0.14510654161039563\n",
      "Training clf_holdout...\n",
      "Scores: 0.1122556481989665 0.0935396501193689\n",
      "Scores: 0.09171517665564939 0.0879542224663255\n",
      "Scores: 0.11525397076157251 0.0969568072825369\n",
      "Number of iterations:  15\n",
      "Nonadaptive error:  0.11479136849201298\n",
      "Adaptive error:  0.179514444703534\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.050471083116058 0.1308559188856007\n",
      "Training clf_holdout...\n",
      "Scores: 0.07579890868531955 0.13806915571807743\n",
      "Scores: 0.09244385692458185 0.1202507801091158\n",
      "Scores: 0.15350915513894667 0.07821276187440057\n",
      "Number of iterations:  42\n",
      "Nonadaptive error:  0.19264473577817337\n",
      "Adaptive error:  0.10471823767392549\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13671525303359724 0.14405030531659763\n",
      "Training clf_holdout...\n",
      "Scores: 0.13041647963735414 0.14719475819671132\n",
      "Scores: 0.06913493970452639 0.12201127000533701\n",
      "Scores: 0.12927177524282363 0.15782109380677864\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.07719999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.08015098769635855 0.11595837655594894\n",
      "Training clf_holdout...\n",
      "Scores: 0.07895056792315605 0.1313630962252679\n",
      "Scores: 0.07328760457011277 0.14756465583701983\n",
      "Scores: 0.1463174983485927 0.11502348257217203\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.06200000000000001\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.09025844193878223 0.17295268920343215\n",
      "Training clf_holdout...\n",
      "Scores: 0.10501606584265533 0.0935818841949276\n",
      "Scores: 0.10769381774978487 0.10999019924439636\n",
      "Scores: 0.11716828440328374 0.08296429483597308\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.04159999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.06930376077898182 0.1632568671947783\n",
      "Training clf_holdout...\n",
      "Scores: 0.1045343346220043 0.12610897962991227\n",
      "Scores: 0.12812062523023254 0.1570024740918354\n",
      "Scores: 0.12110961750767774 0.15775456339152602\n",
      "Number of iterations:  95\n",
      "Nonadaptive error:  0.23050215960879486\n",
      "Adaptive error:  0.2503928654072433\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.10303418916346178 0.13647010758143632\n",
      "Training clf_holdout...\n",
      "Scores: 0.10281567637125265 0.1326768120821945\n",
      "Scores: 0.11335436088875092 0.07951310434206464\n",
      "Scores: 0.09807135020323224 0.13706828612189517\n",
      "Number of iterations:  16\n",
      "Nonadaptive error:  0.12046000806658223\n",
      "Adaptive error:  0.1362544627511379\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.11499678152014889 0.1429908766436759\n",
      "Training clf_holdout...\n",
      "Scores: 0.112341797718028 0.142176703159857\n",
      "Scores: 0.09576115433706608 0.11397752909698072\n",
      "Scores: 0.12677744743305677 0.11424607597505884\n",
      "Number of iterations:  34\n",
      "Nonadaptive error:  0.1944836819503701\n",
      "Adaptive error:  0.5079363924156041\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.10506596709565466 0.11679593601794037\n",
      "Training clf_holdout...\n",
      "Scores: 0.08448180702722688 0.14042042348377715\n",
      "Scores: 0.11287290848130567 0.11549978784761525\n",
      "Scores: 0.11861619409493689 0.11129553662973894\n",
      "Number of iterations:  29\n",
      "Nonadaptive error:  0.15202647250165904\n",
      "Adaptive error:  0.5102719987100072\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.10191752718240471 0.1391261424225414\n",
      "Training clf_holdout...\n",
      "Scores: 0.0777855370565119 0.14887997989349228\n",
      "Scores: 0.09856261695452322 0.11803319847671623\n",
      "Scores: 0.1290821153505079 0.10377087806153366\n",
      "Number of iterations:  24\n",
      "Nonadaptive error:  0.21711585837412276\n",
      "Adaptive error:  0.4518851927148689\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.10116326564667752 0.10139358804494893\n",
      "Training clf_holdout...\n",
      "Scores: 0.12229931433712465 0.12566790291277433\n",
      "Scores: 0.1128207481518714 0.11152007733343641\n",
      "Scores: 0.09259563398909013 0.11399822289672533\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0017333333333333385\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.08442413351967383 0.1148653478849371\n",
      "Training clf_holdout...\n",
      "Scores: 0.08928038145010209 0.11227561263592568\n",
      "Scores: 0.1048006414242898 0.10762241885610217\n",
      "Scores: 0.09747564605388505 0.09194351632921967\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.030973059683798248\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.12539574971173195 0.128510767946681\n",
      "Training clf_holdout...\n",
      "Scores: 0.11608535226516283 0.14071387882244987\n",
      "Scores: 0.12952471681008537 0.10456439615530567\n",
      "Scores: 0.09680862991566062 0.12051363000024913\n",
      "Number of iterations:  11\n",
      "Nonadaptive error:  0.10201755008863883\n",
      "Adaptive error:  0.07294170302753435\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.11255852110710189 0.10961097274950066\n",
      "Training clf_holdout...\n",
      "Scores: 0.12185541033382985 0.11550045140301095\n",
      "Scores: 0.08355513238219472 0.11429307634629281\n",
      "Scores: 0.11146403481131467 0.13391608869698887\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0046666666666666905\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.12808202112921618 0.12578463841965218\n",
      "Training clf_holdout...\n",
      "Scores: 0.10434475753856207 0.08728915514579144\n",
      "Scores: 0.08451019620573517 0.11029944540086759\n",
      "Scores: 0.09468366474783413 0.1164644341507312\n",
      "Number of iterations:  24\n",
      "Nonadaptive error:  0.14242545715421262\n",
      "Adaptive error:  0.5819720000677218\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.15154552797007892 0.12094722561922723\n",
      "Training clf_holdout...\n",
      "Scores: 0.10800640063192776 0.13100709756024545\n",
      "Scores: 0.08940388453569306 0.10111760441488081\n",
      "Scores: 0.10102798829336619 0.13792063820941095\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.07220884043204016\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.09194746441032936 0.09298930478500671\n",
      "Training clf_holdout...\n",
      "Scores: 0.096288470856177 0.10798670266550249\n",
      "Scores: 0.08124032689857921 0.11208879906936461\n",
      "Scores: 0.10578284638016419 0.10685041225116866\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.006115306038454896\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.13592166822456897 0.10169512040524223\n",
      "Training clf_holdout...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 0.10691165372075469 0.09309826003424616\n",
      "Scores: 0.11864766850911107 0.11265401742601237\n",
      "Scores: 0.10922816325260526 0.11372553669172365\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.09923074395110873\n",
      "Adaptive error:  0.10260764334407427\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.11559949010876067 0.12097346929186308\n",
      "Training clf_holdout...\n",
      "Scores: 0.10682370134451033 0.13880515682184627\n",
      "Scores: 0.10046033913906721 0.11730292104858586\n",
      "Scores: 0.10059299613612538 0.1122245170251673\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.028753646283940767\n",
      "Adaptive error:  0.011622568604781683\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.09954677793617578 0.11477366660089508\n",
      "Training clf_holdout...\n",
      "Scores: 0.11289385071991587 0.10020056457566452\n",
      "Scores: 0.09127806066143324 0.12045225566510016\n",
      "Scores: 0.09148259876010656 0.12260604323701127\n",
      "Number of iterations:  30\n",
      "Nonadaptive error:  0.19597137817897098\n",
      "Adaptive error:  0.2959705913451732\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.11033889170474967 0.11495994620046598\n",
      "Training clf_holdout...\n",
      "Scores: 0.10668476613886282 0.12044235709748452\n",
      "Scores: 0.10023899443635517 0.12796588781559007\n",
      "Scores: 0.09281138454143394 0.10276185281382157\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.013040000000000003\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.1243701505655477 0.09954953739669416\n",
      "Training clf_holdout...\n",
      "Scores: 0.10394585821389916 0.09570458804754309\n",
      "Scores: 0.11906871291360963 0.10636602284344526\n",
      "Scores: 0.1264944249763239 0.10883856548158548\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.025679999999999998\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.11178282314410243 0.14055273237924862\n",
      "Training clf_holdout...\n",
      "Scores: 0.09807643919320794 0.10996436479493155\n",
      "Scores: 0.09583182566578301 0.10816014651682558\n",
      "Scores: 0.12006584027120734 0.11716055795326276\n",
      "Number of iterations:  41\n",
      "Nonadaptive error:  0.14959893335394242\n",
      "Adaptive error:  0.3043504616374374\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10915119824805089 0.11130809976069474\n",
      "Training clf_holdout...\n",
      "Scores: 0.10487412811490053 0.14484581891910267\n",
      "Scores: 0.11642150303425872 0.12603266278983505\n",
      "Scores: 0.12065688976810907 0.11548457602436765\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.006799999999999997\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.111285330913203 0.10674176718055664\n",
      "Training clf_holdout...\n",
      "Scores: 0.08945524921172716 0.10062992468976235\n",
      "Scores: 0.10987318020999626 0.11679226738355568\n",
      "Scores: 0.10399220932527051 0.12362091519756546\n",
      "Number of iterations:  42\n",
      "Nonadaptive error:  0.23131135653624926\n",
      "Adaptive error:  0.5885790948850689\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.11937447688965883 0.13067560337996048\n",
      "Training clf_holdout...\n",
      "Scores: 0.1083671704745765 0.11806169878073672\n",
      "Scores: 0.11703076642035859 0.12053980560946562\n",
      "Scores: 0.11378477972833739 0.09615458990784208\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.02623694853080263\n",
      "Adaptive error:  0.009499177579007936\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.09792741944838876 0.08520404446214726\n",
      "Training clf_holdout...\n",
      "Scores: 0.10126102558790209 0.10174228571879726\n",
      "Scores: 0.115308716230807 0.10798141115082033\n",
      "Scores: 0.10050196460946417 0.09895764024761446\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0038421951908521514\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.1209304401135368 0.10762368451828283\n",
      "Training clf_holdout...\n",
      "Scores: 0.11186902090155104 0.10909184387637635\n",
      "Scores: 0.10952588989239487 0.11206827519600798\n",
      "Scores: 0.10075126091739273 0.11462780949023535\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.007093333333333346\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.12455513533613041 0.1198025842396727\n",
      "Training clf_holdout...\n",
      "Scores: 0.10433511755146743 0.10889022083824297\n",
      "Scores: 0.11060417254552853 0.11528977871924209\n",
      "Scores: 0.10335921015784691 0.11290854654231762\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.013599335019116789\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.11371435166789984 0.10200064946386438\n",
      "Training clf_holdout...\n",
      "Scores: 0.11946335613769725 0.089554070660913\n",
      "Scores: 0.11000411394455839 0.10862548143377869\n",
      "Scores: 0.10264164557459957 0.12245659786111986\n",
      "Number of iterations:  9\n",
      "Nonadaptive error:  0.03996318833972045\n",
      "Adaptive error:  0.036970907449664234\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.11764282253123631 0.10796640351046642\n",
      "Training clf_holdout...\n",
      "Scores: 0.11021873756602645 0.11060896927916822\n",
      "Scores: 0.08839265910371147 0.09942970363648854\n",
      "Scores: 0.1223774698998385 0.1256969620729892\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0028266666666666752\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.1262729168436238 0.09733435049657439\n",
      "Training clf_holdout...\n",
      "Scores: 0.1070180920521391 0.10126328648680957\n",
      "Scores: 0.11784676816849356 0.12079370545498355\n",
      "Scores: 0.12918889125260383 0.12417520421072077\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0030933333333333394\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.10565817344369419 0.13399536832476597\n",
      "Training clf_holdout...\n",
      "Scores: 0.10673559579477465 0.09817598475907181\n",
      "Scores: 0.10971913235091715 0.1129243871777841\n",
      "Scores: 0.12224021678618378 0.09104946059513014\n",
      "Number of iterations:  51\n",
      "Nonadaptive error:  0.09225942785972303\n",
      "Adaptive error:  0.22707199358130137\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10456883445488523 0.11948135840477637\n",
      "Training clf_holdout...\n",
      "Scores: 0.10952454059147826 0.12335856147693912\n",
      "Scores: 0.1078875082810781 0.11184093782315888\n",
      "Scores: 0.12380199512576923 0.13379823037117025\n",
      "Number of iterations:  36\n",
      "Nonadaptive error:  0.17585871982372955\n",
      "Adaptive error:  0.6302153006738735\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.11351337686716212 0.10371434097393753\n",
      "Training clf_holdout...\n",
      "Scores: 0.10178387740932457 0.11497826763507801\n",
      "Scores: 0.11992338407780417 0.09845451360167666\n",
      "Scores: 0.11315014472319371 0.10211012618192387\n",
      "Number of iterations:  11\n",
      "Nonadaptive error:  0.047779509344471456\n",
      "Adaptive error:  0.007007635668999718\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.11184224123977889 0.10933256517504461\n",
      "Training clf_holdout...\n",
      "Scores: 0.11119670097227496 0.11574458665003415\n",
      "Scores: 0.11814309780081358 0.10340443590015456\n",
      "Scores: 0.11867534484010185 0.09494569466746026\n",
      "Number of iterations:  5\n",
      "Nonadaptive error:  0.042520201677681974\n",
      "Adaptive error:  0.07044966856695986\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.09850817783229099 0.0903649725422176\n",
      "Training clf_holdout...\n",
      "Scores: 0.10659764003296333 0.1193403052713102\n",
      "Scores: 0.11244603336498549 0.1070439131903742\n",
      "Scores: 0.12210944980704515 0.13182986687340836\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.09125168916678868\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.12183751370397229 0.09824736918071829\n",
      "Training clf_holdout...\n",
      "Scores: 0.12673780789777367 0.10451899392782023\n",
      "Scores: 0.10238413793110206 0.10039731653971921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 0.11141307555634938 0.11580068522382687\n",
      "Number of iterations:  8\n",
      "Nonadaptive error:  0.07128208012569423\n",
      "Adaptive error:  0.11160832047982866\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.10853539953937652 0.11210978804275847\n",
      "Training clf_holdout...\n",
      "Scores: 0.11456915899101444 0.1202893990254779\n",
      "Scores: 0.12026099828496177 0.09960945420245623\n",
      "Scores: 0.11592923234960732 0.09685587477871464\n",
      "Number of iterations:  6\n",
      "Nonadaptive error:  0.04931343652574665\n",
      "Adaptive error:  0.05177184193262664\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.11410313590429272 0.09749310105956842\n",
      "Training clf_holdout...\n",
      "Scores: 0.10305109549390064 0.09636831959130963\n",
      "Scores: 0.10426955483212189 0.11270277338692918\n",
      "Scores: 0.11463200996019371 0.10625386745459384\n",
      "Number of iterations:  17\n",
      "Nonadaptive error:  0.07130844651715665\n",
      "Adaptive error:  0.006482953482764328\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.11677699286773323 0.106148813900413\n",
      "Training clf_holdout...\n",
      "Scores: 0.1110646321061652 0.10295240708496926\n",
      "Scores: 0.11481140809538382 0.12329918921304277\n",
      "Scores: 0.11246407182391399 0.11016707910675012\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.024140449972456304\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.09718101128226816 0.09745141461237669\n",
      "Training clf_holdout...\n",
      "Scores: 0.11573044263528032 0.10675387878672313\n",
      "Scores: 0.12933605501681822 0.11577552641051081\n",
      "Scores: 0.09533271555379175 0.09704679550585782\n",
      "Number of iterations:  13\n",
      "Nonadaptive error:  0.06242038194264153\n",
      "Adaptive error:  0.08749022668751155\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13125192953354595 0.11980995447869376\n",
      "Training clf_holdout...\n",
      "Scores: 0.10662946457514559 0.09836819880703687\n",
      "Scores: 0.10983675774706855 0.09890756997647189\n",
      "Scores: 0.12028754088730642 0.09908072837776506\n",
      "Number of iterations:  77\n",
      "Nonadaptive error:  0.20293181916820796\n",
      "Adaptive error:  0.6036211679384307\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10934280241940651 0.11761739540693039\n",
      "Training clf_holdout...\n",
      "Scores: 0.10080697196757765 0.11049652999058879\n",
      "Scores: 0.11589347431704991 0.10303737106819884\n",
      "Scores: 0.0980861615766933 0.09269871413757645\n",
      "Number of iterations:  14\n",
      "Nonadaptive error:  0.05795254771774086\n",
      "Adaptive error:  0.015382041591171897\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.10201966929865436 0.10325155560837715\n",
      "Training clf_holdout...\n",
      "Scores: 0.10566540837196012 0.12770585306066834\n",
      "Scores: 0.10960361440415883 0.1077119873592571\n",
      "Scores: 0.11590535748607082 0.11261246693922503\n",
      "Number of iterations:  19\n",
      "Nonadaptive error:  0.07162143675552524\n",
      "Adaptive error:  0.1645374630858819\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.10476931318749647 0.10965060949336478\n",
      "Training clf_holdout...\n",
      "Scores: 0.10775717640529982 0.10272689171358104\n",
      "Scores: 0.10582017975685362 0.10699581563825482\n",
      "Scores: 0.12143118564572122 0.10544302482400345\n",
      "Number of iterations:  22\n",
      "Nonadaptive error:  0.07749388221402928\n",
      "Adaptive error:  0.13964671474698562\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 9\n",
      "Training clf_train...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[377], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Obtain classifiers\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining clf_train...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m list_of_score_train, list_of_clf_train, list_of_bestk_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sparse_ML_transformed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_X_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m clf_train \u001b[38;5;241m=\u001b[39m list_of_clf_train[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m k_train \u001b[38;5;241m=\u001b[39m list_of_bestk_train[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[358], line 191\u001b[0m, in \u001b[0;36mtrain_sparse_ML_transformed\u001b[0;34m(all_X_list, all_values, test_size, random_seed)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;66;03m# ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m15\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m14\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m12\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m11\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)]:\n\u001b[0;32m--> 191\u001b[0m                 score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mML_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneg_root_mean_squared_error\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m#                 print(score)\u001b[39;00m\n\u001b[1;32m    193\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m best_cv_score \u001b[38;5;241m>\u001b[39m score:\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:1004\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m     this_Xy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m _, this_coef, this_dual_gap, this_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43malphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthis_Xy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# from here on **params\u001b[39;49;00m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m coef_[k] \u001b[38;5;241m=\u001b[39m this_coef[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1029\u001b[0m dual_gaps_[k] \u001b[38;5;241m=\u001b[39m this_dual_gap[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631\u001b[0m, in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    617\u001b[0m     model \u001b[38;5;241m=\u001b[39m cd_fast\u001b[38;5;241m.\u001b[39menet_coordinate_descent_gram(\n\u001b[1;32m    618\u001b[0m         coef_,\n\u001b[1;32m    619\u001b[0m         l1_reg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         positive,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m precompute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcd_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menet_coordinate_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecompute should be one of True, False, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or array-like. Got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;241m%\u001b[39m precompute\n\u001b[1;32m    638\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "\n",
    "# Ising model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,6))\n",
    "\n",
    "for s, sample_size in enumerate([20, 60, 100, 150, 200, 500]):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_iter):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ising_homog = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 500])\n",
    "df_ising_homog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf61a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp_qml",
   "language": "python",
   "name": "imp_qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
