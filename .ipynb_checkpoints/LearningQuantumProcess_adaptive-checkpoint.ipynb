{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24434da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import ast\n",
    "import math\n",
    "import scipy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sigI = np.array([[1.0, 0.0j], [0.0j, 1.0]])\n",
    "sigX = np.array([[0.0j, 1.0], [1.0, 0.0j]])\n",
    "sigY = np.array([[0.0j, -1.0j], [1.0j, 0.0j]])\n",
    "sigZ = np.array([[1.0, 0.0j], [0.0j, -1.0]])\n",
    "\n",
    "N = 50\n",
    "\n",
    "def kron(ls):\n",
    "    A = ls[0]\n",
    "    for X in ls[1:]:\n",
    "        A = np.kron(A, X)\n",
    "    return A\n",
    "\n",
    "def generate_all_zero_state():\n",
    "    return [np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_one_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) for i in range(N)]\n",
    "\n",
    "def generate_half_half_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i < N/2 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_neel_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i % 2 == 0 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_plus_state():\n",
    "    return [np.array([[0.5, 0.5], [0.5, 0.5+0.0j]]) for i in range(N)]\n",
    "\n",
    "def generate_random_product_state():\n",
    "    list_rhoi = []\n",
    "    for i in range(N):\n",
    "        v = np.random.normal(size=3)\n",
    "        v /= np.linalg.norm(v)\n",
    "        rhoi = sigI / 2.0 + (v[0] * sigX / 2.0) + (v[1] * sigY / 2.0) + (v[2] * sigZ / 2.0)\n",
    "        list_rhoi.append(rhoi)\n",
    "    return list_rhoi\n",
    "\n",
    "def twobytwo_to_Pauli(list_rhoi):\n",
    "    list_rhoi_new = []\n",
    "    for rhoi in list_rhoi:\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "    return list_rhoi_new\n",
    "\n",
    "def get_RDM_in_Pauli(list_rhoi, k):\n",
    "    feat_vec = []\n",
    "    for i in range(N-k+1):\n",
    "        for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "            val = 1.0\n",
    "            for c, P in enumerate(list_P):\n",
    "                if P == -1: continue\n",
    "                val *= list_rhoi[(3*(i+c))+P]\n",
    "            assert(np.abs(val.imag) < 1e-7)\n",
    "            feat_vec.append(val.real)\n",
    "    return feat_vec\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML(all_states, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "        print(\"Pos:\", pos)\n",
    "\n",
    "        def twobytwo_to_Pauli(list_rhoi):\n",
    "            list_rhoi_new = []\n",
    "            for rhoi in list_rhoi:\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "            return list_rhoi_new\n",
    "\n",
    "        def get_RDM_in_Pauli(list_rhoi, k):\n",
    "            feat_vec = []\n",
    "            for i in range(N-k+1):\n",
    "                for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "                    val = 1.0\n",
    "                    for c, P in enumerate(list_P):\n",
    "                        if P == -1: continue\n",
    "                        val *= list_rhoi[(3*(i+c))+P]\n",
    "                    assert(np.abs(val.imag) < 1e-7)\n",
    "                    feat_vec.append(val.real)\n",
    "            return feat_vec\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_states)), range(len(all_states)), test_size=test_size, random_state=random_seed)\n",
    "\n",
    "        for k in [1, 2, 3, 4]:\n",
    "            print(\"Validate k =\", k)\n",
    "            X, y_true, y_noisy = [], [], []\n",
    "\n",
    "            for data in zip(all_states, all_values):\n",
    "                X.append(get_RDM_in_Pauli(data[0], k))\n",
    "                y_true.append(data[1][pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[1][pos]+1)/2, 1)[0] / 500) - 1)\n",
    "\n",
    "            X = np.array(X)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "                print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML_transformed(all_X_list, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "#         print(\"Pos:\", pos)\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_values)), range(len(all_values)), test_size=test_size, random_state=random_seed)\n",
    "        \n",
    "        for k in [1, 2]:\n",
    "#             print(\"Validate k =\", k)\n",
    "\n",
    "            X = all_X_list[k-1]\n",
    "            \n",
    "            y_true, y_noisy = [], []\n",
    "            for data in all_values:\n",
    "                y_true.append(data[pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[pos]+1)/2, 1)[0] / 500) - 1)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "#                 print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "def transform_states(all_states):\n",
    "    all_X_list = []\n",
    "    \n",
    "    for k in [1, 2]:\n",
    "        X = []\n",
    "        for data in all_states:\n",
    "            X.append(get_RDM_in_Pauli(data, k))\n",
    "        all_X_list.append(np.array(X))\n",
    "    return all_X_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de994661",
   "metadata": {},
   "source": [
    "# XY model with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f7e29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "sample_size = 100\n",
    "all_data_training_set_scaling = []\n",
    "seed = 0\n",
    "test_size = 0.5\n",
    "num_holdout = 1\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "\n",
    "# Train / holdout split\n",
    "np.random.seed(seed)\n",
    "sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "train_states, train_values = np.array(all_states)[\n",
    "    sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "\n",
    "# I setup the framework for multiple holdout but right now still should be the same as using \n",
    "# only one holdout in my current code below\n",
    "holdout_states_ens, holdout_values_ens = [],[]\n",
    "for i in range(1, num_holdout+1):\n",
    "    holdout_states, holdout_values = np.array(all_states)[\n",
    "        sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "    holdout_states_ens.append(holdout_states)\n",
    "    holdout_values_ens.append(holdout_values)\n",
    "h = holdout_states_ens[0]\n",
    "\n",
    "train_X_list = transform_states(train_states)\n",
    "holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a858eaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.15589832750540034 0.15131445874505367\n",
      "Training clf_holdout...\n",
      "Scores: 0.14870408289542514 0.13186634459285798\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Obtain classifiers\n",
    "print(\"Training clf_train...\")\n",
    "list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "    train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "clf_train = list_of_clf_train[0]\n",
    "k_train = list_of_bestk_train[0]\n",
    "\n",
    "print(\"Training clf_holdout...\")\n",
    "clf_holdout_ens = []\n",
    "score_holdout_ens = []\n",
    "k_holdout_ens = []\n",
    "for i in range(num_holdout):\n",
    "    list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "        holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "    clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "    score_holdout_ens.append(list_of_score_holdout[0])\n",
    "    k_holdout_ens.append(list_of_bestk_holdout[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf191108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# Adaptive optimization\n",
    "\n",
    "def predict_holdout(x):\n",
    "    # Get holdout prediction\n",
    "    predictions = []\n",
    "    for i in range(num_holdout):\n",
    "        # Get best classifier with best k\n",
    "        clf_holdout = clf_holdout_ens[i]\n",
    "        k_holdout = k_holdout_ens[i]\n",
    "        X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "        predictions.append(clf_holdout.predict(X_holdout))\n",
    "    return predictions[0][0]\n",
    "\n",
    "def predict_train(x):\n",
    "    # Get train prediction\n",
    "    X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "    return clf_train.predict(X_train)[0]\n",
    "\n",
    "def get_product_state(y):\n",
    "    # Compute product state from input spherical coordinates\n",
    "    x = []\n",
    "    for i in range(N):\n",
    "        phi = y[i]\n",
    "        theta = y[N+i]\n",
    "        x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "    return x\n",
    "\n",
    "def objective(y):\n",
    "    # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -abs(y_train - y_holdout)\n",
    "\n",
    "def objective_train(y):\n",
    "    # Maximizes train predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    return -y_train\n",
    "\n",
    "def objective_holdout(y):\n",
    "    x = get_product_state(y)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -y_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4de3f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print: max error, index of maximum error, average error\n",
      "step 0: 0, 0, 0\n",
      "step 500: 0.4951881640908899, 464, 0.005087798726805549\n",
      "step 1000: 0.4951881640908899, 464, 0.010071615595029875\n",
      "step 1500: 0.4951881640908899, 464, 0.01524219034929787\n",
      "step 2000: 0.4951881640908899, 464, 0.020318866205960136\n",
      "step 2500: 0.4951881640908899, 464, 0.025265158267649905\n",
      "step 3000: 0.4951881640908899, 464, 0.03015540066022744\n",
      "step 3500: 0.4951881640908899, 464, 0.03486641324922592\n",
      "step 4000: 0.4951881640908899, 464, 0.04010466205122014\n",
      "step 4500: 0.4951881640908899, 464, 0.045247843082349325\n",
      "step 5000: 0.4951881640908899, 464, 0.05007427816854948\n",
      "step 5500: 0.4951881640908899, 464, 0.055229517179141326\n",
      "step 6000: 0.4951881640908899, 464, 0.059876038650362964\n",
      "step 6500: 0.4951881640908899, 464, 0.0646856326902569\n",
      "step 7000: 0.4951881640908899, 464, 0.06974907444175946\n",
      "step 7500: 0.4951881640908899, 464, 0.07482053214946581\n",
      "step 8000: 0.4951881640908899, 464, 0.07961378663683426\n",
      "step 8500: 0.4951881640908899, 464, 0.08466828274936525\n",
      "step 9000: 0.4951881640908899, 464, 0.0895662591804719\n",
      "step 9500: 0.4951881640908899, 464, 0.09458832978831765\n",
      "0.4951881640908899\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "def objective_state(x):\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return abs(y_train - y_holdout)\n",
    "\n",
    "# max_err = 0\n",
    "# max_idx = 0\n",
    "# avg_err = 0\n",
    "# print(\"Print: max error, index of maximum error, average error\")\n",
    "# np.random.seed(seed)\n",
    "# for i in range(10000):\n",
    "#     y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "#     if i % 500 == 0:\n",
    "#         print(f\"step {i}: {max_err}, {max_idx}, {avg_err}\")\n",
    "#     err = -objective(y0)\n",
    "#     if err > max_err:\n",
    "#         max_err = err\n",
    "#         max_idx = i\n",
    "#     avg_err += err / 10000\n",
    "# print(max_err)\n",
    "\n",
    "# Sometimes I get something like this, i.e. classifier always outputs the same number\n",
    "# for i in range(10000):\n",
    "#     if i % 500 == 0:\n",
    "#         print(predict_train(all_states[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afcb904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          100     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  5.92987D-02    |proj g|=  1.14977D-01\n",
      "\n",
      "At iterate    1    f=  3.39586D-02    |proj g|=  1.11963D-01\n",
      "\n",
      "At iterate    2    f= -3.74996D-01    |proj g|=  9.45515D-02\n",
      "\n",
      "At iterate    3    f= -4.99690D-01    |proj g|=  9.48746D-02\n",
      "\n",
      "At iterate    4    f= -6.28045D-01    |proj g|=  6.82442D-02\n",
      "\n",
      "At iterate    5    f= -7.17937D-01    |proj g|=  5.89050D-02\n",
      "\n",
      "At iterate    6    f= -7.62007D-01    |proj g|=  4.52457D-02\n",
      "\n",
      "At iterate    7    f= -7.81765D-01    |proj g|=  2.95965D-02\n",
      "\n",
      "At iterate    8    f= -7.88011D-01    |proj g|=  5.12506D-03\n",
      "\n",
      "At iterate    9    f= -7.90809D-01    |proj g|=  4.99891D-03\n",
      "\n",
      "At iterate   10    f= -7.93893D-01    |proj g|=  7.14772D-03\n",
      "\n",
      "At iterate   11    f= -7.97325D-01    |proj g|=  8.22969D-03\n",
      "\n",
      "At iterate   12    f= -7.99556D-01    |proj g|=  4.26551D-03\n",
      "\n",
      "At iterate   13    f= -8.00612D-01    |proj g|=  1.92931D-03\n",
      "\n",
      "At iterate   14    f= -8.01203D-01    |proj g|=  1.65066D-03\n",
      "\n",
      "At iterate   15    f= -8.01820D-01    |proj g|=  2.15296D-03\n",
      "\n",
      "At iterate   16    f= -8.02061D-01    |proj g|=  4.93954D-03\n",
      "\n",
      "At iterate   17    f= -8.02491D-01    |proj g|=  7.76779D-04\n",
      "\n",
      "At iterate   18    f= -8.02543D-01    |proj g|=  4.45199D-04\n",
      "\n",
      "At iterate   19    f= -8.02586D-01    |proj g|=  4.20941D-04\n",
      "\n",
      "At iterate   20    f= -8.02618D-01    |proj g|=  2.34683D-03\n",
      "\n",
      "At iterate   21    f= -8.02670D-01    |proj g|=  3.86247D-04\n",
      "\n",
      "At iterate   22    f= -8.02689D-01    |proj g|=  3.96327D-04\n",
      "\n",
      "At iterate   23    f= -8.02730D-01    |proj g|=  4.67792D-04\n",
      "\n",
      "At iterate   24    f= -8.02818D-01    |proj g|=  9.41458D-04\n",
      "\n",
      "At iterate   25    f= -8.02925D-01    |proj g|=  3.81427D-03\n",
      "\n",
      "At iterate   26    f= -8.03093D-01    |proj g|=  1.33122D-03\n",
      "\n",
      "At iterate   27    f= -8.03393D-01    |proj g|=  1.24051D-03\n",
      "\n",
      "At iterate   28    f= -8.03584D-01    |proj g|=  1.27885D-03\n",
      "\n",
      "At iterate   29    f= -8.04063D-01    |proj g|=  1.59790D-03\n",
      "\n",
      "At iterate   30    f= -8.04267D-01    |proj g|=  3.96850D-03\n",
      "\n",
      "At iterate   31    f= -8.04530D-01    |proj g|=  1.10557D-03\n",
      "\n",
      "At iterate   32    f= -8.04659D-01    |proj g|=  1.00900D-03\n",
      "\n",
      "At iterate   33    f= -8.04736D-01    |proj g|=  5.05085D-04\n",
      "\n",
      "At iterate   34    f= -8.04770D-01    |proj g|=  6.09657D-04\n",
      "\n",
      "At iterate   35    f= -8.04780D-01    |proj g|=  1.11501D-03\n",
      "\n",
      "At iterate   36    f= -8.04797D-01    |proj g|=  2.96208D-04\n",
      "\n",
      "At iterate   37    f= -8.04806D-01    |proj g|=  2.32125D-04\n",
      "\n",
      "At iterate   38    f= -8.04814D-01    |proj g|=  2.54441D-04\n",
      "\n",
      "At iterate   39    f= -8.04819D-01    |proj g|=  3.44691D-04\n",
      "\n",
      "At iterate   40    f= -8.04821D-01    |proj g|=  1.23301D-04\n",
      "\n",
      "At iterate   41    f= -8.04822D-01    |proj g|=  7.96807D-05\n",
      "\n",
      "At iterate   42    f= -8.04823D-01    |proj g|=  7.79488D-05\n",
      "\n",
      "At iterate   43    f= -8.04824D-01    |proj g|=  1.00009D-04\n",
      "\n",
      "At iterate   44    f= -8.04824D-01    |proj g|=  1.87284D-04\n",
      "\n",
      "At iterate   45    f= -8.04824D-01    |proj g|=  3.71259D-05\n",
      "\n",
      "At iterate   46    f= -8.04825D-01    |proj g|=  2.60458D-05\n",
      "\n",
      "At iterate   47    f= -8.04825D-01    |proj g|=  1.93290D-05\n",
      "\n",
      "At iterate   48    f= -8.04825D-01    |proj g|=  2.77223D-05\n",
      "\n",
      "At iterate   49    f= -8.04825D-01    |proj g|=  2.85327D-05\n",
      "\n",
      "At iterate   50    f= -8.04825D-01    |proj g|=  1.80966D-05\n",
      "\n",
      "At iterate   51    f= -8.04825D-01    |proj g|=  2.37255D-05\n",
      "\n",
      "At iterate   52    f= -8.04825D-01    |proj g|=  9.94982D-05\n",
      "\n",
      "At iterate   53    f= -8.04825D-01    |proj g|=  1.91958D-05\n",
      "\n",
      "At iterate   54    f= -8.04825D-01    |proj g|=  2.35478D-05\n",
      "\n",
      "At iterate   55    f= -8.04825D-01    |proj g|=  4.08229D-05\n",
      "\n",
      "At iterate   56    f= -8.04825D-01    |proj g|=  6.32494D-05\n",
      "\n",
      "At iterate   57    f= -8.04826D-01    |proj g|=  1.12499D-04\n",
      "\n",
      "At iterate   58    f= -8.04826D-01    |proj g|=  5.90528D-05\n",
      "\n",
      "At iterate   59    f= -8.04826D-01    |proj g|=  5.39346D-05\n",
      "\n",
      "At iterate   60    f= -8.04827D-01    |proj g|=  5.80092D-05\n",
      "\n",
      "At iterate   61    f= -8.04827D-01    |proj g|=  9.30145D-05\n",
      "\n",
      "At iterate   62    f= -8.04828D-01    |proj g|=  2.20124D-04\n",
      "\n",
      "At iterate   63    f= -8.04829D-01    |proj g|=  1.26665D-04\n",
      "\n",
      "At iterate   64    f= -8.04829D-01    |proj g|=  4.99045D-05\n",
      "\n",
      "At iterate   65    f= -8.04830D-01    |proj g|=  5.32352D-05\n",
      "\n",
      "At iterate   66    f= -8.04830D-01    |proj g|=  2.15206D-04\n",
      "\n",
      "At iterate   67    f= -8.04831D-01    |proj g|=  5.88307D-05\n",
      "\n",
      "At iterate   68    f= -8.04831D-01    |proj g|=  5.47784D-05\n",
      "\n",
      "At iterate   69    f= -8.04831D-01    |proj g|=  9.47797D-05\n",
      "\n",
      "At iterate   70    f= -8.04832D-01    |proj g|=  1.35425D-04\n",
      "\n",
      "At iterate   71    f= -8.04834D-01    |proj g|=  2.04381D-04\n",
      "\n",
      "At iterate   72    f= -8.04836D-01    |proj g|=  1.50602D-04\n",
      "\n",
      "At iterate   73    f= -8.04837D-01    |proj g|=  1.57219D-04\n",
      "\n",
      "At iterate   74    f= -8.04840D-01    |proj g|=  1.06959D-04\n",
      "\n",
      "At iterate   75    f= -8.04841D-01    |proj g|=  4.07052D-04\n",
      "\n",
      "At iterate   76    f= -8.04843D-01    |proj g|=  1.72173D-04\n",
      "\n",
      "At iterate   77    f= -8.04845D-01    |proj g|=  1.25544D-04\n",
      "\n",
      "At iterate   78    f= -8.04845D-01    |proj g|=  8.81184D-05\n",
      "\n",
      "At iterate   79    f= -8.04845D-01    |proj g|=  1.50557D-04\n",
      "\n",
      "At iterate   80    f= -8.04846D-01    |proj g|=  3.88578D-05\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -0.8048457626469646\n",
      "        x: [ 1.527e+00  1.786e+00 ...  4.720e+00  1.571e+00]\n",
      "      nit: 85\n",
      "      jac: [ 5.551e-08  4.829e-06 ...  2.642e-06  2.498e-06]\n",
      "     nfev: 9393\n",
      "     njev: 93\n",
      " hess_inv: <100x100 LbfgsInvHessProduct with dtype=float64>\n",
      "\n",
      "At iterate   81    f= -8.04846D-01    |proj g|=  2.13496D-05\n",
      "\n",
      "At iterate   82    f= -8.04846D-01    |proj g|=  2.75113D-05\n",
      "\n",
      "At iterate   83    f= -8.04846D-01    |proj g|=  2.33702D-05\n",
      "\n",
      "At iterate   84    f= -8.04846D-01    |proj g|=  2.50799D-05\n",
      "\n",
      "At iterate   85    f= -8.04846D-01    |proj g|=  4.95159D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  100     85     93     90     0    27   4.952D-06  -8.048D-01\n",
      "  F = -0.80484576264696461     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize, fmin_powell\n",
    "\n",
    "# Nelder-mead\n",
    "np.random.seed(seed)\n",
    "bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N)\n",
    "y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "# res = minimize(objective_holdout, y0, method='Powell', bounds=bnds, options={'disp': True, 'maxfev':1000})\n",
    "# res = minimize(objective_holdout, y0, method='Nelder-Mead', bounds=bnds, options={'disp': True})\n",
    "res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective_train, y0, method='trust-constr', bounds=bnds, options={'disp': True})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8870497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8298405323227883\n",
      "9393\n"
     ]
    }
   ],
   "source": [
    "# Print the error, i.e. if large, then we have overfit\n",
    "state = get_product_state(res.x)\n",
    "y_train = predict_train(state)\n",
    "y_holdout = predict_holdout(state)\n",
    "print(abs(y_train-y_holdout))\n",
    "print(res.nfev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c760504",
   "metadata": {},
   "source": [
    "Repeat this for many initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdc9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.16462395921915518 0.12911599216449912\n",
      "Training clf_holdout...\n",
      "Scores: 0.14764604547061697 0.16193129089822098\n",
      "Scores: 0.11035031949378293 0.19426730647695842\n",
      "Scores: 0.14029271222417383 0.16890914639541876\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.05560000000000001\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.19956430481711274 0.21212317829059438\n",
      "Training clf_holdout...\n",
      "Scores: 0.07594106474910062 0.11924393029821691\n",
      "Scores: 0.09724905918564342 0.20073990480179035\n",
      "Scores: 0.15076365748214465 0.15274202201069817\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.06872858408844441\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.1809684234136905 0.18331344719082018\n",
      "Training clf_holdout...\n",
      "Scores: 0.1370288928713825 0.08149093179813323\n",
      "Scores: 0.11110876308125972 0.18837160930450686\n",
      "Scores: 0.14399075991753205 0.13012206381750874\n",
      "Number of iterations:  72\n",
      "Nonadaptive error:  0.32441840042401227\n",
      "Adaptive error:  0.2637897605349173\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.17431484046116774 0.11523387676883476\n",
      "Training clf_holdout...\n",
      "Scores: 0.051317217898127696 0.21315462794796292\n",
      "Scores: 0.1998027509412795 0.14643586338453438\n",
      "Scores: 0.08101728213311984 0.1390153680708002\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0748\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.09723765775405836 0.19657325743980222\n",
      "Training clf_holdout...\n",
      "Scores: 0.1851144396295481 0.1305014150048535\n",
      "Scores: 0.2432520134583625 0.13520498140924062\n",
      "Scores: 0.11007804613057412 0.16901755214525388\n",
      "Number of iterations:  21\n",
      "Nonadaptive error:  0.15081129031740528\n",
      "Adaptive error:  0.1297732269755647\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13092463860474782 0.15291098645096596\n",
      "Training clf_holdout...\n",
      "Scores: 0.0888366343106755 0.21156788552451633\n",
      "Scores: 0.11767025666529529 0.13804512401923585\n",
      "Scores: 0.13478124068080508 0.2682519098454715\n",
      "Number of iterations:  37\n",
      "Nonadaptive error:  0.3416451571240069\n",
      "Adaptive error:  0.2536628136772109\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10885345207292059 0.15632694534513636\n",
      "Training clf_holdout...\n",
      "Scores: 0.14399866931659647 0.2209167496234981\n",
      "Scores: 0.18487465456139968 0.2057697879751228\n",
      "Scores: 0.08553165707793658 0.2036356622475516\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.10419258059467568\n",
      "Adaptive error:  0.10478352260684624\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.10454798493891745 0.21761204899018355\n",
      "Training clf_holdout...\n",
      "Scores: 0.13714713785620902 0.16283414582157238\n",
      "Scores: 0.1648334702138926 0.16210092770412857\n",
      "Scores: 0.11375471784121365 0.2231071775254389\n",
      "Number of iterations:  30\n",
      "Nonadaptive error:  0.31074108067490425\n",
      "Adaptive error:  0.23387351352337724\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.07993774465615164 0.19623924058615289\n",
      "Training clf_holdout...\n",
      "Scores: 0.0814271530550291 0.18871151113166904\n",
      "Scores: 0.14730716388950715 0.1974157397068785\n",
      "Scores: 0.1934214514568358 0.18594896422269472\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0003999999999999889\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.2223790010678297 0.12862899809987552\n",
      "Training clf_holdout...\n",
      "Scores: 0.13976182577687676 0.1630147207906431\n",
      "Scores: 0.10268121204247103 0.14807072717687722\n",
      "Scores: 0.06393153674038238 0.16425144760988122\n",
      "Number of iterations:  37\n",
      "Nonadaptive error:  0.2969550004099182\n",
      "Adaptive error:  0.42342492541625526\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.16555487373386424 0.19823974160422003\n",
      "Training clf_holdout...\n",
      "Scores: 0.1118413108749438 0.14903290460608767\n",
      "Scores: 0.13350783228827173 0.1650864911534799\n",
      "Scores: 0.18585924600406892 0.11083283620875253\n",
      "Number of iterations:  25\n",
      "Nonadaptive error:  0.2342889076038589\n",
      "Adaptive error:  0.4297251272661792\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.15868136621833712 0.16731201614798036\n",
      "Training clf_holdout...\n",
      "Scores: 0.15661336623100658 0.15102561824306884\n",
      "Scores: 0.15063512726098077 0.14453386385800263\n",
      "Scores: 0.12729023695156078 0.14497966130103956\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0037333333333333324\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.1585116656344577 0.12984311981922222\n",
      "Training clf_holdout...\n",
      "Scores: 0.13460729864802584 0.13734930232379156\n",
      "Scores: 0.13040481220522843 0.15712273485122952\n",
      "Scores: 0.16884104215904153 0.15558038715673814\n",
      "Number of iterations:  24\n",
      "Nonadaptive error:  0.11856163963014157\n",
      "Adaptive error:  0.11951680930443229\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.17534392364327012 0.16825312283462426\n",
      "Training clf_holdout...\n",
      "Scores: 0.10706538659645712 0.1351723707656526\n",
      "Scores: 0.15368233572022233 0.12688873151937755\n",
      "Scores: 0.15045544118362378 0.14643020746734792\n",
      "Number of iterations:  50\n",
      "Nonadaptive error:  0.17750567430439437\n",
      "Adaptive error:  0.5939462215957803\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.13724276748900424 0.15608244031624455\n",
      "Training clf_holdout...\n",
      "Scores: 0.14039535320737206 0.15809072406160596\n",
      "Scores: 0.14129222770705138 0.10583699482860036\n",
      "Scores: 0.1329172374841088 0.15421122715484448\n",
      "Number of iterations:  7\n",
      "Nonadaptive error:  0.24004103086978007\n",
      "Adaptive error:  0.12193085750945079\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13119172706120452 0.16817718022893072\n",
      "Training clf_holdout...\n",
      "Scores: 0.15554452083796177 0.17656319739465642\n",
      "Scores: 0.17225214733145033 0.17099524657977325\n",
      "Scores: 0.14364747446038395 0.158248950349141\n",
      "Number of iterations:  80\n",
      "Nonadaptive error:  0.2677862513072913\n",
      "Adaptive error:  0.5365708465270985\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.17034777865109646 0.14938096391856556\n",
      "Training clf_holdout...\n",
      "Scores: 0.1469116772647764 0.14581596157275675\n",
      "Scores: 0.14354035325668915 0.17182130044585317\n",
      "Scores: 0.15439204131282586 0.1717913079270248\n",
      "Number of iterations:  23\n",
      "Nonadaptive error:  0.31489410268464785\n",
      "Adaptive error:  0.32110570953861184\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.14882146759640835 0.16904749777298336\n",
      "Training clf_holdout...\n",
      "Scores: 0.13980671731244587 0.18885061008513296\n",
      "Scores: 0.1758405192049345 0.19111930837333313\n",
      "Scores: 0.15804548121759412 0.16549919587751352\n",
      "Number of iterations:  20\n",
      "Nonadaptive error:  0.17509955453967346\n",
      "Adaptive error:  0.22176791173502877\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.1763851908878838 0.17566595600731733\n",
      "Training clf_holdout...\n",
      "Scores: 0.1433167387147508 0.18810492607929413\n",
      "Scores: 0.17857409798119017 0.14788425792569368\n",
      "Scores: 0.130572106062081 0.15716238484981304\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.06529297148430172\n",
      "Adaptive error:  0.035950381380529035\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.15701221682810723 0.1632618523761537\n",
      "Training clf_holdout...\n",
      "Scores: 0.14335227065643052 0.15369153702273264\n",
      "Scores: 0.13831217044502464 0.16330321460850958\n",
      "Scores: 0.1401730214157486 0.16209065121151872\n",
      "Number of iterations:  82\n",
      "Nonadaptive error:  0.25738871328065865\n",
      "Adaptive error:  0.5792196660601973\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.15589832750540034 0.15131445874505367\n",
      "Training clf_holdout...\n",
      "Scores: 0.14514413513615765 0.13080919227578866\n",
      "Scores: 0.1283507680460788 0.1648520420178351\n",
      "Scores: 0.1403725819757261 0.14943620492232795\n",
      "Number of iterations:  85\n",
      "Nonadaptive error:  0.32962744827630214\n",
      "Adaptive error:  0.7564223136711901\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.1375474375001225 0.13718042207371414\n",
      "Training clf_holdout...\n",
      "Scores: 0.13373796905727503 0.15512721567614784\n",
      "Scores: 0.12758210183416355 0.136595033814487\n",
      "Scores: 0.12590717024106823 0.13122828552688684\n",
      "Number of iterations:  40\n",
      "Nonadaptive error:  0.19298498811615852\n",
      "Adaptive error:  0.5302104859336618\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.1478818171210039 0.1286202090263973\n",
      "Training clf_holdout...\n",
      "Scores: 0.12686904938313773 0.1516255845956894\n",
      "Scores: 0.12927639483844452 0.1431860168866083\n",
      "Scores: 0.15139747145576377 0.15707955728721187\n",
      "Number of iterations:  9\n",
      "Nonadaptive error:  0.18003640377759306\n",
      "Adaptive error:  0.08113217126486294\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.1469757334171944 0.1318439025453525\n",
      "Training clf_holdout...\n",
      "Scores: 0.1347506892678368 0.13985006564483538\n",
      "Scores: 0.15947713055378263 0.13906217972862855\n",
      "Scores: 0.14481881360420545 0.14581885688450733\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.03281648618256306\n",
      "Adaptive error:  0.051365071449258067\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.14391429906234265 0.13438062574987794\n",
      "Training clf_holdout...\n",
      "Scores: 0.12967933267650486 0.1605641420923032\n",
      "Scores: 0.16547095541309825 0.14760987373563722\n",
      "Scores: 0.1251386183241014 0.12474883763269355\n",
      "Number of iterations:  26\n",
      "Nonadaptive error:  0.27125248876097124\n",
      "Adaptive error:  0.5970490667642209\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.15723430000564598 0.1573196732709076\n",
      "Training clf_holdout...\n",
      "Scores: 0.15270469725229763 0.15629572565885885\n",
      "Scores: 0.12509049078695034 0.11355712906039456\n",
      "Scores: 0.1670488263332937 0.1373917803091153\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.05173632405159938\n",
      "Adaptive error:  0.0016475828363142486\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.1449783577132056 0.1415492677495508\n",
      "Training clf_holdout...\n",
      "Scores: 0.1561654415406819 0.15180005144240402\n",
      "Scores: 0.13802900589590883 0.18621274413614133\n",
      "Scores: 0.12820176868522362 0.16316350467103705\n",
      "Number of iterations:  14\n",
      "Nonadaptive error:  0.24463739946383567\n",
      "Adaptive error:  0.11949996100805886\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.15160340838836717 0.13183852544736566\n",
      "Training clf_holdout...\n",
      "Scores: 0.16225806581496027 0.1362732367229744\n",
      "Scores: 0.13016510818831745 0.15223762848380054\n",
      "Scores: 0.11562201020654479 0.1634506655590547\n",
      "Number of iterations:  47\n",
      "Nonadaptive error:  0.30785301521054864\n",
      "Adaptive error:  0.498516706233482\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.16368886155289333 0.14689167492773003\n",
      "Training clf_holdout...\n",
      "Scores: 0.14831378587151633 0.16735593744718644\n",
      "Scores: 0.1267819188878626 0.13232784821905746\n",
      "Scores: 0.13062429574032203 0.1347153812447341\n",
      "Number of iterations:  31\n",
      "Nonadaptive error:  0.18937438827276462\n",
      "Adaptive error:  0.3587003668551263\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.15715381656013097 0.15135445472044126\n",
      "Training clf_holdout...\n",
      "Scores: 0.13968030007528315 0.1279161515485507\n",
      "Scores: 0.14614568283782658 0.14052229730479876\n",
      "Scores: 0.11241152660776302 0.18972130499795503\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.046149308591448404\n",
      "Adaptive error:  0.042923172279755575\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.1479387347802868 0.16646960747008344\n",
      "Training clf_holdout...\n",
      "Scores: 0.15702641873259154 0.12105444497613309\n",
      "Scores: 0.14366698680855275 0.13520366848825394\n",
      "Scores: 0.1581334670911239 0.12243546659899118\n",
      "Number of iterations:  119\n",
      "Nonadaptive error:  0.24616102558865868\n",
      "Adaptive error:  0.6740100991863545\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.14913293626441548 0.16838420691133435\n",
      "Training clf_holdout...\n",
      "Scores: 0.13642437326231535 0.12854628364917403\n",
      "Scores: 0.13888780373740986 0.1448639139607413\n",
      "Scores: 0.1422029652166019 0.16645888455050395\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.02576841776679041\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.14632658519690556 0.14752196758942016\n",
      "Training clf_holdout...\n",
      "Scores: 0.15167608665146395 0.1277952052828876\n",
      "Scores: 0.14671721463049492 0.11878319207190018\n",
      "Scores: 0.12643704523297333 0.14092446485546123\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.027611280078237536\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.11241439648038853 0.1384260872995269\n",
      "Training clf_holdout...\n",
      "Scores: 0.165763828047067 0.12862709389497837\n",
      "Scores: 0.13940615388105432 0.12496581913390357\n",
      "Scores: 0.1481164167972341 0.15260211381842392\n",
      "Number of iterations:  39\n",
      "Nonadaptive error:  0.13245350005123968\n",
      "Adaptive error:  0.17257854736777795\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.13185281674644517 0.11730648540849947\n",
      "Training clf_holdout...\n",
      "Scores: 0.13853454500616194 0.1412163956344648\n",
      "Scores: 0.12083100965298359 0.12826784363323418\n",
      "Scores: 0.1485059839302159 0.1294407639366643\n",
      "Number of iterations:  65\n",
      "Nonadaptive error:  0.26636895537908856\n",
      "Adaptive error:  0.9005958356271613\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 5\n",
      "Training clf_train...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Obtain classifiers\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining clf_train...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m list_of_score_train, list_of_clf_train, list_of_bestk_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sparse_ML_transformed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_X_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m clf_train \u001b[38;5;241m=\u001b[39m list_of_clf_train[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m k_train \u001b[38;5;241m=\u001b[39m list_of_bestk_train[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[1], line 191\u001b[0m, in \u001b[0;36mtrain_sparse_ML_transformed\u001b[0;34m(all_X_list, all_values, test_size, random_seed)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;66;03m# ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m15\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m14\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m12\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m11\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)]:\n\u001b[0;32m--> 191\u001b[0m                 score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mML_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneg_root_mean_squared_error\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m#                 print(score)\u001b[39;00m\n\u001b[1;32m    193\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m best_cv_score \u001b[38;5;241m>\u001b[39m score:\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:1004\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m     this_Xy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m _, this_coef, this_dual_gap, this_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43malphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthis_Xy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# from here on **params\u001b[39;49;00m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m coef_[k] \u001b[38;5;241m=\u001b[39m this_coef[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1029\u001b[0m dual_gaps_[k] \u001b[38;5;241m=\u001b[39m this_dual_gap[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631\u001b[0m, in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    617\u001b[0m     model \u001b[38;5;241m=\u001b[39m cd_fast\u001b[38;5;241m.\u001b[39menet_coordinate_descent_gram(\n\u001b[1;32m    618\u001b[0m         coef_,\n\u001b[1;32m    619\u001b[0m         l1_reg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         positive,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m precompute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcd_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menet_coordinate_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecompute should be one of True, False, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or array-like. Got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;241m%\u001b[39m precompute\n\u001b[1;32m    638\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statistics\n",
    "from scipy.optimize import minimize\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "sample_size_list = [20, 60, 100, 150, 200, 300, 500]\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,len(sample_size_list)))\n",
    "\n",
    "for s, sample_size in enumerate(sample_size_list):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='Nelder-Mead', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_iter):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413af529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 300, 500])\n",
    "df_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "c46b1283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>60</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>600</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.195436</td>\n",
       "      <td>0.426795</td>\n",
       "      <td>0.040752</td>\n",
       "      <td>-0.011818</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068729</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>0.337225</td>\n",
       "      <td>0.508477</td>\n",
       "      <td>0.120843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.060629</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>-0.098904</td>\n",
       "      <td>0.357554</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.416441</td>\n",
       "      <td>0.018549</td>\n",
       "      <td>-0.042219</td>\n",
       "      <td>-0.096620</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.021038</td>\n",
       "      <td>-0.118110</td>\n",
       "      <td>0.325797</td>\n",
       "      <td>-0.133662</td>\n",
       "      <td>-0.075769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.087982</td>\n",
       "      <td>0.268785</td>\n",
       "      <td>-0.050089</td>\n",
       "      <td>-0.075521</td>\n",
       "      <td>0.059733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>-0.125137</td>\n",
       "      <td>0.139078</td>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.076868</td>\n",
       "      <td>0.046668</td>\n",
       "      <td>0.190664</td>\n",
       "      <td>-0.017018</td>\n",
       "      <td>0.050716</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000400</td>\n",
       "      <td>-0.029343</td>\n",
       "      <td>0.169326</td>\n",
       "      <td>0.466005</td>\n",
       "      <td>-0.078977</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.126470</td>\n",
       "      <td>0.321831</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>0.462680</td>\n",
       "      <td>-0.028243</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       20        60        100       200       600   2000\n",
       "0  0.055600  0.195436  0.426795  0.040752 -0.011818   0.0\n",
       "1  0.068729  0.003733  0.337225  0.508477  0.120843   0.0\n",
       "2 -0.060629  0.000955 -0.098904  0.357554  0.004261   0.0\n",
       "3  0.074800  0.416441  0.018549 -0.042219 -0.096620   0.0\n",
       "4 -0.021038 -0.118110  0.325797 -0.133662 -0.075769   0.0\n",
       "5 -0.087982  0.268785 -0.050089 -0.075521  0.059733   0.0\n",
       "6  0.000591  0.006212 -0.125137  0.139078  0.014373   0.0\n",
       "7 -0.076868  0.046668  0.190664 -0.017018  0.050716   0.0\n",
       "8  0.000400 -0.029343  0.169326  0.466005 -0.078977   0.0\n",
       "9  0.126470  0.321831 -0.003226  0.462680 -0.028243   0.0"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b10e7a",
   "metadata": {},
   "source": [
    "# Ising with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.09148684550355596 0.10356823968380986\n",
      "Training clf_holdout...\n",
      "Scores: 0.09000554335199401 0.12394863867265815\n",
      "Scores: 0.15886182714772337 0.24736171195576845\n",
      "Scores: 0.1036874941148683 0.12195346956958858\n",
      "Number of iterations:  21\n",
      "Nonadaptive error:  0.1322223671244211\n",
      "Adaptive error:  0.27261770293731835\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.09337342602775593 0.15645991895709516\n",
      "Training clf_holdout...\n",
      "Scores: 0.13984585663163449 0.09709296278668628\n",
      "Scores: 0.11949184631430475 0.14147948750380598\n",
      "Scores: 0.0403799567443455 0.10935942101906534\n",
      "Number of iterations:  44\n",
      "Nonadaptive error:  0.25233303465688395\n",
      "Adaptive error:  0.3395264744682905\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.059231291468772915 0.10853695515858641\n",
      "Training clf_holdout...\n",
      "Scores: 0.09176398906636432 0.11235641897605583\n",
      "Scores: 0.07152507526028266 0.11090388999871681\n",
      "Scores: 0.07874880673129955 0.1146075230366167\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.012605161675882801\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.08588242843820093 0.14510654161039563\n",
      "Training clf_holdout...\n",
      "Scores: 0.1122556481989665 0.0935396501193689\n",
      "Scores: 0.09171517665564939 0.0879542224663255\n",
      "Scores: 0.11525397076157251 0.0969568072825369\n",
      "Number of iterations:  15\n",
      "Nonadaptive error:  0.11479136849201298\n",
      "Adaptive error:  0.179514444703534\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.050471083116058 0.1308559188856007\n",
      "Training clf_holdout...\n",
      "Scores: 0.07579890868531955 0.13806915571807743\n",
      "Scores: 0.09244385692458185 0.1202507801091158\n",
      "Scores: 0.15350915513894667 0.07821276187440057\n",
      "Number of iterations:  42\n",
      "Nonadaptive error:  0.19264473577817337\n",
      "Adaptive error:  0.10471823767392549\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13671525303359724 0.14405030531659763\n",
      "Training clf_holdout...\n",
      "Scores: 0.13041647963735414 0.14719475819671132\n",
      "Scores: 0.06913493970452639 0.12201127000533701\n",
      "Scores: 0.12927177524282363 0.15782109380677864\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.07719999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.08015098769635855 0.11595837655594894\n",
      "Training clf_holdout...\n",
      "Scores: 0.07895056792315605 0.1313630962252679\n",
      "Scores: 0.07328760457011277 0.14756465583701983\n",
      "Scores: 0.1463174983485927 0.11502348257217203\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.06200000000000001\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.09025844193878223 0.17295268920343215\n",
      "Training clf_holdout...\n",
      "Scores: 0.10501606584265533 0.0935818841949276\n",
      "Scores: 0.10769381774978487 0.10999019924439636\n",
      "Scores: 0.11716828440328374 0.08296429483597308\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.04159999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.06930376077898182 0.1632568671947783\n",
      "Training clf_holdout...\n",
      "Scores: 0.1045343346220043 0.12610897962991227\n",
      "Scores: 0.12812062523023254 0.1570024740918354\n",
      "Scores: 0.12110961750767774 0.15775456339152602\n",
      "Number of iterations:  95\n",
      "Nonadaptive error:  0.23050215960879486\n",
      "Adaptive error:  0.2503928654072433\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.10303418916346178 0.13647010758143632\n",
      "Training clf_holdout...\n",
      "Scores: 0.10281567637125265 0.1326768120821945\n",
      "Scores: 0.11335436088875092 0.07951310434206464\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "\n",
    "# Ising model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,6))\n",
    "\n",
    "for s, sample_size in enumerate([20, 60, 100, 150, 200, 500]):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_iter):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ising_homog = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 500])\n",
    "df_ising_homog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa94024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp_qml",
   "language": "python",
   "name": "imp_qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
