{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24434da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import ast\n",
    "import math\n",
    "import scipy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sigI = np.array([[1.0, 0.0j], [0.0j, 1.0]])\n",
    "sigX = np.array([[0.0j, 1.0], [1.0, 0.0j]])\n",
    "sigY = np.array([[0.0j, -1.0j], [1.0j, 0.0j]])\n",
    "sigZ = np.array([[1.0, 0.0j], [0.0j, -1.0]])\n",
    "\n",
    "N = 50\n",
    "\n",
    "def kron(ls):\n",
    "    A = ls[0]\n",
    "    for X in ls[1:]:\n",
    "        A = np.kron(A, X)\n",
    "    return A\n",
    "\n",
    "def generate_all_zero_state():\n",
    "    return [np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_one_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) for i in range(N)]\n",
    "\n",
    "def generate_half_half_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i < N/2 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_neel_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i % 2 == 0 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_plus_state():\n",
    "    return [np.array([[0.5, 0.5], [0.5, 0.5+0.0j]]) for i in range(N)]\n",
    "\n",
    "def generate_random_product_state():\n",
    "    list_rhoi = []\n",
    "    for i in range(N):\n",
    "        v = np.random.normal(size=3)\n",
    "        v /= np.linalg.norm(v)\n",
    "        rhoi = sigI / 2.0 + (v[0] * sigX / 2.0) + (v[1] * sigY / 2.0) + (v[2] * sigZ / 2.0)\n",
    "        list_rhoi.append(rhoi)\n",
    "    return list_rhoi\n",
    "\n",
    "def twobytwo_to_Pauli(list_rhoi):\n",
    "    list_rhoi_new = []\n",
    "    for rhoi in list_rhoi:\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "    return list_rhoi_new\n",
    "\n",
    "def get_RDM_in_Pauli(list_rhoi, k):\n",
    "    feat_vec = []\n",
    "    for i in range(N-k+1):\n",
    "        for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "            val = 1.0\n",
    "            for c, P in enumerate(list_P):\n",
    "                if P == -1: continue\n",
    "                val *= list_rhoi[(3*(i+c))+P]\n",
    "            assert(np.abs(val.imag) < 1e-7)\n",
    "            feat_vec.append(val.real)\n",
    "    return feat_vec\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML(all_states, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "        print(\"Pos:\", pos)\n",
    "\n",
    "        def twobytwo_to_Pauli(list_rhoi):\n",
    "            list_rhoi_new = []\n",
    "            for rhoi in list_rhoi:\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "            return list_rhoi_new\n",
    "\n",
    "        def get_RDM_in_Pauli(list_rhoi, k):\n",
    "            feat_vec = []\n",
    "            for i in range(N-k+1):\n",
    "                for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "                    val = 1.0\n",
    "                    for c, P in enumerate(list_P):\n",
    "                        if P == -1: continue\n",
    "                        val *= list_rhoi[(3*(i+c))+P]\n",
    "                    assert(np.abs(val.imag) < 1e-7)\n",
    "                    feat_vec.append(val.real)\n",
    "            return feat_vec\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_states)), range(len(all_states)), test_size=test_size, random_state=random_seed)\n",
    "\n",
    "        for k in [1, 2, 3, 4]:\n",
    "            print(\"Validate k =\", k)\n",
    "            X, y_true, y_noisy = [], [], []\n",
    "\n",
    "            for data in zip(all_states, all_values):\n",
    "                X.append(get_RDM_in_Pauli(data[0], k))\n",
    "                y_true.append(data[1][pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[1][pos]+1)/2, 1)[0] / 500) - 1)\n",
    "\n",
    "            X = np.array(X)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "                print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML_transformed(all_X_list, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "#         print(\"Pos:\", pos)\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_values)), range(len(all_values)), test_size=test_size, random_state=random_seed)\n",
    "        \n",
    "        for k in [1, 2, 3, 4]:\n",
    "#             print(\"Validate k =\", k)\n",
    "\n",
    "            X = all_X_list[k-1]\n",
    "            \n",
    "            y_true, y_noisy = [], []\n",
    "            for data in all_values:\n",
    "                y_true.append(data[pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[pos]+1)/2, 1)[0] / 500) - 1)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "#                 print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "def transform_states(all_states):\n",
    "    all_X_list = []\n",
    "    \n",
    "    for k in [1, 2, 3, 4]:\n",
    "        X = []\n",
    "        for data in all_states:\n",
    "            X.append(get_RDM_in_Pauli(data, k))\n",
    "        all_X_list.append(np.array(X))\n",
    "    return all_X_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de994661",
   "metadata": {},
   "source": [
    "# XY model with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "8f7e29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "sample_size = 100\n",
    "all_data_training_set_scaling = []\n",
    "seed = 0\n",
    "test_size = 0.5\n",
    "num_holdout = 1\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "\n",
    "# Train / holdout split\n",
    "np.random.seed(seed)\n",
    "sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "train_states, train_values = np.array(all_states)[\n",
    "    sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "\n",
    "# I setup the framework for multiple holdout but right now still should be the same as using \n",
    "# only one holdout in my current code below\n",
    "holdout_states_ens, holdout_values_ens = [],[]\n",
    "for i in range(1, num_holdout+1):\n",
    "    holdout_states, holdout_values = np.array(all_states)[\n",
    "        sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "    holdout_states_ens.append(holdout_states)\n",
    "    holdout_values_ens.append(holdout_values)\n",
    "h = holdout_states_ens[0]\n",
    "\n",
    "train_X_list = transform_states(train_states)\n",
    "holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "08e332a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.1809684234136905 0.18331344719082018\n",
      "Training clf_holdout...\n",
      "Scores: 0.1370288928713825 0.08149093179813323\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Obtain classifiers\n",
    "print(\"Training clf_train...\")\n",
    "list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "    train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "clf_train = list_of_clf_train[0]\n",
    "k_train = list_of_bestk_train[0]\n",
    "\n",
    "print(\"Training clf_holdout...\")\n",
    "clf_holdout_ens = []\n",
    "score_holdout_ens = []\n",
    "k_holdout_ens = []\n",
    "for i in range(num_holdout):\n",
    "    list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "        holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "    clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "    score_holdout_ens.append(list_of_score_holdout[0])\n",
    "    k_holdout_ens.append(list_of_bestk_holdout[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "7c792d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# Adaptive optimization\n",
    "\n",
    "def predict_holdout(x):\n",
    "    # Get holdout prediction\n",
    "    predictions = []\n",
    "    for i in range(num_holdout):\n",
    "        # Get best classifier with best k\n",
    "        clf_holdout = clf_holdout_ens[i]\n",
    "        k_holdout = k_holdout_ens[i]\n",
    "        X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "        predictions.append(clf_holdout.predict(X_holdout))\n",
    "    return predictions[0][0]\n",
    "\n",
    "def predict_train(x):\n",
    "    # Get train prediction\n",
    "    X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "    return clf_train.predict(X_train)[0]\n",
    "\n",
    "def get_product_state(y):\n",
    "    # Compute product state from input spherical coordinates\n",
    "    x = []\n",
    "    for i in range(N):\n",
    "        phi = y[i]\n",
    "        theta = y[N+i]\n",
    "        x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "    return x\n",
    "\n",
    "def objective(y):\n",
    "    # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -abs(y_train - y_holdout)\n",
    "\n",
    "def objective_train(y):\n",
    "    # Maximizes train predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    return -y_train\n",
    "\n",
    "def objective_holdout(y):\n",
    "    x = get_product_state(y)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -y_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "8b3bb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "def objective_state(x):\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return abs(y_train - y_holdout)\n",
    "\n",
    "# max_err = 0\n",
    "# max_idx = 0\n",
    "# avg_err = 0\n",
    "# print(\"Print: max error, index of maximum error, average error\")\n",
    "# np.random.seed(seed)\n",
    "# for i in range(100):\n",
    "#     y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "#     if i % 500 == 0:\n",
    "#         print(f\"step {i}: {max_err}, {max_idx}, {avg_err}\")\n",
    "#     err = -objective(y0)\n",
    "#     if err > max_err:\n",
    "#         max_err = err\n",
    "#         max_idx = i\n",
    "#     avg_err += err / 10000\n",
    "# print(max_err)\n",
    "\n",
    "# Sometimes I get something like this, i.e. classifier always outputs the same number\n",
    "# for i in range(10000):\n",
    "#     if i % 500 == 0:\n",
    "#         print(predict_train(all_states[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "e3532f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          100     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.04176D-02    |proj g|=  8.14497D-02\n",
      "\n",
      "At iterate    1    f=  4.49316D-02    |proj g|=  7.17918D-02\n",
      "\n",
      "At iterate    2    f= -1.66070D-01    |proj g|=  4.45008D-02\n",
      "\n",
      "At iterate    3    f= -1.87221D-01    |proj g|=  3.03842D-02\n",
      "\n",
      "At iterate    4    f= -1.94897D-01    |proj g|=  6.67487D-03\n",
      "\n",
      "At iterate    5    f= -1.98043D-01    |proj g|=  8.17441D-03\n",
      "\n",
      "At iterate    6    f= -2.14766D-01    |proj g|=  1.19964D-02\n",
      "\n",
      "At iterate    7    f= -2.21893D-01    |proj g|=  1.08330D-02\n",
      "\n",
      "At iterate    8    f= -2.24519D-01    |proj g|=  3.40317D-03\n",
      "\n",
      "At iterate    9    f= -2.25831D-01    |proj g|=  5.57586D-03\n",
      "\n",
      "At iterate   10    f= -2.27506D-01    |proj g|=  6.91223D-03\n",
      "\n",
      "At iterate   11    f= -2.29020D-01    |proj g|=  4.39133D-03\n",
      "\n",
      "At iterate   12    f= -2.29883D-01    |proj g|=  1.36219D-03\n",
      "\n",
      "At iterate   13    f= -2.30070D-01    |proj g|=  6.54837D-04\n",
      "\n",
      "At iterate   14    f= -2.30124D-01    |proj g|=  6.88771D-04\n",
      "\n",
      "At iterate   15    f= -2.30136D-01    |proj g|=  1.22871D-03\n",
      "\n",
      "At iterate   16    f= -2.30150D-01    |proj g|=  3.07848D-04\n",
      "\n",
      "At iterate   17    f= -2.30155D-01    |proj g|=  3.72247D-04\n",
      "\n",
      "At iterate   18    f= -2.30174D-01    |proj g|=  9.16101D-04\n",
      "\n",
      "At iterate   19    f= -2.30225D-01    |proj g|=  1.61875D-03\n",
      "\n",
      "At iterate   20    f= -2.31399D-01    |proj g|=  6.38500D-03\n",
      "  ys=-8.750E-04  -gs= 4.670E-04 BFGS update SKIPPED\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   21    f= -2.31782D-01    |proj g|=  5.08168D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   22    f= -2.32876D-01    |proj g|=  5.68266D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   23    f= -2.33293D-01    |proj g|=  4.74369D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   24    f= -2.34484D-01    |proj g|=  6.25546D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   25    f= -2.34896D-01    |proj g|=  4.83271D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   26    f= -2.35812D-01    |proj g|=  8.02018D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   27    f= -2.36512D-01    |proj g|=  4.58321D-03\n",
      "\n",
      "At iterate   28    f= -2.39490D-01    |proj g|=  2.24085D-03\n",
      "\n",
      "At iterate   29    f= -2.39744D-01    |proj g|=  2.14533D-03\n",
      "\n",
      "At iterate   30    f= -2.40062D-01    |proj g|=  8.35387D-04\n",
      "\n",
      "At iterate   31    f= -2.40191D-01    |proj g|=  7.37066D-04\n",
      "\n",
      "At iterate   32    f= -2.40197D-01    |proj g|=  1.89215D-04\n",
      "\n",
      "At iterate   33    f= -2.40198D-01    |proj g|=  1.89615D-04\n",
      "\n",
      "At iterate   34    f= -2.40206D-01    |proj g|=  1.90514D-04\n",
      "\n",
      "At iterate   35    f= -2.40216D-01    |proj g|=  1.91769D-04\n",
      "\n",
      "At iterate   36    f= -2.40234D-01    |proj g|=  2.71649D-04\n",
      "\n",
      "At iterate   37    f= -2.40273D-01    |proj g|=  5.93647D-04\n",
      "\n",
      "At iterate   38    f= -2.40323D-01    |proj g|=  5.46396D-04\n",
      "\n",
      "At iterate   39    f= -2.40410D-01    |proj g|=  1.18487D-03\n",
      "\n",
      "At iterate   40    f= -2.40432D-01    |proj g|=  1.10832D-03\n",
      "\n",
      "At iterate   41    f= -2.40474D-01    |proj g|=  4.00302D-04\n",
      "\n",
      "At iterate   42    f= -2.40491D-01    |proj g|=  2.29827D-04\n",
      "\n",
      "At iterate   43    f= -2.40501D-01    |proj g|=  2.14095D-04\n",
      "\n",
      "At iterate   44    f= -2.40503D-01    |proj g|=  8.46734D-04\n",
      "\n",
      "At iterate   45    f= -2.40512D-01    |proj g|=  1.89038D-04\n",
      "\n",
      "At iterate   46    f= -2.40515D-01    |proj g|=  1.87350D-04\n",
      "\n",
      "At iterate   47    f= -2.40520D-01    |proj g|=  1.79684D-04\n",
      "\n",
      "At iterate   48    f= -2.40532D-01    |proj g|=  2.99227D-04\n",
      "\n",
      "At iterate   49    f= -2.40550D-01    |proj g|=  6.55154D-04\n",
      "\n",
      "At iterate   50    f= -2.40559D-01    |proj g|=  3.85159D-04\n",
      "\n",
      "At iterate   51    f= -2.40571D-01    |proj g|=  1.67394D-04\n",
      "\n",
      "At iterate   52    f= -2.40575D-01    |proj g|=  1.33321D-04\n",
      "\n",
      "At iterate   53    f= -2.40576D-01    |proj g|=  2.23532D-04\n",
      "\n",
      "At iterate   54    f= -2.40576D-01    |proj g|=  1.29330D-04\n",
      "\n",
      "At iterate   55    f= -2.40577D-01    |proj g|=  1.91402D-05\n",
      "\n",
      "At iterate   56    f= -2.40577D-01    |proj g|=  3.22131D-05\n",
      "\n",
      "At iterate   57    f= -2.40577D-01    |proj g|=  2.83329D-05\n",
      "\n",
      "At iterate   58    f= -2.40577D-01    |proj g|=  1.45051D-05\n",
      "\n",
      "At iterate   59    f= -2.40577D-01    |proj g|=  1.44162D-05\n",
      "\n",
      "At iterate   60    f= -2.40577D-01    |proj g|=  1.64313D-05\n",
      "\n",
      "At iterate   61    f= -2.40578D-01    |proj g|=  2.75502D-05\n",
      "\n",
      "At iterate   62    f= -2.40578D-01    |proj g|=  3.77476D-05\n",
      "\n",
      "At iterate   63    f= -2.40578D-01    |proj g|=  9.30978D-05\n",
      "\n",
      "At iterate   64    f= -2.40578D-01    |proj g|=  4.55747D-05\n",
      "\n",
      "At iterate   65    f= -2.40579D-01    |proj g|=  3.84026D-05\n",
      "\n",
      "At iterate   66    f= -2.40579D-01    |proj g|=  1.07603D-04\n",
      "\n",
      "At iterate   67    f= -2.40579D-01    |proj g|=  3.10640D-05\n",
      "\n",
      "At iterate   68    f= -2.40579D-01    |proj g|=  6.45040D-05\n",
      "\n",
      "At iterate   69    f= -2.40580D-01    |proj g|=  4.52027D-05\n",
      "\n",
      "At iterate   70    f= -2.40580D-01    |proj g|=  7.42684D-05\n",
      "\n",
      "At iterate   71    f= -2.40580D-01    |proj g|=  1.92180D-05\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -0.24057966646879983\n",
      "        x: [ 0.000e+00  0.000e+00 ...  3.142e+00  5.540e+00]\n",
      "      nit: 72\n",
      "      jac: [ 0.000e+00  4.083e-02 ...  4.974e-06  0.000e+00]\n",
      "     nfev: 8282\n",
      "     njev: 82\n",
      " hess_inv: <100x100 LbfgsInvHessProduct with dtype=float64>\n",
      "\n",
      "At iterate   72    f= -2.40580D-01    |proj g|=  6.83897D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  100     72     82     75     1    14   6.839D-06  -2.406D-01\n",
      "  F = -0.24057966646879983     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Nelder-mead\n",
    "np.random.seed(seed)\n",
    "bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N)\n",
    "y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "# res = minimize(objective_train, y0, method='Powell', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective, y0, method='Nelder-Mead', bounds=bnds, options={'disp': True})\n",
    "res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective_train, y0, method='trust-constr', bounds=bnds, options={'disp': True})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "800fd199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2741796664687998"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the error, i.e. if large, then we have overfit\n",
    "state = get_product_state(res.x)\n",
    "y_train = predict_train(state)\n",
    "y_holdout = predict_holdout(state)\n",
    "abs(y_train-y_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd82e34",
   "metadata": {},
   "source": [
    "Repeat this for many initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac839818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statistics\n",
    "from scipy.optimize import minimize\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "sample_size_list = [20, 60, 100, 150, 200, 300, 500]\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,len(sample_size_list)))\n",
    "\n",
    "for s, sample_size in enumerate(sample_size_list):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_iter):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc458c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 300, 500])\n",
    "df_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "4fa07afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>60</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>600</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.195436</td>\n",
       "      <td>0.426795</td>\n",
       "      <td>0.040752</td>\n",
       "      <td>-0.011818</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068729</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>0.337225</td>\n",
       "      <td>0.508477</td>\n",
       "      <td>0.120843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.060629</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>-0.098904</td>\n",
       "      <td>0.357554</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.416441</td>\n",
       "      <td>0.018549</td>\n",
       "      <td>-0.042219</td>\n",
       "      <td>-0.096620</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.021038</td>\n",
       "      <td>-0.118110</td>\n",
       "      <td>0.325797</td>\n",
       "      <td>-0.133662</td>\n",
       "      <td>-0.075769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.087982</td>\n",
       "      <td>0.268785</td>\n",
       "      <td>-0.050089</td>\n",
       "      <td>-0.075521</td>\n",
       "      <td>0.059733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>-0.125137</td>\n",
       "      <td>0.139078</td>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.076868</td>\n",
       "      <td>0.046668</td>\n",
       "      <td>0.190664</td>\n",
       "      <td>-0.017018</td>\n",
       "      <td>0.050716</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000400</td>\n",
       "      <td>-0.029343</td>\n",
       "      <td>0.169326</td>\n",
       "      <td>0.466005</td>\n",
       "      <td>-0.078977</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.126470</td>\n",
       "      <td>0.321831</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>0.462680</td>\n",
       "      <td>-0.028243</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       20        60        100       200       600   2000\n",
       "0  0.055600  0.195436  0.426795  0.040752 -0.011818   0.0\n",
       "1  0.068729  0.003733  0.337225  0.508477  0.120843   0.0\n",
       "2 -0.060629  0.000955 -0.098904  0.357554  0.004261   0.0\n",
       "3  0.074800  0.416441  0.018549 -0.042219 -0.096620   0.0\n",
       "4 -0.021038 -0.118110  0.325797 -0.133662 -0.075769   0.0\n",
       "5 -0.087982  0.268785 -0.050089 -0.075521  0.059733   0.0\n",
       "6  0.000591  0.006212 -0.125137  0.139078  0.014373   0.0\n",
       "7 -0.076868  0.046668  0.190664 -0.017018  0.050716   0.0\n",
       "8  0.000400 -0.029343  0.169326  0.466005 -0.078977   0.0\n",
       "9  0.126470  0.321831 -0.003226  0.462680 -0.028243   0.0"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6543f",
   "metadata": {},
   "source": [
    "# Ising with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98482a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.09148684550355596 0.10356823968380986\n",
      "Training clf_holdout...\n",
      "Scores: 0.09000554335199401 0.12394863867265815\n",
      "Scores: 0.15886182714772337 0.24736171195576845\n",
      "Scores: 0.1036874941148683 0.12195346956958858\n",
      "Number of iterations:  21\n",
      "Nonadaptive error:  0.1322223671244211\n",
      "Adaptive error:  0.27261770293731835\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.09337342602775593 0.15645991895709516\n",
      "Training clf_holdout...\n",
      "Scores: 0.13984585663163449 0.09709296278668628\n",
      "Scores: 0.11949184631430475 0.14147948750380598\n",
      "Scores: 0.0403799567443455 0.10935942101906534\n",
      "Number of iterations:  44\n",
      "Nonadaptive error:  0.25233303465688395\n",
      "Adaptive error:  0.3395264744682905\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.059231291468772915 0.10853695515858641\n",
      "Training clf_holdout...\n",
      "Scores: 0.09176398906636432 0.11235641897605583\n",
      "Scores: 0.07152507526028266 0.11090388999871681\n",
      "Scores: 0.07874880673129955 0.1146075230366167\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.012605161675882801\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.08588242843820093 0.14510654161039563\n",
      "Training clf_holdout...\n",
      "Scores: 0.1122556481989665 0.0935396501193689\n",
      "Scores: 0.09171517665564939 0.0879542224663255\n",
      "Scores: 0.11525397076157251 0.0969568072825369\n",
      "Number of iterations:  15\n",
      "Nonadaptive error:  0.11479136849201298\n",
      "Adaptive error:  0.179514444703534\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.050471083116058 0.1308559188856007\n",
      "Training clf_holdout...\n",
      "Scores: 0.07579890868531955 0.13806915571807743\n",
      "Scores: 0.09244385692458185 0.1202507801091158\n",
      "Scores: 0.15350915513894667 0.07821276187440057\n",
      "Number of iterations:  42\n",
      "Nonadaptive error:  0.19264473577817337\n",
      "Adaptive error:  0.10471823767392549\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13671525303359724 0.14405030531659763\n",
      "Training clf_holdout...\n",
      "Scores: 0.13041647963735414 0.14719475819671132\n",
      "Scores: 0.06913493970452639 0.12201127000533701\n",
      "Scores: 0.12927177524282363 0.15782109380677864\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.07719999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.08015098769635855 0.11595837655594894\n",
      "Training clf_holdout...\n",
      "Scores: 0.07895056792315605 0.1313630962252679\n",
      "Scores: 0.07328760457011277 0.14756465583701983\n",
      "Scores: 0.1463174983485927 0.11502348257217203\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.06200000000000001\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.09025844193878223 0.17295268920343215\n",
      "Training clf_holdout...\n",
      "Scores: 0.10501606584265533 0.0935818841949276\n",
      "Scores: 0.10769381774978487 0.10999019924439636\n",
      "Scores: 0.11716828440328374 0.08296429483597308\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.04159999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.06930376077898182 0.1632568671947783\n",
      "Training clf_holdout...\n",
      "Scores: 0.1045343346220043 0.12610897962991227\n",
      "Scores: 0.12812062523023254 0.1570024740918354\n",
      "Scores: 0.12110961750767774 0.15775456339152602\n",
      "Number of iterations:  95\n",
      "Nonadaptive error:  0.23050215960879486\n",
      "Adaptive error:  0.2503928654072433\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.10303418916346178 0.13647010758143632\n",
      "Training clf_holdout...\n",
      "Scores: 0.10281567637125265 0.1326768120821945\n",
      "Scores: 0.11335436088875092 0.07951310434206464\n",
      "Scores: 0.09807135020323224 0.13706828612189517\n",
      "Number of iterations:  16\n",
      "Nonadaptive error:  0.12046000806658223\n",
      "Adaptive error:  0.1362544627511379\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.11499678152014889 0.1429908766436759\n",
      "Training clf_holdout...\n",
      "Scores: 0.112341797718028 0.142176703159857\n",
      "Scores: 0.09576115433706608 0.11397752909698072\n",
      "Scores: 0.12677744743305677 0.11424607597505884\n",
      "Number of iterations:  34\n",
      "Nonadaptive error:  0.1944836819503701\n",
      "Adaptive error:  0.5079363924156041\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.10506596709565466 0.11679593601794037\n",
      "Training clf_holdout...\n",
      "Scores: 0.08448180702722688 0.14042042348377715\n",
      "Scores: 0.11287290848130567 0.11549978784761525\n",
      "Scores: 0.11861619409493689 0.11129553662973894\n",
      "Number of iterations:  29\n",
      "Nonadaptive error:  0.15202647250165904\n",
      "Adaptive error:  0.5102719987100072\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.10191752718240471 0.1391261424225414\n",
      "Training clf_holdout...\n",
      "Scores: 0.0777855370565119 0.14887997989349228\n",
      "Scores: 0.09856261695452322 0.11803319847671623\n",
      "Scores: 0.1290821153505079 0.10377087806153366\n",
      "Number of iterations:  24\n",
      "Nonadaptive error:  0.21711585837412276\n",
      "Adaptive error:  0.4518851927148689\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.10116326564667752 0.10139358804494893\n",
      "Training clf_holdout...\n",
      "Scores: 0.12229931433712465 0.12566790291277433\n",
      "Scores: 0.1128207481518714 0.11152007733343641\n",
      "Scores: 0.09259563398909013 0.11399822289672533\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0017333333333333385\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.08442413351967383 0.1148653478849371\n",
      "Training clf_holdout...\n",
      "Scores: 0.08928038145010209 0.11227561263592568\n",
      "Scores: 0.1048006414242898 0.10762241885610217\n",
      "Scores: 0.09747564605388505 0.09194351632921967\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.030973059683798248\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.12539574971173195 0.128510767946681\n",
      "Training clf_holdout...\n",
      "Scores: 0.11608535226516283 0.14071387882244987\n",
      "Scores: 0.12952471681008537 0.10456439615530567\n",
      "Scores: 0.09680862991566062 0.12051363000024913\n",
      "Number of iterations:  11\n",
      "Nonadaptive error:  0.10201755008863883\n",
      "Adaptive error:  0.07294170302753435\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.11255852110710189 0.10961097274950066\n",
      "Training clf_holdout...\n",
      "Scores: 0.12185541033382985 0.11550045140301095\n",
      "Scores: 0.08355513238219472 0.11429307634629281\n",
      "Scores: 0.11146403481131467 0.13391608869698887\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0046666666666666905\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.12808202112921618 0.12578463841965218\n",
      "Training clf_holdout...\n",
      "Scores: 0.10434475753856207 0.08728915514579144\n",
      "Scores: 0.08451019620573517 0.11029944540086759\n",
      "Scores: 0.09468366474783413 0.1164644341507312\n",
      "Number of iterations:  24\n",
      "Nonadaptive error:  0.14242545715421262\n",
      "Adaptive error:  0.5819720000677218\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.15154552797007892 0.12094722561922723\n",
      "Training clf_holdout...\n",
      "Scores: 0.10800640063192776 0.13100709756024545\n",
      "Scores: 0.08940388453569306 0.10111760441488081\n",
      "Scores: 0.10102798829336619 0.13792063820941095\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.07220884043204016\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.09194746441032936 0.09298930478500671\n",
      "Training clf_holdout...\n",
      "Scores: 0.096288470856177 0.10798670266550249\n",
      "Scores: 0.08124032689857921 0.11208879906936461\n",
      "Scores: 0.10578284638016419 0.10685041225116866\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.006115306038454896\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.13592166822456897 0.10169512040524223\n",
      "Training clf_holdout...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 0.10691165372075469 0.09309826003424616\n",
      "Scores: 0.11864766850911107 0.11265401742601237\n",
      "Scores: 0.10922816325260526 0.11372553669172365\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.09923074395110873\n",
      "Adaptive error:  0.10260764334407427\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.11559949010876067 0.12097346929186308\n",
      "Training clf_holdout...\n",
      "Scores: 0.10682370134451033 0.13880515682184627\n",
      "Scores: 0.10046033913906721 0.11730292104858586\n",
      "Scores: 0.10059299613612538 0.1122245170251673\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.028753646283940767\n",
      "Adaptive error:  0.011622568604781683\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.09954677793617578 0.11477366660089508\n",
      "Training clf_holdout...\n",
      "Scores: 0.11289385071991587 0.10020056457566452\n",
      "Scores: 0.09127806066143324 0.12045225566510016\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "\n",
    "# Ising model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,6))\n",
    "\n",
    "for s, sample_size in enumerate([20, 60, 100, 150, 200, 500]):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_iter):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ising_homog = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 500])\n",
    "df_ising_homog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae64e1",
   "metadata": {},
   "source": [
    "# XY with disordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473a47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.32631147734137395 0.32017867879146966\n",
      "Training clf_holdout...\n",
      "Scores: 0.23317611888073975 0.3755153232393866\n",
      "Scores: 0.2471670625748269 0.46685085032118895\n",
      "Scores: 0.3822529429118727 0.42093657487609903\n",
      "Number of iterations:  69\n",
      "Nonadaptive error:  0.6246556142168482\n",
      "Adaptive error:  0.7294643469318309\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.24325410567727512 0.31693253124279974\n",
      "Training clf_holdout...\n",
      "Scores: 0.2822555384043341 0.3575206454238721\n",
      "Scores: 0.2531197430952498 0.30027179621597716\n",
      "Scores: 0.45575306631848767 0.5230218849142682\n",
      "Number of iterations:  20\n",
      "Nonadaptive error:  0.7809971087299649\n",
      "Adaptive error:  0.6967890077287353\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.34742500583551594 0.30285461877458053\n",
      "Training clf_holdout...\n",
      "Scores: 0.26149860078471326 0.5585226777814604\n",
      "Scores: 0.26509875226837 0.6714912378794596\n",
      "Scores: 0.24148164509642886 0.4405607586350667\n",
      "Number of iterations:  21\n",
      "Nonadaptive error:  0.6538130588536869\n",
      "Adaptive error:  0.23426970301142486\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.6123688613379024 0.5400266546153849\n",
      "Training clf_holdout...\n",
      "Scores: 0.33727781515676314 0.3817227185427409\n",
      "Scores: 0.20750301693881734 0.34018209661360493\n",
      "Scores: 0.2843154863385878 0.6071995306060362\n",
      "Number of iterations:  11\n",
      "Nonadaptive error:  0.3559843986201725\n",
      "Adaptive error:  0.18446522808019253\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.3354937435566998 0.28910994883174523\n",
      "Training clf_holdout...\n",
      "Scores: 0.3213860441495595 0.4184754243043604\n",
      "Scores: 0.19828867467362565 0.4195924936478205\n",
      "Scores: 0.3737617286164721 0.3376656291095509\n",
      "Number of iterations:  11\n",
      "Nonadaptive error:  0.74672592306428\n",
      "Adaptive error:  1.0665675315684482\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.3286048374317732 0.47373125119210996\n",
      "Training clf_holdout...\n",
      "Scores: 0.40359956100980277 0.3329915201169306\n",
      "Scores: 0.40840624410089543 0.5774445671470448\n",
      "Scores: 0.415881949581431 0.5689145189959343\n",
      "Number of iterations:  116\n",
      "Nonadaptive error:  0.940900742681066\n",
      "Adaptive error:  0.9732345696924709\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.3990352192578852 0.2989132080283661\n",
      "Training clf_holdout...\n",
      "Scores: 0.52339915053671 0.4167962342488506\n",
      "Scores: 0.21719088013291382 0.3448784756071303\n",
      "Scores: 0.38319237859841193 0.42375194123713267\n",
      "Number of iterations:  6\n",
      "Nonadaptive error:  0.3900807607877812\n",
      "Adaptive error:  0.4453584773189487\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.40302895380217696 0.3890041619608536\n",
      "Training clf_holdout...\n",
      "Scores: 0.2051162875446496 0.35237412151398567\n",
      "Scores: 0.2905282222452231 0.4430314935426356\n",
      "Scores: 0.626805512207937 0.28539152038303495\n",
      "Number of iterations:  19\n",
      "Nonadaptive error:  0.64945590578969\n",
      "Adaptive error:  1.1123239233756104\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.40138640982193285 0.5729711412210977\n",
      "Training clf_holdout...\n",
      "Scores: 0.4867354125359338 0.4857602133827317\n",
      "Scores: 0.3340762593934835 0.27636150388632746\n",
      "Scores: 0.23479542169411338 0.38109080968590714\n",
      "Number of iterations:  39\n",
      "Nonadaptive error:  0.8465037553982839\n",
      "Adaptive error:  0.5928702997936295\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.3206701778050044 0.6429461956689829\n",
      "Training clf_holdout...\n",
      "Scores: 0.3654429923389264 0.42528077299941763\n",
      "Scores: 0.3567687484879102 0.553706750284666\n",
      "Scores: 0.3395332318727106 0.32566850198483716\n",
      "Number of iterations:  82\n",
      "Nonadaptive error:  0.7833719605458793\n",
      "Adaptive error:  0.36111718763126177\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.2931145771937448 0.3134811278091717\n",
      "Training clf_holdout...\n",
      "Scores: 0.26207192456704526 0.29887542408883183\n",
      "Scores: 0.34971750282033204 0.26236246682821274\n",
      "Scores: 0.19673121886305095 0.2933473963492298\n",
      "Number of iterations:  24\n",
      "Nonadaptive error:  0.5021786809674\n",
      "Adaptive error:  0.1577932049845197\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.31020924845865605 0.4014084294580775\n",
      "Training clf_holdout...\n",
      "Scores: 0.2838678305501851 0.3335324988674036\n",
      "Scores: 0.25184634937183054 0.25354235972461153\n",
      "Scores: 0.24279471850431739 0.3339492381120917\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.3444658948240428\n",
      "Adaptive error:  0.32415497451002956\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.3154700292520691 0.18652288190471336\n",
      "Training clf_holdout...\n",
      "Scores: 0.31351298276580614 0.32278675163773884\n",
      "Scores: 0.23036608256602017 0.2558286141761347\n",
      "Scores: 0.28158163306624684 0.27495148844272155\n",
      "Number of iterations:  18\n",
      "Nonadaptive error:  0.2569310954891218\n",
      "Adaptive error:  0.4083182214568416\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.30717454611707795 0.2554525246245436\n",
      "Training clf_holdout...\n",
      "Scores: 0.2697620807156237 0.19449887171588787\n",
      "Scores: 0.3253007895142677 0.23523976153915999\n",
      "Scores: 0.27365322987503754 0.36878858054324215\n",
      "Number of iterations:  10\n",
      "Nonadaptive error:  0.19082877447557117\n",
      "Adaptive error:  0.0652847535462483\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.2755950685611447 0.2741223918057832\n",
      "Training clf_holdout...\n",
      "Scores: 0.3757415284278458 0.2276112904997191\n",
      "Scores: 0.3652289680785071 0.21858272374578802\n",
      "Scores: 0.28982257021097996 0.2571769677511181\n",
      "Number of iterations:  15\n",
      "Nonadaptive error:  0.17892352773550776\n",
      "Adaptive error:  0.2651804434409336\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.26673665706579264 0.31791720254021516\n",
      "Training clf_holdout...\n",
      "Scores: 0.37309231127238013 0.24008137333211937\n",
      "Scores: 0.2515525832627389 0.2959655688991314\n",
      "Scores: 0.3154108683065032 0.2483568325741647\n",
      "Number of iterations:  52\n",
      "Nonadaptive error:  0.6311862046217176\n",
      "Adaptive error:  0.9998927724122673\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.31819437003323486 0.40546266324843144\n",
      "Training clf_holdout...\n",
      "Scores: 0.30486154663426457 0.20888397020264124\n",
      "Scores: 0.2889800314371973 0.17053981795724701\n",
      "Scores: 0.28770186136954595 0.26361296819258223\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.21989286064437946\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.241481989618005 0.31535659335308064\n",
      "Training clf_holdout...\n",
      "Scores: 0.25797631843400604 0.28871244919004513\n",
      "Scores: 0.3612586200903878 0.3915748347731014\n",
      "Scores: 0.3182350946490856 0.23789391509312913\n",
      "Number of iterations:  34\n",
      "Nonadaptive error:  0.4112502058673912\n",
      "Adaptive error:  0.7850235117199227\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.3358774790505724 0.2550939088821358\n",
      "Training clf_holdout...\n",
      "Scores: 0.28729802277256355 0.2715845019269866\n",
      "Scores: 0.26930325597587057 0.33898911973110624\n",
      "Scores: 0.2799114032875809 0.27150666398027323\n",
      "Number of iterations:  21\n",
      "Nonadaptive error:  0.32000409217074066\n",
      "Adaptive error:  0.4071379176422223\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.29843908573104394 0.2532044379279523\n",
      "Training clf_holdout...\n",
      "Scores: 0.29612434116371067 0.20304369680020107\n",
      "Scores: 0.2433843390820563 0.3038739388317063\n",
      "Scores: 0.33046454567617195 0.21757282600053485\n",
      "Number of iterations:  29\n",
      "Nonadaptive error:  0.40737464964081793\n",
      "Adaptive error:  0.7562634646026885\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.26324761261985696 0.23958852560005753\n",
      "Training clf_holdout...\n",
      "Scores: 0.2735441226301822 0.2808469320521068\n",
      "Scores: 0.24681690209563767 0.2503214091062331\n",
      "Scores: 0.2588957239129242 0.24381707168270603\n",
      "Number of iterations:  9\n",
      "Nonadaptive error:  0.14191278370264399\n",
      "Adaptive error:  0.022163973039765916\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.29674300287056776 0.23204386726379977\n",
      "Training clf_holdout...\n",
      "Scores: 0.28270496173467985 0.30629931661294163\n",
      "Scores: 0.24313881490448214 0.26190374234583824\n",
      "Scores: 0.23612848184394272 0.24241007006496024\n",
      "Number of iterations:  69\n",
      "Nonadaptive error:  0.5492880563200914\n",
      "Adaptive error:  1.0249333000527496\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.25595448692162126 0.19635658865641406\n",
      "Training clf_holdout...\n",
      "Scores: 0.2841502242956975 0.23093831971739012\n",
      "Scores: 0.26956409584110297 0.2768073882394601\n",
      "Scores: 0.2796029853978971 0.2822285291447148\n",
      "Number of iterations:  14\n",
      "Nonadaptive error:  0.1396985127969628\n",
      "Adaptive error:  0.10984479804531566\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.231661767709792 0.2335193309004628\n",
      "Training clf_holdout...\n",
      "Scores: 0.26527676923439336 0.19532372688461389\n",
      "Scores: 0.2407122872675852 0.26667811819596504\n",
      "Scores: 0.24750796371846526 0.2862954932719261\n",
      "Number of iterations:  16\n",
      "Nonadaptive error:  0.13001964246408054\n",
      "Adaptive error:  0.04303228346962973\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.27405838526202897 0.2374073020258127\n",
      "Training clf_holdout...\n",
      "Scores: 0.22747313256843255 0.3010343920758306\n",
      "Scores: 0.27128628511765124 0.2398443855947622\n",
      "Scores: 0.2466347561659391 0.2169955005822213\n",
      "Number of iterations:  14\n",
      "Nonadaptive error:  0.1747891245777432\n",
      "Adaptive error:  0.061253688138872\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.2753891368031557 0.2646793448678265\n",
      "Training clf_holdout...\n",
      "Scores: 0.22550448595486172 0.25204507349627914\n",
      "Scores: 0.24173870052057045 0.23621340388104403\n",
      "Scores: 0.2244083021654555 0.19836048224217712\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.17448328825872456\n",
      "Adaptive error:  0.02722462885044985\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.23712322523007884 0.2869619245999145\n",
      "Training clf_holdout...\n",
      "Scores: 0.2632210122746426 0.2513828672942136\n",
      "Scores: 0.2559610840493993 0.27590034562905036\n",
      "Scores: 0.262964840952838 0.22186972110681763\n",
      "Number of iterations:  40\n",
      "Nonadaptive error:  0.2564523960903961\n",
      "Adaptive error:  0.4145084125214382\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.26560128368094915 0.22008255922483067\n",
      "Training clf_holdout...\n",
      "Scores: 0.2888850636710812 0.2694646370750157\n",
      "Scores: 0.2319817459002541 0.23402937327774426\n",
      "Scores: 0.27110093399972995 0.2623056189826003\n",
      "Number of iterations:  26\n",
      "Nonadaptive error:  0.2343489965792146\n",
      "Adaptive error:  0.15907934500150755\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.2785124743171175 0.2455388390453184\n",
      "Training clf_holdout...\n",
      "Scores: 0.3032194507598971 0.26352918623486626\n",
      "Scores: 0.26322883947686904 0.27255648198159943\n",
      "Scores: 0.2795319801298671 0.27674040704686936\n",
      "Number of iterations:  13\n",
      "Nonadaptive error:  0.1519461048075224\n",
      "Adaptive error:  0.23548663146058701\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.2581948923819569 0.224494464443871\n",
      "Training clf_holdout...\n",
      "Scores: 0.2949413666832726 0.23342825216535504\n",
      "Scores: 0.2601415303934542 0.22183174116919913\n",
      "Scores: 0.26857516042508517 0.185899339082589\n",
      "Number of iterations:  22\n",
      "Nonadaptive error:  0.2027884729077401\n",
      "Adaptive error:  0.27349729642713005\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.2527881583642969 0.18018250723473736\n",
      "Training clf_holdout...\n",
      "Scores: 0.22861035546520647 0.2075540709265658\n",
      "Scores: 0.2517380779332382 0.230344903878275\n",
      "Scores: 0.22194836870385898 0.18766741177748944\n",
      "Number of iterations:  21\n",
      "Nonadaptive error:  0.18197697034814247\n",
      "Adaptive error:  0.23234358415881018\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.2536954385565994 0.17788144668779032\n",
      "Training clf_holdout...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m k_holdout_ens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_holdout):\n\u001b[0;32m---> 60\u001b[0m     list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sparse_ML_transformed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mholdout_X_list_ens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholdout_values_ens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     clf_holdout_ens\u001b[38;5;241m.\u001b[39mappend(list_of_clf_holdout[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     63\u001b[0m     score_holdout_ens\u001b[38;5;241m.\u001b[39mappend(list_of_score_holdout[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 191\u001b[0m, in \u001b[0;36mtrain_sparse_ML_transformed\u001b[0;34m(all_X_list, all_values, test_size, random_seed)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;66;03m# ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m15\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m14\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m12\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m11\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)]:\n\u001b[0;32m--> 191\u001b[0m                 score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mML_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneg_root_mean_squared_error\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m#                 print(score)\u001b[39;00m\n\u001b[1;32m    193\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m best_cv_score \u001b[38;5;241m>\u001b[39m score:\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:1004\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m     this_Xy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m _, this_coef, this_dual_gap, this_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43malphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthis_Xy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# from here on **params\u001b[39;49;00m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m coef_[k] \u001b[38;5;241m=\u001b[39m this_coef[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1029\u001b[0m dual_gaps_[k] \u001b[38;5;241m=\u001b[39m this_dual_gap[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631\u001b[0m, in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    617\u001b[0m     model \u001b[38;5;241m=\u001b[39m cd_fast\u001b[38;5;241m.\u001b[39menet_coordinate_descent_gram(\n\u001b[1;32m    618\u001b[0m         coef_,\n\u001b[1;32m    619\u001b[0m         l1_reg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         positive,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m precompute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcd_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menet_coordinate_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecompute should be one of True, False, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or array-like. Got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;241m%\u001b[39m precompute\n\u001b[1;32m    638\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statistics\n",
    "from scipy.optimize import minimize\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "sample_size_list = [20, 60, 100, 150, 200, 300, 500]\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-disorder/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-disorder/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,len(sample_size_list)))\n",
    "\n",
    "for s, sample_size in enumerate(sample_size_list):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_iter):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22674ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy_disorder = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 300, 500])\n",
    "df_xy_disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bfe42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp_qml",
   "language": "python",
   "name": "imp_qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
