{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "24434da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import ast\n",
    "import math\n",
    "import scipy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sigI = np.array([[1.0, 0.0j], [0.0j, 1.0]])\n",
    "sigX = np.array([[0.0j, 1.0], [1.0, 0.0j]])\n",
    "sigY = np.array([[0.0j, -1.0j], [1.0j, 0.0j]])\n",
    "sigZ = np.array([[1.0, 0.0j], [0.0j, -1.0]])\n",
    "\n",
    "N = 50\n",
    "\n",
    "def kron(ls):\n",
    "    A = ls[0]\n",
    "    for X in ls[1:]:\n",
    "        A = np.kron(A, X)\n",
    "    return A\n",
    "\n",
    "def generate_all_zero_state():\n",
    "    return [np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_one_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) for i in range(N)]\n",
    "\n",
    "def generate_half_half_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i < N/2 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_neel_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i % 2 == 0 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_plus_state():\n",
    "    return [np.array([[0.5, 0.5], [0.5, 0.5+0.0j]]) for i in range(N)]\n",
    "\n",
    "def generate_random_product_state():\n",
    "    list_rhoi = []\n",
    "    for i in range(N):\n",
    "        v = np.random.normal(size=3)\n",
    "        v /= np.linalg.norm(v)\n",
    "        rhoi = sigI / 2.0 + (v[0] * sigX / 2.0) + (v[1] * sigY / 2.0) + (v[2] * sigZ / 2.0)\n",
    "        list_rhoi.append(rhoi)\n",
    "    return list_rhoi\n",
    "\n",
    "def twobytwo_to_Pauli(list_rhoi):\n",
    "    list_rhoi_new = []\n",
    "    for rhoi in list_rhoi:\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "    return list_rhoi_new\n",
    "\n",
    "def get_RDM_in_Pauli(list_rhoi, k):\n",
    "    feat_vec = []\n",
    "    for i in range(N-k+1):\n",
    "        for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "            val = 1.0\n",
    "            for c, P in enumerate(list_P):\n",
    "                if P == -1: continue\n",
    "                val *= list_rhoi[(3*(i+c))+P]\n",
    "            assert(np.abs(val.imag) < 1e-7)\n",
    "            feat_vec.append(val.real)\n",
    "    return feat_vec\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML(all_states, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "        print(\"Pos:\", pos)\n",
    "\n",
    "        def twobytwo_to_Pauli(list_rhoi):\n",
    "            list_rhoi_new = []\n",
    "            for rhoi in list_rhoi:\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "            return list_rhoi_new\n",
    "\n",
    "        def get_RDM_in_Pauli(list_rhoi, k):\n",
    "            feat_vec = []\n",
    "            for i in range(N-k+1):\n",
    "                for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "                    val = 1.0\n",
    "                    for c, P in enumerate(list_P):\n",
    "                        if P == -1: continue\n",
    "                        val *= list_rhoi[(3*(i+c))+P]\n",
    "                    assert(np.abs(val.imag) < 1e-7)\n",
    "                    feat_vec.append(val.real)\n",
    "            return feat_vec\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_states)), range(len(all_states)), test_size=test_size, random_state=random_seed)\n",
    "\n",
    "        for k in [1, 2, 3, 4]:\n",
    "            print(\"Validate k =\", k)\n",
    "            X, y_true, y_noisy = [], [], []\n",
    "\n",
    "            for data in zip(all_states, all_values):\n",
    "                X.append(get_RDM_in_Pauli(data[0], k))\n",
    "                y_true.append(data[1][pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[1][pos]+1)/2, 1)[0] / 500) - 1)\n",
    "\n",
    "            X = np.array(X)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "                print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML_transformed(all_X_list, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "#         print(\"Pos:\", pos)\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_values)), range(len(all_values)), test_size=test_size, random_state=random_seed)\n",
    "        \n",
    "        for k in [1, 2]:\n",
    "#             print(\"Validate k =\", k)\n",
    "\n",
    "            X = all_X_list[k-1]\n",
    "            \n",
    "            y_true, y_noisy = [], []\n",
    "            for data in all_values:\n",
    "                y_true.append(data[pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[pos]+1)/2, 1)[0] / 500) - 1)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "#                 print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "def transform_states(all_states):\n",
    "    all_X_list = []\n",
    "    \n",
    "    for k in [1, 2]:\n",
    "        X = []\n",
    "        for data in all_states:\n",
    "            X.append(get_RDM_in_Pauli(data, k))\n",
    "        all_X_list.append(np.array(X))\n",
    "    return all_X_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de994661",
   "metadata": {},
   "source": [
    "# XY model with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "8f7e29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "sample_size = 100\n",
    "all_data_training_set_scaling = []\n",
    "seed = 0\n",
    "test_size = 0.5\n",
    "num_holdout = 1\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "\n",
    "# Train / holdout split\n",
    "np.random.seed(seed)\n",
    "sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "train_states, train_values = np.array(all_states)[\n",
    "    sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "\n",
    "# I setup the framework for multiple holdout but right now still should be the same as using \n",
    "# only one holdout in my current code below\n",
    "holdout_states_ens, holdout_values_ens = [],[]\n",
    "for i in range(1, num_holdout+1):\n",
    "    holdout_states, holdout_values = np.array(all_states)[\n",
    "        sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "    holdout_states_ens.append(holdout_states)\n",
    "    holdout_values_ens.append(holdout_values)\n",
    "h = holdout_states_ens[0]\n",
    "\n",
    "train_X_list = transform_states(train_states)\n",
    "holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "995f0c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.1809684234136905 0.18331344719082018\n",
      "Training clf_holdout...\n",
      "Scores: 0.1370288928713825 0.08149093179813323\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Obtain classifiers\n",
    "print(\"Training clf_train...\")\n",
    "list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "    train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "clf_train = list_of_clf_train[0]\n",
    "k_train = list_of_bestk_train[0]\n",
    "\n",
    "print(\"Training clf_holdout...\")\n",
    "clf_holdout_ens = []\n",
    "score_holdout_ens = []\n",
    "k_holdout_ens = []\n",
    "for i in range(num_holdout):\n",
    "    list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "        holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "    clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "    score_holdout_ens.append(list_of_score_holdout[0])\n",
    "    k_holdout_ens.append(list_of_bestk_holdout[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f4907bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# Adaptive optimization\n",
    "\n",
    "def predict_holdout(x):\n",
    "    # Get holdout prediction\n",
    "    predictions = []\n",
    "    for i in range(num_holdout):\n",
    "        # Get best classifier with best k\n",
    "        clf_holdout = clf_holdout_ens[i]\n",
    "        k_holdout = k_holdout_ens[i]\n",
    "        X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "        predictions.append(clf_holdout.predict(X_holdout))\n",
    "    return predictions[0][0]\n",
    "\n",
    "def predict_train(x):\n",
    "    # Get train prediction\n",
    "    X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "    return clf_train.predict(X_train)[0]\n",
    "\n",
    "def get_product_state(y):\n",
    "    # Compute product state from input spherical coordinates\n",
    "    x = []\n",
    "    for i in range(N):\n",
    "        phi = y[i]\n",
    "        theta = y[N+i]\n",
    "        x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "    return x\n",
    "\n",
    "def objective(y):\n",
    "    # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -abs(y_train - y_holdout)\n",
    "\n",
    "def objective_train(y):\n",
    "    # Maximizes train predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    return -y_train\n",
    "\n",
    "def objective_holdout(y):\n",
    "    x = get_product_state(y)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -y_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "da1b763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "def objective_state(x):\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return abs(y_train - y_holdout)\n",
    "\n",
    "# max_err = 0\n",
    "# max_idx = 0\n",
    "# avg_err = 0\n",
    "# print(\"Print: max error, index of maximum error, average error\")\n",
    "# np.random.seed(seed)\n",
    "# for i in range(100):\n",
    "#     y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "#     if i % 500 == 0:\n",
    "#         print(f\"step {i}: {max_err}, {max_idx}, {avg_err}\")\n",
    "#     err = -objective(y0)\n",
    "#     if err > max_err:\n",
    "#         max_err = err\n",
    "#         max_idx = i\n",
    "#     avg_err += err / 10000\n",
    "# print(max_err)\n",
    "\n",
    "# Sometimes I get something like this, i.e. classifier always outputs the same number\n",
    "# for i in range(10000):\n",
    "#     if i % 500 == 0:\n",
    "#         print(predict_train(all_states[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "71f3b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          100     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.04176D-02    |proj g|=  8.14497D-02\n",
      "\n",
      "At iterate    1    f=  4.49316D-02    |proj g|=  7.17918D-02\n",
      "\n",
      "At iterate    2    f= -1.66070D-01    |proj g|=  4.45008D-02\n",
      "\n",
      "At iterate    3    f= -1.87221D-01    |proj g|=  3.03842D-02\n",
      "\n",
      "At iterate    4    f= -1.94897D-01    |proj g|=  6.67487D-03\n",
      "\n",
      "At iterate    5    f= -1.98043D-01    |proj g|=  8.17441D-03\n",
      "\n",
      "At iterate    6    f= -2.14766D-01    |proj g|=  1.19964D-02\n",
      "\n",
      "At iterate    7    f= -2.21893D-01    |proj g|=  1.08330D-02\n",
      "\n",
      "At iterate    8    f= -2.24519D-01    |proj g|=  3.40317D-03\n",
      "\n",
      "At iterate    9    f= -2.25831D-01    |proj g|=  5.57586D-03\n",
      "\n",
      "At iterate   10    f= -2.27506D-01    |proj g|=  6.91223D-03\n",
      "\n",
      "At iterate   11    f= -2.29020D-01    |proj g|=  4.39133D-03\n",
      "\n",
      "At iterate   12    f= -2.29883D-01    |proj g|=  1.36219D-03\n",
      "\n",
      "At iterate   13    f= -2.30070D-01    |proj g|=  6.54837D-04\n",
      "\n",
      "At iterate   14    f= -2.30124D-01    |proj g|=  6.88771D-04\n",
      "\n",
      "At iterate   15    f= -2.30136D-01    |proj g|=  1.22871D-03\n",
      "\n",
      "At iterate   16    f= -2.30150D-01    |proj g|=  3.07848D-04\n",
      "\n",
      "At iterate   17    f= -2.30155D-01    |proj g|=  3.72247D-04\n",
      "\n",
      "At iterate   18    f= -2.30174D-01    |proj g|=  9.16101D-04\n",
      "\n",
      "At iterate   19    f= -2.30225D-01    |proj g|=  1.61875D-03\n",
      "\n",
      "At iterate   20    f= -2.31399D-01    |proj g|=  6.38500D-03\n",
      "  ys=-8.750E-04  -gs= 4.670E-04 BFGS update SKIPPED\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   21    f= -2.31782D-01    |proj g|=  5.08168D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   22    f= -2.32876D-01    |proj g|=  5.68266D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   23    f= -2.33293D-01    |proj g|=  4.74369D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   24    f= -2.34484D-01    |proj g|=  6.25546D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   25    f= -2.34896D-01    |proj g|=  4.83271D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   26    f= -2.35812D-01    |proj g|=  8.02018D-03\n",
      "  Positive dir derivative in projection \n",
      "  Using the backtracking step \n",
      "\n",
      "At iterate   27    f= -2.36512D-01    |proj g|=  4.58321D-03\n",
      "\n",
      "At iterate   28    f= -2.39490D-01    |proj g|=  2.24085D-03\n",
      "\n",
      "At iterate   29    f= -2.39744D-01    |proj g|=  2.14533D-03\n",
      "\n",
      "At iterate   30    f= -2.40062D-01    |proj g|=  8.35387D-04\n",
      "\n",
      "At iterate   31    f= -2.40191D-01    |proj g|=  7.37066D-04\n",
      "\n",
      "At iterate   32    f= -2.40197D-01    |proj g|=  1.89215D-04\n",
      "\n",
      "At iterate   33    f= -2.40198D-01    |proj g|=  1.89615D-04\n",
      "\n",
      "At iterate   34    f= -2.40206D-01    |proj g|=  1.90514D-04\n",
      "\n",
      "At iterate   35    f= -2.40216D-01    |proj g|=  1.91769D-04\n",
      "\n",
      "At iterate   36    f= -2.40234D-01    |proj g|=  2.71649D-04\n",
      "\n",
      "At iterate   37    f= -2.40273D-01    |proj g|=  5.93647D-04\n",
      "\n",
      "At iterate   38    f= -2.40323D-01    |proj g|=  5.46396D-04\n",
      "\n",
      "At iterate   39    f= -2.40410D-01    |proj g|=  1.18487D-03\n",
      "\n",
      "At iterate   40    f= -2.40432D-01    |proj g|=  1.10832D-03\n",
      "\n",
      "At iterate   41    f= -2.40474D-01    |proj g|=  4.00302D-04\n",
      "\n",
      "At iterate   42    f= -2.40491D-01    |proj g|=  2.29827D-04\n",
      "\n",
      "At iterate   43    f= -2.40501D-01    |proj g|=  2.14095D-04\n",
      "\n",
      "At iterate   44    f= -2.40503D-01    |proj g|=  8.46734D-04\n",
      "\n",
      "At iterate   45    f= -2.40512D-01    |proj g|=  1.89038D-04\n",
      "\n",
      "At iterate   46    f= -2.40515D-01    |proj g|=  1.87350D-04\n",
      "\n",
      "At iterate   47    f= -2.40520D-01    |proj g|=  1.79684D-04\n",
      "\n",
      "At iterate   48    f= -2.40532D-01    |proj g|=  2.99227D-04\n",
      "\n",
      "At iterate   49    f= -2.40550D-01    |proj g|=  6.55154D-04\n",
      "\n",
      "At iterate   50    f= -2.40559D-01    |proj g|=  3.85159D-04\n",
      "\n",
      "At iterate   51    f= -2.40571D-01    |proj g|=  1.67394D-04\n",
      "\n",
      "At iterate   52    f= -2.40575D-01    |proj g|=  1.33321D-04\n",
      "\n",
      "At iterate   53    f= -2.40576D-01    |proj g|=  2.23532D-04\n",
      "\n",
      "At iterate   54    f= -2.40576D-01    |proj g|=  1.29330D-04\n",
      "\n",
      "At iterate   55    f= -2.40577D-01    |proj g|=  1.91402D-05\n",
      "\n",
      "At iterate   56    f= -2.40577D-01    |proj g|=  3.22131D-05\n",
      "\n",
      "At iterate   57    f= -2.40577D-01    |proj g|=  2.83329D-05\n",
      "\n",
      "At iterate   58    f= -2.40577D-01    |proj g|=  1.45051D-05\n",
      "\n",
      "At iterate   59    f= -2.40577D-01    |proj g|=  1.44162D-05\n",
      "\n",
      "At iterate   60    f= -2.40577D-01    |proj g|=  1.64313D-05\n",
      "\n",
      "At iterate   61    f= -2.40578D-01    |proj g|=  2.75502D-05\n",
      "\n",
      "At iterate   62    f= -2.40578D-01    |proj g|=  3.77476D-05\n",
      "\n",
      "At iterate   63    f= -2.40578D-01    |proj g|=  9.30978D-05\n",
      "\n",
      "At iterate   64    f= -2.40578D-01    |proj g|=  4.55747D-05\n",
      "\n",
      "At iterate   65    f= -2.40579D-01    |proj g|=  3.84026D-05\n",
      "\n",
      "At iterate   66    f= -2.40579D-01    |proj g|=  1.07603D-04\n",
      "\n",
      "At iterate   67    f= -2.40579D-01    |proj g|=  3.10640D-05\n",
      "\n",
      "At iterate   68    f= -2.40579D-01    |proj g|=  6.45040D-05\n",
      "\n",
      "At iterate   69    f= -2.40580D-01    |proj g|=  4.52027D-05\n",
      "\n",
      "At iterate   70    f= -2.40580D-01    |proj g|=  7.42684D-05\n",
      "\n",
      "At iterate   71    f= -2.40580D-01    |proj g|=  1.92180D-05\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -0.24057966646879983\n",
      "        x: [ 0.000e+00  0.000e+00 ...  3.142e+00  5.540e+00]\n",
      "      nit: 72\n",
      "      jac: [ 0.000e+00  4.083e-02 ...  4.974e-06  0.000e+00]\n",
      "     nfev: 8282\n",
      "     njev: 82\n",
      " hess_inv: <100x100 LbfgsInvHessProduct with dtype=float64>\n",
      "\n",
      "At iterate   72    f= -2.40580D-01    |proj g|=  6.83897D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  100     72     82     75     1    14   6.839D-06  -2.406D-01\n",
      "  F = -0.24057966646879983     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Nelder-mead\n",
    "np.random.seed(seed)\n",
    "bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N)\n",
    "y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "# res = minimize(objective_train, y0, method='Powell', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective, y0, method='Nelder-Mead', bounds=bnds, options={'disp': True})\n",
    "res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective_train, y0, method='trust-constr', bounds=bnds, options={'disp': True})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "0c8e1c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2741796664687998"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the error, i.e. if large, then we have overfit\n",
    "state = get_product_state(res.x)\n",
    "y_train = predict_train(state)\n",
    "y_holdout = predict_holdout(state)\n",
    "abs(y_train-y_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d20ad",
   "metadata": {},
   "source": [
    "Repeat this for many initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statistics\n",
    "from scipy.optimize import minimize\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "sample_size_list = [20, 60, 100, 150, 200, 300, 500]\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,len(sample_size_list)))\n",
    "\n",
    "for s, sample_size in enumerate(sample_size_list):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_iter):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf51681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 300, 500])\n",
    "df_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "2138a552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>60</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>600</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.195436</td>\n",
       "      <td>0.426795</td>\n",
       "      <td>0.040752</td>\n",
       "      <td>-0.011818</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068729</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>0.337225</td>\n",
       "      <td>0.508477</td>\n",
       "      <td>0.120843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.060629</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>-0.098904</td>\n",
       "      <td>0.357554</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.416441</td>\n",
       "      <td>0.018549</td>\n",
       "      <td>-0.042219</td>\n",
       "      <td>-0.096620</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.021038</td>\n",
       "      <td>-0.118110</td>\n",
       "      <td>0.325797</td>\n",
       "      <td>-0.133662</td>\n",
       "      <td>-0.075769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.087982</td>\n",
       "      <td>0.268785</td>\n",
       "      <td>-0.050089</td>\n",
       "      <td>-0.075521</td>\n",
       "      <td>0.059733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>-0.125137</td>\n",
       "      <td>0.139078</td>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.076868</td>\n",
       "      <td>0.046668</td>\n",
       "      <td>0.190664</td>\n",
       "      <td>-0.017018</td>\n",
       "      <td>0.050716</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000400</td>\n",
       "      <td>-0.029343</td>\n",
       "      <td>0.169326</td>\n",
       "      <td>0.466005</td>\n",
       "      <td>-0.078977</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.126470</td>\n",
       "      <td>0.321831</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>0.462680</td>\n",
       "      <td>-0.028243</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       20        60        100       200       600   2000\n",
       "0  0.055600  0.195436  0.426795  0.040752 -0.011818   0.0\n",
       "1  0.068729  0.003733  0.337225  0.508477  0.120843   0.0\n",
       "2 -0.060629  0.000955 -0.098904  0.357554  0.004261   0.0\n",
       "3  0.074800  0.416441  0.018549 -0.042219 -0.096620   0.0\n",
       "4 -0.021038 -0.118110  0.325797 -0.133662 -0.075769   0.0\n",
       "5 -0.087982  0.268785 -0.050089 -0.075521  0.059733   0.0\n",
       "6  0.000591  0.006212 -0.125137  0.139078  0.014373   0.0\n",
       "7 -0.076868  0.046668  0.190664 -0.017018  0.050716   0.0\n",
       "8  0.000400 -0.029343  0.169326  0.466005 -0.078977   0.0\n",
       "9  0.126470  0.321831 -0.003226  0.462680 -0.028243   0.0"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f1559b",
   "metadata": {},
   "source": [
    "# Ising with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f774503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.09148684550355596 0.10356823968380986\n",
      "Training clf_holdout...\n",
      "Scores: 0.09000554335199401 0.12394863867265815\n",
      "Scores: 0.15886182714772337 0.24736171195576845\n",
      "Scores: 0.1036874941148683 0.12195346956958858\n",
      "Number of iterations:  21\n",
      "Nonadaptive error:  0.1322223671244211\n",
      "Adaptive error:  0.27261770293731835\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.09337342602775593 0.15645991895709516\n",
      "Training clf_holdout...\n",
      "Scores: 0.13984585663163449 0.09709296278668628\n",
      "Scores: 0.11949184631430475 0.14147948750380598\n",
      "Scores: 0.0403799567443455 0.10935942101906534\n",
      "Number of iterations:  44\n",
      "Nonadaptive error:  0.25233303465688395\n",
      "Adaptive error:  0.3395264744682905\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.059231291468772915 0.10853695515858641\n",
      "Training clf_holdout...\n",
      "Scores: 0.09176398906636432 0.11235641897605583\n",
      "Scores: 0.07152507526028266 0.11090388999871681\n",
      "Scores: 0.07874880673129955 0.1146075230366167\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.012605161675882801\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.08588242843820093 0.14510654161039563\n",
      "Training clf_holdout...\n",
      "Scores: 0.1122556481989665 0.0935396501193689\n",
      "Scores: 0.09171517665564939 0.0879542224663255\n",
      "Scores: 0.11525397076157251 0.0969568072825369\n",
      "Number of iterations:  15\n",
      "Nonadaptive error:  0.11479136849201298\n",
      "Adaptive error:  0.179514444703534\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.050471083116058 0.1308559188856007\n",
      "Training clf_holdout...\n",
      "Scores: 0.07579890868531955 0.13806915571807743\n",
      "Scores: 0.09244385692458185 0.1202507801091158\n",
      "Scores: 0.15350915513894667 0.07821276187440057\n",
      "Number of iterations:  42\n",
      "Nonadaptive error:  0.19264473577817337\n",
      "Adaptive error:  0.10471823767392549\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13671525303359724 0.14405030531659763\n",
      "Training clf_holdout...\n",
      "Scores: 0.13041647963735414 0.14719475819671132\n",
      "Scores: 0.06913493970452639 0.12201127000533701\n",
      "Scores: 0.12927177524282363 0.15782109380677864\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.07719999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.08015098769635855 0.11595837655594894\n",
      "Training clf_holdout...\n",
      "Scores: 0.07895056792315605 0.1313630962252679\n",
      "Scores: 0.07328760457011277 0.14756465583701983\n",
      "Scores: 0.1463174983485927 0.11502348257217203\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.06200000000000001\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.09025844193878223 0.17295268920343215\n",
      "Training clf_holdout...\n",
      "Scores: 0.10501606584265533 0.0935818841949276\n",
      "Scores: 0.10769381774978487 0.10999019924439636\n",
      "Scores: 0.11716828440328374 0.08296429483597308\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.04159999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.06930376077898182 0.1632568671947783\n",
      "Training clf_holdout...\n",
      "Scores: 0.1045343346220043 0.12610897962991227\n",
      "Scores: 0.12812062523023254 0.1570024740918354\n",
      "Scores: 0.12110961750767774 0.15775456339152602\n",
      "Number of iterations:  95\n",
      "Nonadaptive error:  0.23050215960879486\n",
      "Adaptive error:  0.2503928654072433\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.10303418916346178 0.13647010758143632\n",
      "Training clf_holdout...\n",
      "Scores: 0.10281567637125265 0.1326768120821945\n",
      "Scores: 0.11335436088875092 0.07951310434206464\n",
      "Scores: 0.09807135020323224 0.13706828612189517\n",
      "Number of iterations:  16\n",
      "Nonadaptive error:  0.12046000806658223\n",
      "Adaptive error:  0.1362544627511379\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.11499678152014889 0.1429908766436759\n",
      "Training clf_holdout...\n",
      "Scores: 0.112341797718028 0.142176703159857\n",
      "Scores: 0.09576115433706608 0.11397752909698072\n",
      "Scores: 0.12677744743305677 0.11424607597505884\n",
      "Number of iterations:  34\n",
      "Nonadaptive error:  0.1944836819503701\n",
      "Adaptive error:  0.5079363924156041\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.10506596709565466 0.11679593601794037\n",
      "Training clf_holdout...\n",
      "Scores: 0.08448180702722688 0.14042042348377715\n",
      "Scores: 0.11287290848130567 0.11549978784761525\n",
      "Scores: 0.11861619409493689 0.11129553662973894\n",
      "Number of iterations:  29\n",
      "Nonadaptive error:  0.15202647250165904\n",
      "Adaptive error:  0.5102719987100072\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.10191752718240471 0.1391261424225414\n",
      "Training clf_holdout...\n",
      "Scores: 0.0777855370565119 0.14887997989349228\n",
      "Scores: 0.09856261695452322 0.11803319847671623\n",
      "Scores: 0.1290821153505079 0.10377087806153366\n",
      "Number of iterations:  24\n",
      "Nonadaptive error:  0.21711585837412276\n",
      "Adaptive error:  0.4518851927148689\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.10116326564667752 0.10139358804494893\n",
      "Training clf_holdout...\n",
      "Scores: 0.12229931433712465 0.12566790291277433\n",
      "Scores: 0.1128207481518714 0.11152007733343641\n",
      "Scores: 0.09259563398909013 0.11399822289672533\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0017333333333333385\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.08442413351967383 0.1148653478849371\n",
      "Training clf_holdout...\n",
      "Scores: 0.08928038145010209 0.11227561263592568\n",
      "Scores: 0.1048006414242898 0.10762241885610217\n",
      "Scores: 0.09747564605388505 0.09194351632921967\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.030973059683798248\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.12539574971173195 0.128510767946681\n",
      "Training clf_holdout...\n",
      "Scores: 0.11608535226516283 0.14071387882244987\n",
      "Scores: 0.12952471681008537 0.10456439615530567\n",
      "Scores: 0.09680862991566062 0.12051363000024913\n",
      "Number of iterations:  11\n",
      "Nonadaptive error:  0.10201755008863883\n",
      "Adaptive error:  0.07294170302753435\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.11255852110710189 0.10961097274950066\n",
      "Training clf_holdout...\n",
      "Scores: 0.12185541033382985 0.11550045140301095\n",
      "Scores: 0.08355513238219472 0.11429307634629281\n",
      "Scores: 0.11146403481131467 0.13391608869698887\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0046666666666666905\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.12808202112921618 0.12578463841965218\n",
      "Training clf_holdout...\n",
      "Scores: 0.10434475753856207 0.08728915514579144\n",
      "Scores: 0.08451019620573517 0.11029944540086759\n",
      "Scores: 0.09468366474783413 0.1164644341507312\n",
      "Number of iterations:  24\n",
      "Nonadaptive error:  0.14242545715421262\n",
      "Adaptive error:  0.5819720000677218\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.15154552797007892 0.12094722561922723\n",
      "Training clf_holdout...\n",
      "Scores: 0.10800640063192776 0.13100709756024545\n",
      "Scores: 0.08940388453569306 0.10111760441488081\n",
      "Scores: 0.10102798829336619 0.13792063820941095\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.07220884043204016\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.09194746441032936 0.09298930478500671\n",
      "Training clf_holdout...\n",
      "Scores: 0.096288470856177 0.10798670266550249\n",
      "Scores: 0.08124032689857921 0.11208879906936461\n",
      "Scores: 0.10578284638016419 0.10685041225116866\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.006115306038454896\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.13592166822456897 0.10169512040524223\n",
      "Training clf_holdout...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 0.10691165372075469 0.09309826003424616\n",
      "Scores: 0.11864766850911107 0.11265401742601237\n",
      "Scores: 0.10922816325260526 0.11372553669172365\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.09923074395110873\n",
      "Adaptive error:  0.10260764334407427\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.11559949010876067 0.12097346929186308\n",
      "Training clf_holdout...\n",
      "Scores: 0.10682370134451033 0.13880515682184627\n",
      "Scores: 0.10046033913906721 0.11730292104858586\n",
      "Scores: 0.10059299613612538 0.1122245170251673\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.028753646283940767\n",
      "Adaptive error:  0.011622568604781683\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.09954677793617578 0.11477366660089508\n",
      "Training clf_holdout...\n",
      "Scores: 0.11289385071991587 0.10020056457566452\n",
      "Scores: 0.09127806066143324 0.12045225566510016\n",
      "Scores: 0.09148259876010656 0.12260604323701127\n",
      "Number of iterations:  30\n",
      "Nonadaptive error:  0.19597137817897098\n",
      "Adaptive error:  0.2959705913451732\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.11033889170474967 0.11495994620046598\n",
      "Training clf_holdout...\n",
      "Scores: 0.10668476613886282 0.12044235709748452\n",
      "Scores: 0.10023899443635517 0.12796588781559007\n",
      "Scores: 0.09281138454143394 0.10276185281382157\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.013040000000000003\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.1243701505655477 0.09954953739669416\n",
      "Training clf_holdout...\n",
      "Scores: 0.10394585821389916 0.09570458804754309\n",
      "Scores: 0.11906871291360963 0.10636602284344526\n",
      "Scores: 0.1264944249763239 0.10883856548158548\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.025679999999999998\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.11178282314410243 0.14055273237924862\n",
      "Training clf_holdout...\n",
      "Scores: 0.09807643919320794 0.10996436479493155\n",
      "Scores: 0.09583182566578301 0.10816014651682558\n",
      "Scores: 0.12006584027120734 0.11716055795326276\n",
      "Number of iterations:  41\n",
      "Nonadaptive error:  0.14959893335394242\n",
      "Adaptive error:  0.3043504616374374\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10915119824805089 0.11130809976069474\n",
      "Training clf_holdout...\n",
      "Scores: 0.10487412811490053 0.14484581891910267\n",
      "Scores: 0.11642150303425872 0.12603266278983505\n",
      "Scores: 0.12065688976810907 0.11548457602436765\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.006799999999999997\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.111285330913203 0.10674176718055664\n",
      "Training clf_holdout...\n",
      "Scores: 0.08945524921172716 0.10062992468976235\n",
      "Scores: 0.10987318020999626 0.11679226738355568\n",
      "Scores: 0.10399220932527051 0.12362091519756546\n",
      "Number of iterations:  42\n",
      "Nonadaptive error:  0.23131135653624926\n",
      "Adaptive error:  0.5885790948850689\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.11937447688965883 0.13067560337996048\n",
      "Training clf_holdout...\n",
      "Scores: 0.1083671704745765 0.11806169878073672\n",
      "Scores: 0.11703076642035859 0.12053980560946562\n",
      "Scores: 0.11378477972833739 0.09615458990784208\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.02623694853080263\n",
      "Adaptive error:  0.009499177579007936\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.09792741944838876 0.08520404446214726\n",
      "Training clf_holdout...\n",
      "Scores: 0.10126102558790209 0.10174228571879726\n",
      "Scores: 0.115308716230807 0.10798141115082033\n",
      "Scores: 0.10050196460946417 0.09895764024761446\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0038421951908521514\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.1209304401135368 0.10762368451828283\n",
      "Training clf_holdout...\n",
      "Scores: 0.11186902090155104 0.10909184387637635\n",
      "Scores: 0.10952588989239487 0.11206827519600798\n",
      "Scores: 0.10075126091739273 0.11462780949023535\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.007093333333333346\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.12455513533613041 0.1198025842396727\n",
      "Training clf_holdout...\n",
      "Scores: 0.10433511755146743 0.10889022083824297\n",
      "Scores: 0.11060417254552853 0.11528977871924209\n",
      "Scores: 0.10335921015784691 0.11290854654231762\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.013599335019116789\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.11371435166789984 0.10200064946386438\n",
      "Training clf_holdout...\n",
      "Scores: 0.11946335613769725 0.089554070660913\n",
      "Scores: 0.11000411394455839 0.10862548143377869\n",
      "Scores: 0.10264164557459957 0.12245659786111986\n",
      "Number of iterations:  9\n",
      "Nonadaptive error:  0.03996318833972045\n",
      "Adaptive error:  0.036970907449664234\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.11764282253123631 0.10796640351046642\n",
      "Training clf_holdout...\n",
      "Scores: 0.11021873756602645 0.11060896927916822\n",
      "Scores: 0.08839265910371147 0.09942970363648854\n",
      "Scores: 0.1223774698998385 0.1256969620729892\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0028266666666666752\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.1262729168436238 0.09733435049657439\n",
      "Training clf_holdout...\n",
      "Scores: 0.1070180920521391 0.10126328648680957\n",
      "Scores: 0.11784676816849356 0.12079370545498355\n",
      "Scores: 0.12918889125260383 0.12417520421072077\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.0030933333333333394\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.10565817344369419 0.13399536832476597\n",
      "Training clf_holdout...\n",
      "Scores: 0.10673559579477465 0.09817598475907181\n",
      "Scores: 0.10971913235091715 0.1129243871777841\n",
      "Scores: 0.12224021678618378 0.09104946059513014\n",
      "Number of iterations:  51\n",
      "Nonadaptive error:  0.09225942785972303\n",
      "Adaptive error:  0.22707199358130137\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10456883445488523 0.11948135840477637\n",
      "Training clf_holdout...\n",
      "Scores: 0.10952454059147826 0.12335856147693912\n",
      "Scores: 0.1078875082810781 0.11184093782315888\n",
      "Scores: 0.12380199512576923 0.13379823037117025\n",
      "Number of iterations:  36\n",
      "Nonadaptive error:  0.17585871982372955\n",
      "Adaptive error:  0.6302153006738735\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.11351337686716212 0.10371434097393753\n",
      "Training clf_holdout...\n",
      "Scores: 0.10178387740932457 0.11497826763507801\n",
      "Scores: 0.11992338407780417 0.09845451360167666\n",
      "Scores: 0.11315014472319371 0.10211012618192387\n",
      "Number of iterations:  11\n",
      "Nonadaptive error:  0.047779509344471456\n",
      "Adaptive error:  0.007007635668999718\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.11184224123977889 0.10933256517504461\n",
      "Training clf_holdout...\n",
      "Scores: 0.11119670097227496 0.11574458665003415\n",
      "Scores: 0.11814309780081358 0.10340443590015456\n",
      "Scores: 0.11867534484010185 0.09494569466746026\n",
      "Number of iterations:  5\n",
      "Nonadaptive error:  0.042520201677681974\n",
      "Adaptive error:  0.07044966856695986\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.09850817783229099 0.0903649725422176\n",
      "Training clf_holdout...\n",
      "Scores: 0.10659764003296333 0.1193403052713102\n",
      "Scores: 0.11244603336498549 0.1070439131903742\n",
      "Scores: 0.12210944980704515 0.13182986687340836\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.09125168916678868\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.12183751370397229 0.09824736918071829\n",
      "Training clf_holdout...\n",
      "Scores: 0.12673780789777367 0.10451899392782023\n",
      "Scores: 0.10238413793110206 0.10039731653971921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 0.11141307555634938 0.11580068522382687\n",
      "Number of iterations:  8\n",
      "Nonadaptive error:  0.07128208012569423\n",
      "Adaptive error:  0.11160832047982866\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.10853539953937652 0.11210978804275847\n",
      "Training clf_holdout...\n",
      "Scores: 0.11456915899101444 0.1202893990254779\n",
      "Scores: 0.12026099828496177 0.09960945420245623\n",
      "Scores: 0.11592923234960732 0.09685587477871464\n",
      "Number of iterations:  6\n",
      "Nonadaptive error:  0.04931343652574665\n",
      "Adaptive error:  0.05177184193262664\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.11410313590429272 0.09749310105956842\n",
      "Training clf_holdout...\n",
      "Scores: 0.10305109549390064 0.09636831959130963\n",
      "Scores: 0.10426955483212189 0.11270277338692918\n",
      "Scores: 0.11463200996019371 0.10625386745459384\n",
      "Number of iterations:  17\n",
      "Nonadaptive error:  0.07130844651715665\n",
      "Adaptive error:  0.006482953482764328\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.11677699286773323 0.106148813900413\n",
      "Training clf_holdout...\n",
      "Scores: 0.1110646321061652 0.10295240708496926\n",
      "Scores: 0.11481140809538382 0.12329918921304277\n",
      "Scores: 0.11246407182391399 0.11016707910675012\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.024140449972456304\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.09718101128226816 0.09745141461237669\n",
      "Training clf_holdout...\n",
      "Scores: 0.11573044263528032 0.10675387878672313\n",
      "Scores: 0.12933605501681822 0.11577552641051081\n",
      "Scores: 0.09533271555379175 0.09704679550585782\n",
      "Number of iterations:  13\n",
      "Nonadaptive error:  0.06242038194264153\n",
      "Adaptive error:  0.08749022668751155\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13125192953354595 0.11980995447869376\n",
      "Training clf_holdout...\n",
      "Scores: 0.10662946457514559 0.09836819880703687\n",
      "Scores: 0.10983675774706855 0.09890756997647189\n",
      "Scores: 0.12028754088730642 0.09908072837776506\n",
      "Number of iterations:  77\n",
      "Nonadaptive error:  0.20293181916820796\n",
      "Adaptive error:  0.6036211679384307\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10934280241940651 0.11761739540693039\n",
      "Training clf_holdout...\n",
      "Scores: 0.10080697196757765 0.11049652999058879\n",
      "Scores: 0.11589347431704991 0.10303737106819884\n",
      "Scores: 0.0980861615766933 0.09269871413757645\n",
      "Number of iterations:  14\n",
      "Nonadaptive error:  0.05795254771774086\n",
      "Adaptive error:  0.015382041591171897\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.10201966929865436 0.10325155560837715\n",
      "Training clf_holdout...\n",
      "Scores: 0.10566540837196012 0.12770585306066834\n",
      "Scores: 0.10960361440415883 0.1077119873592571\n",
      "Scores: 0.11590535748607082 0.11261246693922503\n",
      "Number of iterations:  19\n",
      "Nonadaptive error:  0.07162143675552524\n",
      "Adaptive error:  0.1645374630858819\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.10476931318749647 0.10965060949336478\n",
      "Training clf_holdout...\n",
      "Scores: 0.10775717640529982 0.10272689171358104\n",
      "Scores: 0.10582017975685362 0.10699581563825482\n",
      "Scores: 0.12143118564572122 0.10544302482400345\n",
      "Number of iterations:  22\n",
      "Nonadaptive error:  0.07749388221402928\n",
      "Adaptive error:  0.13964671474698562\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 9\n",
      "Training clf_train...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[377], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Obtain classifiers\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining clf_train...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m list_of_score_train, list_of_clf_train, list_of_bestk_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sparse_ML_transformed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_X_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m clf_train \u001b[38;5;241m=\u001b[39m list_of_clf_train[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m k_train \u001b[38;5;241m=\u001b[39m list_of_bestk_train[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[358], line 191\u001b[0m, in \u001b[0;36mtrain_sparse_ML_transformed\u001b[0;34m(all_X_list, all_values, test_size, random_seed)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;66;03m# ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m15\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m14\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m12\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m11\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)]:\n\u001b[0;32m--> 191\u001b[0m                 score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mML_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneg_root_mean_squared_error\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m#                 print(score)\u001b[39;00m\n\u001b[1;32m    193\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m best_cv_score \u001b[38;5;241m>\u001b[39m score:\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:1004\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m     this_Xy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m _, this_coef, this_dual_gap, this_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43malphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthis_Xy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# from here on **params\u001b[39;49;00m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m coef_[k] \u001b[38;5;241m=\u001b[39m this_coef[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1029\u001b[0m dual_gaps_[k] \u001b[38;5;241m=\u001b[39m this_dual_gap[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/imp_qml/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631\u001b[0m, in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    617\u001b[0m     model \u001b[38;5;241m=\u001b[39m cd_fast\u001b[38;5;241m.\u001b[39menet_coordinate_descent_gram(\n\u001b[1;32m    618\u001b[0m         coef_,\n\u001b[1;32m    619\u001b[0m         l1_reg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         positive,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m precompute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcd_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menet_coordinate_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecompute should be one of True, False, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or array-like. Got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;241m%\u001b[39m precompute\n\u001b[1;32m    638\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "\n",
    "# Ising model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,6))\n",
    "\n",
    "for s, sample_size in enumerate([20, 60, 100, 150, 200, 500]):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_iter):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ec3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ising_homog = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 500])\n",
    "df_ising_homog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d771c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp_qml",
   "language": "python",
   "name": "imp_qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
