{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24434da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import ast\n",
    "import math\n",
    "import scipy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sigI = np.array([[1.0, 0.0j], [0.0j, 1.0]])\n",
    "sigX = np.array([[0.0j, 1.0], [1.0, 0.0j]])\n",
    "sigY = np.array([[0.0j, -1.0j], [1.0j, 0.0j]])\n",
    "sigZ = np.array([[1.0, 0.0j], [0.0j, -1.0]])\n",
    "\n",
    "N = 50\n",
    "\n",
    "def kron(ls):\n",
    "    A = ls[0]\n",
    "    for X in ls[1:]:\n",
    "        A = np.kron(A, X)\n",
    "    return A\n",
    "\n",
    "def generate_all_zero_state():\n",
    "    return [np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_one_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) for i in range(N)]\n",
    "\n",
    "def generate_half_half_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i < N/2 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_neel_state():\n",
    "    return [np.array([[0.0, 0.0j], [0.0j, 1.0]]) if i % 2 == 0 else np.array([[1.0, 0.0j], [0.0j, 0.0]]) for i in range(N)]\n",
    "\n",
    "def generate_all_plus_state():\n",
    "    return [np.array([[0.5, 0.5], [0.5, 0.5+0.0j]]) for i in range(N)]\n",
    "\n",
    "def generate_random_product_state():\n",
    "    list_rhoi = []\n",
    "    for i in range(N):\n",
    "        v = np.random.normal(size=3)\n",
    "        v /= np.linalg.norm(v)\n",
    "        rhoi = sigI / 2.0 + (v[0] * sigX / 2.0) + (v[1] * sigY / 2.0) + (v[2] * sigZ / 2.0)\n",
    "        list_rhoi.append(rhoi)\n",
    "    return list_rhoi\n",
    "\n",
    "def twobytwo_to_Pauli(list_rhoi):\n",
    "    list_rhoi_new = []\n",
    "    for rhoi in list_rhoi:\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "        list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "    return list_rhoi_new\n",
    "\n",
    "def get_RDM_in_Pauli(list_rhoi, k):\n",
    "    feat_vec = []\n",
    "    for i in range(N-k+1):\n",
    "        for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "            val = 1.0\n",
    "            for c, P in enumerate(list_P):\n",
    "                if P == -1: continue\n",
    "                val *= list_rhoi[(3*(i+c))+P]\n",
    "            assert(np.abs(val.imag) < 1e-7)\n",
    "            feat_vec.append(val.real)\n",
    "    return feat_vec\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML(all_states, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "        print(\"Pos:\", pos)\n",
    "\n",
    "        def twobytwo_to_Pauli(list_rhoi):\n",
    "            list_rhoi_new = []\n",
    "            for rhoi in list_rhoi:\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigX, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigY, rhoi)).real)\n",
    "                list_rhoi_new.append(np.trace(np.matmul(sigZ, rhoi)).real)\n",
    "            return list_rhoi_new\n",
    "\n",
    "        def get_RDM_in_Pauli(list_rhoi, k):\n",
    "            feat_vec = []\n",
    "            for i in range(N-k+1):\n",
    "                for list_P in itertools.product([-1, 0, 1, 2], repeat=k):\n",
    "                    val = 1.0\n",
    "                    for c, P in enumerate(list_P):\n",
    "                        if P == -1: continue\n",
    "                        val *= list_rhoi[(3*(i+c))+P]\n",
    "                    assert(np.abs(val.imag) < 1e-7)\n",
    "                    feat_vec.append(val.real)\n",
    "            return feat_vec\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_states)), range(len(all_states)), test_size=test_size, random_state=random_seed)\n",
    "\n",
    "        for k in [1, 2, 3, 4]:\n",
    "            print(\"Validate k =\", k)\n",
    "            X, y_true, y_noisy = [], [], []\n",
    "\n",
    "            for data in zip(all_states, all_values):\n",
    "                X.append(get_RDM_in_Pauli(data[0], k))\n",
    "                y_true.append(data[1][pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[1][pos]+1)/2, 1)[0] / 500) - 1)\n",
    "\n",
    "            X = np.array(X)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "                print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "# Train a sparsity-enforcing ML model\n",
    "def train_sparse_ML_transformed(all_X_list, all_values, test_size = 0.25, random_seed = 0):\n",
    "    list_of_score = []\n",
    "    list_of_clf = []\n",
    "    list_of_bestk = []\n",
    "\n",
    "    for pos in range(0, len(all_values[0])):\n",
    "#         print(\"Pos:\", pos)\n",
    "\n",
    "        best_cv_score = 999.0\n",
    "        best_clf = None\n",
    "        best_k = None\n",
    "\n",
    "        _, test_idx, _, _ = train_test_split(range(len(all_values)), range(len(all_values)), test_size=test_size, random_state=random_seed)\n",
    "        \n",
    "        for k in [1, 2]:\n",
    "#             print(\"Validate k =\", k)\n",
    "\n",
    "            X = all_X_list[k-1]\n",
    "            \n",
    "            y_true, y_noisy = [], []\n",
    "            for data in all_values:\n",
    "                y_true.append(data[pos])\n",
    "                y_noisy.append((2 * np.random.binomial(500, (data[pos]+1)/2, 1)[0] / 500) - 1)\n",
    "            y_true = np.array(y_true)\n",
    "            y_noisy = np.array(y_noisy)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx)\n",
    "            # ML_method = lambda Cx: linear_model.Ridge(alpha=Cx)\n",
    "\n",
    "            for alpha in [2**(-15), 2**(-14), 2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7), 2**(-6), 2**(-5), 2**(-4), 2**(-3)]:\n",
    "                score = -np.mean(cross_val_score(ML_method(alpha), X_train, y_train, cv=2, scoring=\"neg_root_mean_squared_error\"))\n",
    "#                 print(score)\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(alpha).fit(X_train, y_train)\n",
    "\n",
    "                    best_cv_score = score\n",
    "                    best_clf = clf\n",
    "                    best_k = k\n",
    "\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    test_score = np.linalg.norm(y_pred - y_true[test_idx]) / (len(y_pred) ** 0.5)\n",
    "\n",
    "        print(\"Scores:\", best_cv_score, test_score)\n",
    "        list_of_score.append(test_score)\n",
    "        list_of_clf.append(best_clf)\n",
    "        list_of_bestk.append(best_k)\n",
    "        \n",
    "    return list_of_score, list_of_clf, list_of_bestk\n",
    "\n",
    "def transform_states(all_states):\n",
    "    all_X_list = []\n",
    "    \n",
    "    for k in [1, 2]:\n",
    "        X = []\n",
    "        for data in all_states:\n",
    "            X.append(get_RDM_in_Pauli(data, k))\n",
    "        all_X_list.append(np.array(X))\n",
    "    return all_X_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de994661",
   "metadata": {},
   "source": [
    "# XY model with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f7e29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "sample_size = 100\n",
    "all_data_training_set_scaling = []\n",
    "seed = 0\n",
    "test_size = 0.5\n",
    "num_holdout = 1\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "\n",
    "# Train / holdout split\n",
    "np.random.seed(seed)\n",
    "sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "train_states, train_values = np.array(all_states)[\n",
    "    sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "\n",
    "# I setup the framework for multiple holdout but right now still should be the same as using \n",
    "# only one holdout in my current code below\n",
    "holdout_states_ens, holdout_values_ens = [],[]\n",
    "for i in range(1, num_holdout+1):\n",
    "    holdout_states, holdout_values = np.array(all_states)[\n",
    "        sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "    holdout_states_ens.append(holdout_states)\n",
    "    holdout_values_ens.append(holdout_values)\n",
    "h = holdout_states_ens[0]\n",
    "\n",
    "train_X_list = transform_states(train_states)\n",
    "holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a858eaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.15589832750540034 0.15131445874505367\n",
      "Training clf_holdout...\n",
      "Scores: 0.14870408289542514 0.13186634459285798\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Obtain classifiers\n",
    "print(\"Training clf_train...\")\n",
    "list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "    train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "clf_train = list_of_clf_train[0]\n",
    "k_train = list_of_bestk_train[0]\n",
    "\n",
    "print(\"Training clf_holdout...\")\n",
    "clf_holdout_ens = []\n",
    "score_holdout_ens = []\n",
    "k_holdout_ens = []\n",
    "for i in range(num_holdout):\n",
    "    list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "        holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "    clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "    score_holdout_ens.append(list_of_score_holdout[0])\n",
    "    k_holdout_ens.append(list_of_bestk_holdout[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf191108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# Adaptive optimization\n",
    "\n",
    "def predict_holdout(x):\n",
    "    # Get holdout prediction\n",
    "    predictions = []\n",
    "    for i in range(num_holdout):\n",
    "        # Get best classifier with best k\n",
    "        clf_holdout = clf_holdout_ens[i]\n",
    "        k_holdout = k_holdout_ens[i]\n",
    "        X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "        predictions.append(clf_holdout.predict(X_holdout))\n",
    "    return predictions[0][0]\n",
    "\n",
    "def predict_train(x):\n",
    "    # Get train prediction\n",
    "    X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "    return clf_train.predict(X_train)[0]\n",
    "\n",
    "def get_product_state(y):\n",
    "    # Compute product state from input spherical coordinates\n",
    "    x = []\n",
    "    for i in range(N):\n",
    "        phi = y[i]\n",
    "        theta = y[N+i]\n",
    "        x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "    return x\n",
    "\n",
    "def objective(y):\n",
    "    # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -abs(y_train - y_holdout)\n",
    "\n",
    "def objective_train(y):\n",
    "    # Maximizes train predictions\n",
    "    x = get_product_state(y)\n",
    "    y_train = predict_train(x)\n",
    "    return -y_train\n",
    "\n",
    "def objective_holdout(y):\n",
    "    x = get_product_state(y)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return -y_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4de3f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02989498178038802\n",
      "-0.003952952740798337\n",
      "-0.003984906875219496\n",
      "-0.012859176363692045\n",
      "-0.014430906301153894\n",
      "-0.042361020431807984\n",
      "-0.035377841828585624\n",
      "0.0034870980550325732\n",
      "-0.008008942019106981\n",
      "-0.0017956570916227675\n",
      "0.0003991383187626693\n",
      "0.010380826212937417\n",
      "-0.013539911701509161\n",
      "-0.040601530581937134\n",
      "0.01369567055487688\n",
      "0.007523707802490575\n",
      "-0.007727939983817527\n",
      "0.0020305517304285946\n",
      "-0.0331312661791566\n",
      "-0.03786184743731322\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "def objective_state(x):\n",
    "    y_train = predict_train(x)\n",
    "    y_holdout = predict_holdout(x)\n",
    "    return abs(y_train - y_holdout)\n",
    "\n",
    "# max_err = 0\n",
    "# max_idx = 0\n",
    "# avg_err = 0\n",
    "# print(\"Print: max error, index of maximum error, average error\")\n",
    "# np.random.seed(seed)\n",
    "# for i in range(10000):\n",
    "#     y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "#     if i % 500 == 0:\n",
    "#         print(f\"step {i}: {max_err}, {max_idx}, {avg_err}\")\n",
    "#     err = -objective(y0)\n",
    "#     if err > max_err:\n",
    "#         max_err = err\n",
    "#         max_idx = i\n",
    "#     avg_err += err / 10000\n",
    "# print(max_err)\n",
    "\n",
    "# Sometimes I get something like this, i.e. classifier always outputs the same number\n",
    "for i in range(10000):\n",
    "    if i % 500 == 0:\n",
    "        print(predict_holdout(all_states[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afcb904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.853297\n",
      "         Iterations: 5\n",
      "         Function evaluations: 7346\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: -0.8532967224410052\n",
      "       x: [ 1.527e+00  1.793e+00 ...  4.720e+00  1.571e+00]\n",
      "     nit: 5\n",
      "   direc: [[ 1.000e+00  0.000e+00 ...  0.000e+00  0.000e+00]\n",
      "           [ 0.000e+00  1.000e+00 ...  0.000e+00  0.000e+00]\n",
      "           ...\n",
      "           [ 0.000e+00  0.000e+00 ...  1.000e+00  0.000e+00]\n",
      "           [ 9.658e-08 -2.862e-07 ... -8.315e-17 -1.507e-18]]\n",
      "    nfev: 7346\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize, fmin_powell\n",
    "\n",
    "# Nelder-mead\n",
    "np.random.seed(seed)\n",
    "bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N)\n",
    "y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "res = minimize(objective_train, y0, method='Powell', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective_train, y0, method='Nelder-Mead', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': True})\n",
    "# res = minimize(objective_train, y0, method='trust-constr', bounds=bnds, options={'disp': True})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8870497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8701776855146985\n",
      "7346\n"
     ]
    }
   ],
   "source": [
    "# Print the error, i.e. if large, then we have overfit\n",
    "state = get_product_state(res.x)\n",
    "y_train = predict_train(state)\n",
    "y_holdout = predict_holdout(state)\n",
    "print(abs(y_train-y_holdout))\n",
    "print(res.nfev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c760504",
   "metadata": {},
   "source": [
    "Repeat this for many initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4cdc9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.16462395921915518 0.12911599216449912\n",
      "Training clf_holdout...\n",
      "Scores: 0.16864085852093336 0.1638682023767973\n",
      "Scores: 0.0863293194447738 0.20104740214809017\n",
      "Scores: 0.14473888498874282 0.16870663097824282\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.0532\n",
      "Adaptive error:  0.0532\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.2072709887596873 0.21446674440225758\n",
      "Training clf_holdout...\n",
      "Scores: 0.08619344518090447 0.08878962089002698\n",
      "Scores: 0.08500319447232102 0.17236315656378495\n",
      "Scores: 0.1449320146221621 0.16573287256100375\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.23273078130859914\n",
      "Adaptive error:  0.1252\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.1809684234136905 0.18331344719082018\n",
      "Training clf_holdout...\n",
      "Scores: 0.1403567738076975 0.08566088281898569\n",
      "Scores: 0.16105392298779414 0.1831447131138543\n",
      "Scores: 0.13583516214519836 0.12564461664818954\n",
      "Number of iterations:  5\n",
      "Nonadaptive error:  0.44161968490665887\n",
      "Adaptive error:  0.2978185135491464\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.175982966402861 0.11105624734869712\n",
      "Training clf_holdout...\n",
      "Scores: 0.04013721725212728 0.19363859569769915\n",
      "Scores: 0.16019606354985289 0.14635228884159623\n",
      "Scores: 0.07063405355075479 0.13635777949062197\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.06359999999999998\n",
      "Adaptive error:  0.06359999999999998\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.14268979401810927 0.17783165819566557\n",
      "Training clf_holdout...\n",
      "Scores: 0.19296780332002683 0.12887756871647213\n",
      "Scores: 0.24853630355917877 0.12606893007071823\n",
      "Scores: 0.07351387253089108 0.16065505005169772\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.17495991357373739\n",
      "Adaptive error:  0.14333160816368468\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13509075655054445 0.16837202375885688\n",
      "Training clf_holdout...\n",
      "Scores: 0.1195047528157996 0.17792089551869392\n",
      "Scores: 0.10740547151156657 0.12469586596645843\n",
      "Scores: 0.14236989787817808 0.2638616277362352\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.347612048596656\n",
      "Adaptive error:  0.35370945426761136\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10885345207292059 0.15632694534513636\n",
      "Training clf_holdout...\n",
      "Scores: 0.12319106771247024 0.19324010704292988\n",
      "Scores: 0.20401838151845067 0.19776439520357691\n",
      "Scores: 0.13308807100012202 0.20926838530754902\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.0919813737250018\n",
      "Adaptive error:  0.09198355645467815\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.10454798493891745 0.21761204899018355\n",
      "Training clf_holdout...\n",
      "Scores: 0.14585781760366212 0.16278003691557005\n",
      "Scores: 0.16007748109630654 0.15012378471654717\n",
      "Scores: 0.12860890679730913 0.1651680878705387\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.3417044821636681\n",
      "Adaptive error:  0.15550491820032153\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.08998591458903421 0.21443212004228746\n",
      "Training clf_holdout...\n",
      "Scores: 0.08217725243353137 0.19983742607681307\n",
      "Scores: 0.14257182176748862 0.2201229514417737\n",
      "Scores: 0.187214330325206 0.23952354897930744\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.26589994445270804\n",
      "Adaptive error:  0.05691717005975987\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.2223790010678297 0.12862899809987552\n",
      "Training clf_holdout...\n",
      "Scores: 0.1714315684282035 0.15632104376258893\n",
      "Scores: 0.120980399300163 0.14845040799232992\n",
      "Scores: 0.08498578164605877 0.15856813117118332\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.4343912757384337\n",
      "Adaptive error:  0.5553149792959203\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.16555487373386424 0.19823974160422003\n",
      "Training clf_holdout...\n",
      "Scores: 0.141660943390973 0.1424848792856864\n",
      "Scores: 0.1440843883641621 0.16399673358952815\n",
      "Scores: 0.1922018634882003 0.11948626370323376\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.3841569767245542\n",
      "Adaptive error:  0.5990478201380551\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.15888564136623867 0.16763499370116888\n",
      "Training clf_holdout...\n",
      "Scores: 0.1705993673095105 0.15079396489312846\n",
      "Scores: 0.1570716548173578 0.14379915664947904\n",
      "Scores: 0.12519898368939078 0.14546767317877948\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.012533333333333344\n",
      "Adaptive error:  0.0007110483309804311\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.16818849681507425 0.13214870555968972\n",
      "Training clf_holdout...\n",
      "Scores: 0.13761139979248033 0.13915013333977347\n",
      "Scores: 0.12014570998548377 0.15861445312600964\n",
      "Scores: 0.15593355501785194 0.15050934028276122\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.15411254080970954\n",
      "Adaptive error:  0.11201264613953774\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.1820802459563976 0.13775268208915606\n",
      "Training clf_holdout...\n",
      "Scores: 0.12069103145809984 0.13554635269114024\n",
      "Scores: 0.15407228265218664 0.12956972189610946\n",
      "Scores: 0.14925864700674052 0.14576529758118204\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.015866666666666668\n",
      "Adaptive error:  0.015866666666666668\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.14696839109835996 0.15162113508459343\n",
      "Training clf_holdout...\n",
      "Scores: 0.1393688097158235 0.13858201082045066\n",
      "Scores: 0.1566144799211649 0.1303837796036408\n",
      "Scores: 0.1417586735741313 0.16228733285042535\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.16660594269772774\n",
      "Adaptive error:  0.1635327477143081\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13119172706120452 0.16817718022893072\n",
      "Training clf_holdout...\n",
      "Scores: 0.15993806724380077 0.1794790727133574\n",
      "Scores: 0.1697509959369361 0.1608306021832971\n",
      "Scores: 0.15569988497243037 0.17205663579034156\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.42574279270605636\n",
      "Adaptive error:  0.7777956841519307\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.17034777865109646 0.14938096391856556\n",
      "Training clf_holdout...\n",
      "Scores: 0.14377927468101465 0.14523650350992234\n",
      "Scores: 0.12289978693152562 0.14458636796148386\n",
      "Scores: 0.14629142504509568 0.16352684889904887\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.4318480582443247\n",
      "Adaptive error:  0.6552104056011118\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.15011060667146217 0.17252408000049313\n",
      "Training clf_holdout...\n",
      "Scores: 0.13982757810413382 0.1872734984627613\n",
      "Scores: 0.1807891977501835 0.1622465558016876\n",
      "Scores: 0.16660967675533167 0.14202476756963373\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.27977702395804005\n",
      "Adaptive error:  0.14148926487311003\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.1763851908878838 0.17566595600731733\n",
      "Training clf_holdout...\n",
      "Scores: 0.1701751136835165 0.1728061907413417\n",
      "Scores: 0.19500484682849573 0.14930167210051895\n",
      "Scores: 0.1386937217165918 0.15517445095447677\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.025683711451514133\n",
      "Adaptive error:  0.025683714690468526\n",
      "--------------------------------------\n",
      "ITERATION: sample size 60, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.1651918622756881 0.14287307189048618\n",
      "Training clf_holdout...\n",
      "Scores: 0.14137518734864263 0.15371890550239564\n",
      "Scores: 0.14678138746387148 0.15681479716949173\n",
      "Scores: 0.13001691955689407 0.17412677105740096\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.08229520156725019\n",
      "Adaptive error:  0.037110457977320016\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.15589832750540034 0.15131445874505367\n",
      "Training clf_holdout...\n",
      "Scores: 0.14870408289542514 0.13186634459285798\n",
      "Scores: 0.13734130296602953 0.1646625965324173\n",
      "Scores: 0.14178673063317782 0.15393353032467216\n",
      "Number of iterations:  5\n",
      "Nonadaptive error:  0.4951881640908899\n",
      "Adaptive error:  0.8431367224410052\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.1375474375001225 0.13718042207371414\n",
      "Training clf_holdout...\n",
      "Scores: 0.13555574058699071 0.15245610607401194\n",
      "Scores: 0.11632517897192995 0.1364837021705868\n",
      "Scores: 0.14261656148971202 0.12830740133625182\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.34852333110356876\n",
      "Adaptive error:  0.5767345374679959\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.1478818171210039 0.1286202090263973\n",
      "Training clf_holdout...\n",
      "Scores: 0.13537471906774196 0.1569042127146887\n",
      "Scores: 0.14380480813076657 0.15118432762963688\n",
      "Scores: 0.16254952036131576 0.1569401142872965\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.23496799862898082\n",
      "Adaptive error:  0.22143997095390938\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.1469757334171944 0.1318439025453525\n",
      "Training clf_holdout...\n",
      "Scores: 0.1496016555189292 0.12778865616920843\n",
      "Scores: 0.17848887261947383 0.13586761392013824\n",
      "Scores: 0.1389239499585853 0.15041384736157626\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.11091758715193178\n",
      "Adaptive error:  0.06031053344323166\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.14391429906234265 0.13438062574987794\n",
      "Training clf_holdout...\n",
      "Scores: 0.12828914662071625 0.1601820428813584\n",
      "Scores: 0.15882422443136474 0.1451121645638212\n",
      "Scores: 0.12994582658013756 0.13070737730542428\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.4281742429544842\n",
      "Adaptive error:  0.6924591816361497\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.15723430000564598 0.1573196732709076\n",
      "Training clf_holdout...\n",
      "Scores: 0.16201712496452997 0.15082569305894\n",
      "Scores: 0.1380222499413377 0.14064614959598498\n",
      "Scores: 0.16750559442811158 0.13506653099184937\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.17050834226257705\n",
      "Adaptive error:  0.031152210618410454\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.14501883873708188 0.13832446755646977\n",
      "Training clf_holdout...\n",
      "Scores: 0.16270213049069154 0.13338717139161504\n",
      "Scores: 0.15836346843349872 0.17397788371344086\n",
      "Scores: 0.14827766308468668 0.15909237580445423\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.23796776771396402\n",
      "Adaptive error:  0.2703467816848317\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.15160340838836717 0.13183852544736566\n",
      "Training clf_holdout...\n",
      "Scores: 0.17974007884401338 0.1639035353679017\n",
      "Scores: 0.13779007914406338 0.1523782226484732\n",
      "Scores: 0.11671209418415282 0.17214397672873188\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.37171388723174476\n",
      "Adaptive error:  0.6162076159602673\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.16368886155289333 0.14689167492773003\n",
      "Training clf_holdout...\n",
      "Scores: 0.15860704709118553 0.14318461084215478\n",
      "Scores: 0.11471078483772959 0.12770047152078012\n",
      "Scores: 0.1364113588717713 0.13695293199685393\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.2919581526659215\n",
      "Adaptive error:  0.3738292928963682\n",
      "--------------------------------------\n",
      "ITERATION: sample size 100, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.15715381656013097 0.15135445472044126\n",
      "Training clf_holdout...\n",
      "Scores: 0.1362092081621273 0.152458229491861\n",
      "Scores: 0.13744539662584718 0.1393049062081293\n",
      "Scores: 0.1296175737438447 0.17282515475170604\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.08038038140850762\n",
      "Adaptive error:  0.02197383940285609\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.1479387347802868 0.16646960747008344\n",
      "Training clf_holdout...\n",
      "Scores: 0.1685471490026643 0.15084122629766777\n",
      "Scores: 0.14383329140782766 0.14429085379983567\n",
      "Scores: 0.16342211740683255 0.15248657740010596\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.46008641978673637\n",
      "Adaptive error:  0.9310312789339767\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.14913293626441548 0.16838420691133435\n",
      "Training clf_holdout...\n",
      "Scores: 0.1393600816459571 0.12207893257563383\n",
      "Scores: 0.1387179017603445 0.13509436640876799\n",
      "Scores: 0.1391943405374363 0.16661359317140095\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.11804762956715617\n",
      "Adaptive error:  0.11832583539461149\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.14708641399973 0.14525645112994293\n",
      "Training clf_holdout...\n",
      "Scores: 0.14931961826312784 0.1400980301447811\n",
      "Scores: 0.12486677444150567 0.1215001227858097\n",
      "Scores: 0.12681446198817625 0.1394075077620706\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.30020077594208283\n",
      "Adaptive error:  0.3538785110196023\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.11241439648038853 0.1384260872995269\n",
      "Training clf_holdout...\n",
      "Scores: 0.1746166274691529 0.11090983648830133\n",
      "Scores: 0.14387864599716138 0.1280450606894335\n",
      "Scores: 0.14427036520754882 0.14255780188445832\n",
      "Number of iterations:  6\n",
      "Nonadaptive error:  0.23541987601369346\n",
      "Adaptive error:  0.3491280152270672\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.1449708692178256 0.12333904515595287\n",
      "Training clf_holdout...\n",
      "Scores: 0.13514624391539928 0.1432633743362042\n",
      "Scores: 0.13172588255596898 0.1318679917297249\n",
      "Scores: 0.15528055365736204 0.1334147147358206\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.19018263590761225\n",
      "Adaptive error:  0.2574299474876148\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.16846948272141132 0.17257177964483242\n",
      "Training clf_holdout...\n",
      "Scores: 0.13832513795592982 0.13275433354526694\n",
      "Scores: 0.15583077135122667 0.15429630578408945\n",
      "Scores: 0.16015142499547563 0.13129802090270049\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.2912612610325135\n",
      "Adaptive error:  0.31472908894863233\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.1326032998952112 0.12671306646671732\n",
      "Training clf_holdout...\n",
      "Scores: 0.15098438446198 0.1453395642638317\n",
      "Scores: 0.1374683571624445 0.14758191634666212\n",
      "Scores: 0.14566000761021008 0.14011140966784272\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.3404876500212821\n",
      "Adaptive error:  0.4293027688960066\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.14867374275968975 0.14425640562480246\n",
      "Training clf_holdout...\n",
      "Scores: 0.155457965465285 0.13841211053310332\n",
      "Scores: 0.1240095480913331 0.1302479632013891\n",
      "Scores: 0.1680514312563095 0.11987145649338764\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.3681455680185271\n",
      "Adaptive error:  0.6142094563791937\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.15054854328102177 0.12054995778600819\n",
      "Training clf_holdout...\n",
      "Scores: 0.14128008365624967 0.14043832724232216\n",
      "Scores: 0.14461526977417177 0.14718666460979463\n",
      "Scores: 0.14941360668676515 0.14279052835550005\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.38537978954997526\n",
      "Adaptive error:  0.6298055877568519\n",
      "--------------------------------------\n",
      "ITERATION: sample size 150, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.13784210416652992 0.13348681508463764\n",
      "Training clf_holdout...\n",
      "Scores: 0.14652635671595188 0.15042273873022205\n",
      "Scores: 0.14638309609574823 0.1564690902546255\n",
      "Scores: 0.11433627746977837 0.13087239946061455\n",
      "Number of iterations:  5\n",
      "Nonadaptive error:  0.40443144117993846\n",
      "Adaptive error:  1.1422045142396877\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.14665970304927345 0.1546261114204475\n",
      "Training clf_holdout...\n",
      "Scores: 0.15323832080019648 0.09777384937748847\n",
      "Scores: 0.13279389904077338 0.12914604689016335\n",
      "Scores: 0.12834693236556408 0.13222067402501517\n",
      "Number of iterations:  1\n",
      "Nonadaptive error:  0.3243987031740513\n",
      "Adaptive error:  0.3649345299141259\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.1401101410576877 0.13347575744401455\n",
      "Training clf_holdout...\n",
      "Scores: 0.1389489629413892 0.10536219673773928\n",
      "Scores: 0.1401394768764822 0.11485893801223553\n",
      "Scores: 0.15897617657141874 0.14833513874832796\n",
      "Number of iterations:  5\n",
      "Nonadaptive error:  0.4439648329103598\n",
      "Adaptive error:  0.9499980662467538\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.13230909520948106 0.13321333624582754\n",
      "Training clf_holdout...\n",
      "Scores: 0.1378529745718604 0.13011832505519666\n",
      "Scores: 0.12474773422861843 0.11249704667360469\n",
      "Scores: 0.13038636811825943 0.12807211173148114\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.23452017507400588\n",
      "Adaptive error:  0.14502297587194107\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.12175644823354403 0.15522384822956414\n",
      "Training clf_holdout...\n",
      "Scores: 0.1335681463598531 0.15253023108868988\n",
      "Scores: 0.13104104198230832 0.11372867742992347\n",
      "Scores: 0.13023533442039706 0.10960367834618598\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.32988709721127485\n",
      "Adaptive error:  0.15519273090135713\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.13772873544047007 0.1438376404894916\n",
      "Training clf_holdout...\n",
      "Scores: 0.13695073027423976 0.12326064872924032\n",
      "Scores: 0.13515482946493182 0.20626350776082408\n",
      "Scores: 0.14673614452644632 0.1418283946558288\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.428931672261208\n",
      "Adaptive error:  0.06878190329856876\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.1626288730411735 0.16154153556270356\n",
      "Training clf_holdout...\n",
      "Scores: 0.13226431803836447 0.12735627335117627\n",
      "Scores: 0.14794945641397053 0.15050621378977563\n",
      "Scores: 0.11403486630184273 0.11665671844529565\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.23123644808603186\n",
      "Adaptive error:  0.2363681011739792\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.12712316323518238 0.109223075491701\n",
      "Training clf_holdout...\n",
      "Scores: 0.13796311250446275 0.14310151469507298\n",
      "Scores: 0.14115566600350793 0.11300492337674617\n",
      "Scores: 0.15052893189272984 0.12244303848761429\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.32182866146248057\n",
      "Adaptive error:  0.2874394025230087\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.14973980022695205 0.13091995923161992\n",
      "Training clf_holdout...\n",
      "Scores: 0.14770182340627197 0.13794721668739637\n",
      "Scores: 0.1335726249631717 0.10822819344675003\n",
      "Scores: 0.14176293279875643 0.1002975897517078\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.19825197104271766\n",
      "Adaptive error:  0.21061009944509493\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.16437901377669756 0.15905013933507522\n",
      "Training clf_holdout...\n",
      "Scores: 0.12915636655639778 0.09931780361293749\n",
      "Scores: 0.1300088763258888 0.13277895630767278\n",
      "Scores: 0.1364255622285626 0.1215037024683007\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.3034160178361073\n",
      "Adaptive error:  0.38067995009270017\n",
      "--------------------------------------\n",
      "ITERATION: sample size 200, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.13690797273012742 0.14591352685451206\n",
      "Training clf_holdout...\n",
      "Scores: 0.14318883936782653 0.11780260212421116\n",
      "Scores: 0.16570253464400508 0.1294083844007487\n",
      "Scores: 0.12510706113789072 0.11941015322773078\n",
      "Number of iterations:  2\n",
      "Nonadaptive error:  0.28958907306735576\n",
      "Adaptive error:  0.15612380507304405\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.12300009059495243 0.11183348028214991\n",
      "Training clf_holdout...\n",
      "Scores: 0.13180787312318248 0.12419351615503255\n",
      "Scores: 0.13910556481114197 0.12268858265777582\n",
      "Scores: 0.11414419300132822 0.10421271739961531\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.2262421785403923\n",
      "Adaptive error:  0.11931774760374408\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.12704122261449793 0.11182437034145366\n",
      "Training clf_holdout...\n",
      "Scores: 0.12238104017264728 0.09654341188543401\n",
      "Scores: 0.12481609912638128 0.10541898135275442\n",
      "Scores: 0.12139395079948133 0.10216579598230602\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.25539975707062723\n",
      "Adaptive error:  0.11316111552418107\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.13152990827847913 0.10235560106688697\n",
      "Training clf_holdout...\n",
      "Scores: 0.13155891447581275 0.11772685143019729\n",
      "Scores: 0.13125656789504986 0.09634186729343458\n",
      "Scores: 0.1319156384300409 0.09016894638262468\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.21707223421210176\n",
      "Adaptive error:  0.23251228586351969\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.1267098892382392 0.10664604042948611\n",
      "Training clf_holdout...\n",
      "Scores: 0.12146445563295236 0.10669732698046959\n",
      "Scores: 0.11702778665432978 0.0972626466750422\n",
      "Scores: 0.12789662688067316 0.10316611744503831\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.24765159489740185\n",
      "Adaptive error:  0.2649557693622742\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.1329365527887185 0.10213681811178615\n",
      "Training clf_holdout...\n",
      "Scores: 0.1279335156863528 0.10374819576384277\n",
      "Scores: 0.13104241542284903 0.12414023165803549\n",
      "Scores: 0.14194244236272288 0.10756015741248204\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.24182555057151933\n",
      "Adaptive error:  0.2867684636423842\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.11718523557713631 0.10122262190743928\n",
      "Training clf_holdout...\n",
      "Scores: 0.12651728077577817 0.10639333339483864\n",
      "Scores: 0.12032889306066272 0.11662732215906624\n",
      "Scores: 0.11620293080050051 0.09981946734339144\n",
      "Number of iterations:  5\n",
      "Nonadaptive error:  0.22541834293143456\n",
      "Adaptive error:  0.25063847272801787\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.1201289913545193 0.106549406624122\n",
      "Training clf_holdout...\n",
      "Scores: 0.128945193213361 0.09643931467343148\n",
      "Scores: 0.14179457406603166 0.10622090012170088\n",
      "Scores: 0.12944367854909106 0.13150875121579259\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.2303612609602093\n",
      "Adaptive error:  0.19083618813063435\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.12472040498214226 0.11352037092310838\n",
      "Training clf_holdout...\n",
      "Scores: 0.13983990279266567 0.09477112510013908\n",
      "Scores: 0.1261094923407444 0.08598981000099419\n",
      "Scores: 0.1353802775610892 0.12144118271409686\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.2164057953755392\n",
      "Adaptive error:  0.05686872973232565\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.1244499945609808 0.09963449796154916\n",
      "Training clf_holdout...\n",
      "Scores: 0.13362573954748186 0.11726803454699203\n",
      "Scores: 0.12910393179929938 0.09553973697227391\n",
      "Scores: 0.1176210889573838 0.10152221604810802\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.24420624083751968\n",
      "Adaptive error:  0.31088795156613414\n",
      "--------------------------------------\n",
      "ITERATION: sample size 300, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.1354655091870967 0.10848101116302457\n",
      "Training clf_holdout...\n",
      "Scores: 0.12572805256268133 0.11208175753684937\n",
      "Scores: 0.12990019007678996 0.09666987534504838\n",
      "Scores: 0.1276318606964759 0.10702820941781081\n",
      "Number of iterations:  6\n",
      "Nonadaptive error:  0.2694573432345398\n",
      "Adaptive error:  0.2923963753683007\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_train...\n",
      "Scores: 0.11236239948795357 0.08648922108937367\n",
      "Training clf_holdout...\n",
      "Scores: 0.12652912246652248 0.09225199727075499\n",
      "Scores: 0.1027126122869448 0.08243883008596879\n",
      "Scores: 0.11207402966128648 0.08525050599132933\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.1947105416411315\n",
      "Adaptive error:  0.060734067549909176\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.10767408707152004 0.08155067491043927\n",
      "Training clf_holdout...\n",
      "Scores: 0.12128490113247478 0.09377931211753657\n",
      "Scores: 0.11107310651392623 0.09742864601564438\n",
      "Scores: 0.11837350240558399 0.08908663613463784\n",
      "Number of iterations:  19\n",
      "Nonadaptive error:  0.2563370082699524\n",
      "Adaptive error:  0.38627837123453346\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.11878784385839575 0.08809306499705702\n",
      "Training clf_holdout...\n",
      "Scores: 0.11792707550524371 0.09357178709330609\n",
      "Scores: 0.1196443896975045 0.09807316708390365\n",
      "Scores: 0.11674655305416666 0.09230310875533725\n",
      "Number of iterations:  5\n",
      "Nonadaptive error:  0.1514010194934608\n",
      "Adaptive error:  0.1494440766304591\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.10958734885123186 0.08712895070779482\n",
      "Training clf_holdout...\n",
      "Scores: 0.10579313988142722 0.08757774815000555\n",
      "Scores: 0.11366250599754808 0.09035240226951365\n",
      "Scores: 0.10369867570016691 0.08516143020151926\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.13953917142300798\n",
      "Adaptive error:  0.12194639218826453\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.11445580687845831 0.08592954740389386\n",
      "Training clf_holdout...\n",
      "Scores: 0.11173543263369475 0.09198241551019117\n",
      "Scores: 0.1329357460153987 0.09201240730529518\n",
      "Scores: 0.11891000692635596 0.08093360799353559\n",
      "Number of iterations:  6\n",
      "Nonadaptive error:  0.17823866335296992\n",
      "Adaptive error:  0.14634143920258802\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.12354008961075467 0.08175843815272861\n",
      "Training clf_holdout...\n",
      "Scores: 0.10576166447441507 0.08776062887378379\n",
      "Scores: 0.10735879017480349 0.0851410421959011\n",
      "Scores: 0.10694990260864308 0.09298663409895143\n",
      "Number of iterations:  7\n",
      "Nonadaptive error:  0.26700393396252214\n",
      "Adaptive error:  0.5012046009253891\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.10891570981711585 0.09339696229370084\n",
      "Training clf_holdout...\n",
      "Scores: 0.11720228409922907 0.08319692593850249\n",
      "Scores: 0.11129308965465429 0.09701536937208388\n",
      "Scores: 0.11306928814658254 0.08417776719271995\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.1458694709561296\n",
      "Adaptive error:  0.08580686244024616\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.1068040305186918 0.08923220127366423\n",
      "Training clf_holdout...\n",
      "Scores: 0.11148815918630808 0.08582662773676059\n",
      "Scores: 0.11183354245305589 0.08793562970759394\n",
      "Scores: 0.11464996166150251 0.07321275869413066\n",
      "Number of iterations:  4\n",
      "Nonadaptive error:  0.22948448811675323\n",
      "Adaptive error:  0.23158058056367203\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.11645628199508441 0.09456851689319677\n",
      "Training clf_holdout...\n",
      "Scores: 0.09749880247375467 0.09188662111755966\n",
      "Scores: 0.11634736449518465 0.0723484450395279\n",
      "Scores: 0.10510331853108465 0.09678213522358853\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.21648763603453614\n",
      "Adaptive error:  0.17540927413207252\n",
      "--------------------------------------\n",
      "ITERATION: sample size 500, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.11649347770978033 0.08448691600479791\n",
      "Training clf_holdout...\n",
      "Scores: 0.11969285423283936 0.10749888025119061\n",
      "Scores: 0.11326680919054823 0.0926644897367926\n",
      "Scores: 0.11406415796505175 0.09388137332350278\n",
      "Number of iterations:  3\n",
      "Nonadaptive error:  0.16333023861582202\n",
      "Adaptive error:  0.18969165631408036\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statistics\n",
    "from scipy.optimize import minimize\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "sample_size_list = [20, 60, 100, 150, 200, 300, 500]\n",
    "\n",
    "# XY model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,len(sample_size_list)))\n",
    "\n",
    "for s, sample_size in enumerate(sample_size_list):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='Powell', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_eval):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "413af529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>60</th>\n",
       "      <th>100</th>\n",
       "      <th>150</th>\n",
       "      <th>200</th>\n",
       "      <th>300</th>\n",
       "      <th>500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.148908e-01</td>\n",
       "      <td>0.347949</td>\n",
       "      <td>0.470945</td>\n",
       "      <td>0.040536</td>\n",
       "      <td>-0.106924</td>\n",
       "      <td>-0.133976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.107531</td>\n",
       "      <td>-1.182229e-02</td>\n",
       "      <td>0.228211</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.506033</td>\n",
       "      <td>-0.142239</td>\n",
       "      <td>0.129941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.143801</td>\n",
       "      <td>-4.209989e-02</td>\n",
       "      <td>-0.013528</td>\n",
       "      <td>0.053678</td>\n",
       "      <td>-0.089497</td>\n",
       "      <td>0.015440</td>\n",
       "      <td>-0.001957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.050607</td>\n",
       "      <td>0.113708</td>\n",
       "      <td>-0.174694</td>\n",
       "      <td>0.017304</td>\n",
       "      <td>-0.017593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.031628</td>\n",
       "      <td>-3.073195e-03</td>\n",
       "      <td>0.264285</td>\n",
       "      <td>0.067247</td>\n",
       "      <td>-0.360150</td>\n",
       "      <td>0.044943</td>\n",
       "      <td>-0.031897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006097</td>\n",
       "      <td>3.520529e-01</td>\n",
       "      <td>-0.139356</td>\n",
       "      <td>0.023468</td>\n",
       "      <td>0.005132</td>\n",
       "      <td>0.025220</td>\n",
       "      <td>0.234201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.233623e-01</td>\n",
       "      <td>0.032379</td>\n",
       "      <td>0.088815</td>\n",
       "      <td>-0.034389</td>\n",
       "      <td>-0.039525</td>\n",
       "      <td>-0.060063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.186200</td>\n",
       "      <td>-1.382878e-01</td>\n",
       "      <td>0.244494</td>\n",
       "      <td>0.246064</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>-0.159537</td>\n",
       "      <td>0.002096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.208983</td>\n",
       "      <td>3.238954e-09</td>\n",
       "      <td>0.081871</td>\n",
       "      <td>0.244426</td>\n",
       "      <td>0.077264</td>\n",
       "      <td>0.066682</td>\n",
       "      <td>-0.041078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.120924</td>\n",
       "      <td>-4.518474e-02</td>\n",
       "      <td>-0.058407</td>\n",
       "      <td>0.737773</td>\n",
       "      <td>-0.133465</td>\n",
       "      <td>0.022939</td>\n",
       "      <td>0.026361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        20            60        100       150       200       300       500\n",
       "0  0.000000  2.148908e-01  0.347949  0.470945  0.040536 -0.106924 -0.133976\n",
       "1 -0.107531 -1.182229e-02  0.228211  0.000278  0.506033 -0.142239  0.129941\n",
       "2 -0.143801 -4.209989e-02 -0.013528  0.053678 -0.089497  0.015440 -0.001957\n",
       "3  0.000000  0.000000e+00 -0.050607  0.113708 -0.174694  0.017304 -0.017593\n",
       "4 -0.031628 -3.073195e-03  0.264285  0.067247 -0.360150  0.044943 -0.031897\n",
       "5  0.006097  3.520529e-01 -0.139356  0.023468  0.005132  0.025220  0.234201\n",
       "6  0.000002  2.233623e-01  0.032379  0.088815 -0.034389 -0.039525 -0.060063\n",
       "7 -0.186200 -1.382878e-01  0.244494  0.246064  0.012358 -0.159537  0.002096\n",
       "8 -0.208983  3.238954e-09  0.081871  0.244426  0.077264  0.066682 -0.041078\n",
       "9  0.120924 -4.518474e-02 -0.058407  0.737773 -0.133465  0.022939  0.026361"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xy = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 300, 500])\n",
    "df_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c46b1283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    -0.055112\n",
       "60     0.054984\n",
       "100    0.093729\n",
       "150    0.204640\n",
       "200   -0.015087\n",
       "300   -0.025570\n",
       "500    0.010604\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xy.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b10e7a",
   "metadata": {},
   "source": [
    "# Ising with homogeneous field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 0\n",
      "Training clf_train...\n",
      "Scores: 0.09148684550355596 0.10356823968380986\n",
      "Training clf_holdout...\n",
      "Scores: 0.09000554335199401 0.12394863867265815\n",
      "Scores: 0.15886182714772337 0.24736171195576845\n",
      "Scores: 0.1036874941148683 0.12195346956958858\n",
      "Number of iterations:  21\n",
      "Nonadaptive error:  0.1322223671244211\n",
      "Adaptive error:  0.27261770293731835\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 1\n",
      "Training clf_train...\n",
      "Scores: 0.09337342602775593 0.15645991895709516\n",
      "Training clf_holdout...\n",
      "Scores: 0.13984585663163449 0.09709296278668628\n",
      "Scores: 0.11949184631430475 0.14147948750380598\n",
      "Scores: 0.0403799567443455 0.10935942101906534\n",
      "Number of iterations:  44\n",
      "Nonadaptive error:  0.25233303465688395\n",
      "Adaptive error:  0.3395264744682905\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 2\n",
      "Training clf_train...\n",
      "Scores: 0.059231291468772915 0.10853695515858641\n",
      "Training clf_holdout...\n",
      "Scores: 0.09176398906636432 0.11235641897605583\n",
      "Scores: 0.07152507526028266 0.11090388999871681\n",
      "Scores: 0.07874880673129955 0.1146075230366167\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.012605161675882801\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 3\n",
      "Training clf_train...\n",
      "Scores: 0.08588242843820093 0.14510654161039563\n",
      "Training clf_holdout...\n",
      "Scores: 0.1122556481989665 0.0935396501193689\n",
      "Scores: 0.09171517665564939 0.0879542224663255\n",
      "Scores: 0.11525397076157251 0.0969568072825369\n",
      "Number of iterations:  15\n",
      "Nonadaptive error:  0.11479136849201298\n",
      "Adaptive error:  0.179514444703534\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 4\n",
      "Training clf_train...\n",
      "Scores: 0.050471083116058 0.1308559188856007\n",
      "Training clf_holdout...\n",
      "Scores: 0.07579890868531955 0.13806915571807743\n",
      "Scores: 0.09244385692458185 0.1202507801091158\n",
      "Scores: 0.15350915513894667 0.07821276187440057\n",
      "Number of iterations:  42\n",
      "Nonadaptive error:  0.19264473577817337\n",
      "Adaptive error:  0.10471823767392549\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 5\n",
      "Training clf_train...\n",
      "Scores: 0.13671525303359724 0.14405030531659763\n",
      "Training clf_holdout...\n",
      "Scores: 0.13041647963735414 0.14719475819671132\n",
      "Scores: 0.06913493970452639 0.12201127000533701\n",
      "Scores: 0.12927177524282363 0.15782109380677864\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.07719999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 6\n",
      "Training clf_train...\n",
      "Scores: 0.08015098769635855 0.11595837655594894\n",
      "Training clf_holdout...\n",
      "Scores: 0.07895056792315605 0.1313630962252679\n",
      "Scores: 0.07328760457011277 0.14756465583701983\n",
      "Scores: 0.1463174983485927 0.11502348257217203\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.06200000000000001\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 7\n",
      "Training clf_train...\n",
      "Scores: 0.09025844193878223 0.17295268920343215\n",
      "Training clf_holdout...\n",
      "Scores: 0.10501606584265533 0.0935818841949276\n",
      "Scores: 0.10769381774978487 0.10999019924439636\n",
      "Scores: 0.11716828440328374 0.08296429483597308\n",
      "Number of iterations:  0\n",
      "Nonadaptive error:  0\n",
      "Adaptive error:  0.04159999999999999\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 8\n",
      "Training clf_train...\n",
      "Scores: 0.06930376077898182 0.1632568671947783\n",
      "Training clf_holdout...\n",
      "Scores: 0.1045343346220043 0.12610897962991227\n",
      "Scores: 0.12812062523023254 0.1570024740918354\n",
      "Scores: 0.12110961750767774 0.15775456339152602\n",
      "Number of iterations:  95\n",
      "Nonadaptive error:  0.23050215960879486\n",
      "Adaptive error:  0.2503928654072433\n",
      "--------------------------------------\n",
      "ITERATION: sample size 20, seed 9\n",
      "Training clf_train...\n",
      "Scores: 0.10303418916346178 0.13647010758143632\n",
      "Training clf_holdout...\n",
      "Scores: 0.10281567637125265 0.1326768120821945\n",
      "Scores: 0.11335436088875092 0.07951310434206464\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "N = 50\n",
    "test_size = 0.5\n",
    "num_holdout = 3\n",
    "\n",
    "# Ising model with homogeneous field\n",
    "\n",
    "all_states = []\n",
    "all_values = []\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/states.txt\") as f:\n",
    "    for line in f:\n",
    "        all_states.append(ast.literal_eval(line))\n",
    "\n",
    "with open(\"50spins-oneZ-allt-homogeneous-Ising/values.txt\") as f:\n",
    "    for line in f:\n",
    "        all_values.append([ast.literal_eval(line)[6]])\n",
    "        \n",
    "table = np.zeros((10,6))\n",
    "\n",
    "for s, sample_size in enumerate([20, 60, 100, 150, 200, 500]):\n",
    "    for seed in range(10):\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"ITERATION: sample size {sample_size}, seed {seed}\")\n",
    "        # Train / holdout split\n",
    "        np.random.seed(seed)\n",
    "        sample_idx = np.random.choice(len(all_states), (num_holdout+1)*sample_size, replace=False) # Randomize sampled states\n",
    "        train_states, train_values = np.array(all_states)[\n",
    "            sample_idx[:sample_size]], np.array(all_values)[sample_idx[:sample_size]]\n",
    "        \n",
    "        holdout_states_ens, holdout_values_ens = [],[]\n",
    "        for i in range(1, num_holdout+1):\n",
    "            holdout_states, holdout_values = np.array(all_states)[\n",
    "                sample_idx[i*sample_size:(i+1)*sample_size]], np.array(all_values)[sample_idx[i*sample_size:(i+1)*sample_size]]\n",
    "            holdout_states_ens.append(holdout_states)\n",
    "            holdout_values_ens.append(holdout_values)\n",
    "\n",
    "        # Transform states\n",
    "        train_X_list = transform_states(train_states)\n",
    "        holdout_X_list_ens = [transform_states(holdout_states_ens[i]) for i in range(num_holdout)]\n",
    "        \n",
    "        # Obtain classifiers\n",
    "        print(\"Training clf_train...\")\n",
    "        list_of_score_train, list_of_clf_train, list_of_bestk_train = train_sparse_ML_transformed(\n",
    "            train_X_list, train_values, test_size=test_size, random_seed=seed)\n",
    "        clf_train = list_of_clf_train[0]\n",
    "        k_train = list_of_bestk_train[0]\n",
    "\n",
    "        print(\"Training clf_holdout...\")\n",
    "        clf_holdout_ens = []\n",
    "        score_holdout_ens = []\n",
    "        k_holdout_ens = []\n",
    "        for i in range(num_holdout):\n",
    "            list_of_score_holdout, list_of_clf_holdout, list_of_bestk_holdout = train_sparse_ML_transformed(\n",
    "                holdout_X_list_ens[i], holdout_values_ens[i], test_size=test_size, random_seed=seed)\n",
    "            clf_holdout_ens.append(list_of_clf_holdout[0])\n",
    "            score_holdout_ens.append(list_of_score_holdout[0])\n",
    "            k_holdout_ens.append(list_of_bestk_holdout[0])\n",
    "        \n",
    "        def predict_holdout(x):\n",
    "            # Get holdout prediction\n",
    "            predictions = []\n",
    "            for i in range(num_holdout):\n",
    "                # Get best classifier with best k\n",
    "                clf_holdout = clf_holdout_ens[i]\n",
    "                k_holdout = k_holdout_ens[i]\n",
    "                X_holdout = np.array([get_RDM_in_Pauli(x, k_holdout)])\n",
    "                predictions.append(clf_holdout.predict(X_holdout))\n",
    "            return statistics.median(predictions)[0]\n",
    "\n",
    "        def predict_train(x):\n",
    "            # Get train prediction\n",
    "            X_train = np.array([get_RDM_in_Pauli(x, k_train)])\n",
    "            return clf_train.predict(X_train)[0]\n",
    "\n",
    "        def get_product_state(y):\n",
    "            # Compute product state from input spherical coordinates\n",
    "            x = []\n",
    "            for i in range(N):\n",
    "                phi = y[i]\n",
    "                theta = y[N+i]\n",
    "                x += [np.sin(phi)*np.cos(theta),np.sin(phi)*np.sin(theta),np.cos(phi)] # spherical coordinates\n",
    "            return x\n",
    "\n",
    "        def objective(y):\n",
    "            # Sanity check: optimizes absolute difference between train and holdout predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -abs(y_train - y_holdout)\n",
    "\n",
    "        def objective_train(y):\n",
    "            # Maximizes train predictions\n",
    "            x = get_product_state(y)\n",
    "            y_train = predict_train(x)\n",
    "            return -y_train\n",
    "\n",
    "        def objective_holdout(y):\n",
    "            x = get_product_state(y)\n",
    "            y_holdout = predict_holdout(x)\n",
    "            return -y_holdout\n",
    "        \n",
    "        # Adaptive optimization\n",
    "        np.random.seed(seed)\n",
    "        bnds = tuple([(0,np.pi)]*N+[(0,2*np.pi)]*N) # spherical coords\n",
    "        y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi)) # initial guess\n",
    "        res = minimize(objective_train, y0, method='L-BFGS-B', bounds=bnds, options={'disp': False})\n",
    "        num_eval = res.nfev\n",
    "        num_iter = res.nit\n",
    "        # Adaptive error\n",
    "        state = get_product_state(res.x)\n",
    "        y_train = predict_train(state)\n",
    "        y_holdout = predict_holdout(state)\n",
    "        adapt_err = abs(y_train-y_holdout)\n",
    "        # Nonadaptive error\n",
    "        nonadapt_err = 0\n",
    "        np.random.seed(seed)\n",
    "        # slightly sus, i.e. maybe should do based on num_eval?\n",
    "        for i in range(num_iter):\n",
    "            y0 = np.concatenate((np.random.rand(N)*np.pi,np.random.rand(N)*2*np.pi))\n",
    "            err = -objective(y0)\n",
    "            if err > nonadapt_err:\n",
    "                nonadapt_err = err\n",
    "        print(\"Number of iterations: \", num_iter)\n",
    "        print(\"Nonadaptive error: \", nonadapt_err)\n",
    "        print(\"Adaptive error: \", adapt_err)\n",
    "        table[seed,s] = adapt_err - nonadapt_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ising_homog = pd.DataFrame(data=table, columns=[20, 60, 100, 150, 200, 500])\n",
    "df_ising_homog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa94024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp_qml",
   "language": "python",
   "name": "imp_qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
